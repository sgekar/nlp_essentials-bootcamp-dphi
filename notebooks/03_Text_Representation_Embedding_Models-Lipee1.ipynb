{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JS9QXHRoRiqc"
   },
   "source": [
    "# Text Representation with Feature Engineering\n",
    "\n",
    "### Exploring Word Embeddings with New Deep Learning Models\n",
    "\n",
    "We have discussed in the previous sub-unit that Feature Engineering is the secret sauce to creating superior and better performing machine learning models. \n",
    "\n",
    "Traditional (count-based) feature engineering strategies for textual data involve models belonging to a family of models popularly known as the Bag of Words model. This includes term frequencies, TF-IDF (term frequency-inverse document frequency), N-grams and so on. While they are effective methods for extracting features from text, due to the inherent nature of the model being just a bag of unstructured words, we lose additional information like the semantics, structure, sequence and context around nearby words in each text document. \n",
    "\n",
    "This forms as enough motivation for us to explore more sophisticated models which can capture this information and give us features which are vector representation of words, popularly known as embeddings.\n",
    "\n",
    "Here we will explore the following feature engineering techniques:\n",
    "\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "\n",
    "Predictive methods like Neural Network based language models try to predict words from its neighboring words looking at word sequences in the corpus and in the process it learns distributed representations giving us dense word embeddings. We will be focusing on these predictive methods in this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYUGSn3dRiqd"
   },
   "source": [
    "# Prepare a Sample Corpus\n",
    "\n",
    "Let’s now take a sample corpus of documents on which we will run most of our analyses in this article. A corpus is typically a collection of text documents usually belonging to one or more subjects or domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "KH4ZwBgtRiqe",
    "outputId": "c285dcff-5578-4fa8-9ecf-63dba9b4f7ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Document Category\n",
       "0                                      The sky is blue and beautiful.  weather\n",
       "1                                   Love this blue and beautiful sky!  weather\n",
       "2                        The quick brown fox jumps over the lazy dog.  animals\n",
       "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
       "4                         I love green eggs, ham, sausages and bacon!     food\n",
       "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
       "6            The sky is very blue and the sky is very beautiful today  weather\n",
       "7                         The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLojJzK0Riqi"
   },
   "source": [
    "Let's go ahead and pre-process our text data now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7KR8w3qbRiqi"
   },
   "source": [
    "# Simple Text Pre-processing\n",
    "\n",
    "Since the focus of this unit is on feature engineering, we will build a simple text pre-processor which focuses on removing special characters, extra whitespaces, digits, stopwords and lower casing the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "5YHHrL7VRiqj",
    "outputId": "114e4855-473e-45da-a614-88cc52a190a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\peshw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peshw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog',\n",
       "       'kings breakfast sausages ham bacon eggs toast beans',\n",
       "       'love green eggs ham sausages bacon',\n",
       "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
       "       'dog lazy brown fox quick'], dtype='<U51')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JCyCJ6GRiql"
   },
   "source": [
    "# The Word2Vec Model\n",
    "\n",
    "This model was created by Google in 2013 and is a predictive deep learning based model to compute and generate high quality, distributed and continuous dense vector representations of words, which capture contextual and semantic similarity. Essentially these are unsupervised models which can take in massive textual corpora, create a vocabulary of possible words and generate dense word embeddings for each word in the vector space representing that vocabulary. \n",
    "\n",
    "Usually you can specify the size of the word embedding vectors and the total number of vectors are essentially the size of the vocabulary. This makes the dimensionality of this dense vector space much lower than the high-dimensional sparse vector space built using traditional Bag of Words models.\n",
    "\n",
    "There are two different model architectures which can be leveraged by Word2Vec to create these word embedding representations. These include,\n",
    "\n",
    "- The Continuous Bag of Words (CBOW) Model\n",
    "- The Skip-gram Model\n",
    "\n",
    "## The Continuous Bag of Words (CBOW) Model\n",
    "\n",
    "The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words). \n",
    "\n",
    "Considering a simple sentence, ___“the quick brown fox jumps over the lazy dog”___, this can be pairs of __(context_window, target_word)__ where if we consider a context window of size 2, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on. \n",
    "\n",
    "Thus the model tries to predict the __`target_word`__ based on the __`context_window`__ words.\n",
    "\n",
    "![](https://github.com/dipanjanS/nlp_workshop_odsc19/blob/master/Module04%20-%20Text%20Representation/cbow_arch.png?raw=1)\n",
    "\n",
    "\n",
    "## The Skip-gram Model\n",
    "\n",
    "The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word). \n",
    "\n",
    "Considering our simple sentence from earlier, ___“the quick brown fox jumps over the lazy dog”___. If we used the CBOW model, we get pairs of __(context_window, target_word)__ where if we consider a context window of size 2, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on. \n",
    "\n",
    "Now considering that the skip-gram model’s aim is to predict the context from the target word, the model typically inverts the contexts and targets, and tries to predict each context word from its target word. Hence the task becomes to predict the context __[quick, fox]__ given target word __‘brown’__ or __[the, brown]__ given target word __‘quick’__ and so on. \n",
    "\n",
    "Thus the model tries to predict the context_window words based on the target_word.\n",
    "\n",
    "![](https://github.com/dipanjanS/nlp_workshop_odsc19/blob/master/Module04%20-%20Text%20Representation/skipgram_arch.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHD8pUx7Riqm"
   },
   "source": [
    "# Robust Word2Vec Model with Gensim\n",
    "\n",
    "The __`gensim`__ framework, created by Radim Řehůřek consists of a robust, efficient and scalable implementation of the Word2Vec model. We will leverage the same on our sample toy corpus. In our workflow, we will tokenize our normalized corpus and then focus on the following four parameters in the Word2Vec model to build it.\n",
    "\n",
    "- __`size`:__ The word embedding dimensionality\n",
    "- __`window`:__ The context window size\n",
    "- __`min_count`:__ The minimum word count\n",
    "- __`sample`:__ The downsample setting for frequent words\n",
    "- __`sg`:__ Training model, 1 for skip-gram otherwise CBOW\n",
    "\n",
    "We will build a simple Word2Vec model on the corpus and visualize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-tKqisDuRiqm",
    "outputId": "f7106f35-82d8-4e0d-8940-02d524b4879a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1e816695448>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "\n",
    "tokenized_corpus = [nltk.word_tokenize(doc) for doc in norm_corpus]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 15    # Word vector dimensionality  \n",
    "window_context = 20  # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "sg = 1               # skip-gram model\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus,  \n",
    "                              window=window_context, min_count = min_word_count,\n",
    "                              sg=sg, sample=sample)\n",
    "w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "6NY4l-zaRiqo",
    "outputId": "bd4ef9b9-421e-494a-9a0f-8e3985fba0a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# visualize embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = w2v_model.wv.index_to_key\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=5)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "V7h2GmytRiqq",
    "outputId": "1a4fac8b-d815-4444-da85-cfbde23d1482"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00053284,  0.0002355 ,  0.00510643,  0.00900563, -0.00930537,\n",
       "        -0.00712327,  0.00645674,  0.0089788 , -0.0050154 , -0.00376946,\n",
       "         0.00738573, -0.00153804, -0.00453778,  0.00655376, -0.00486056,\n",
       "        -0.00181047,  0.00287806,  0.00099188, -0.00827994, -0.00944862,\n",
       "         0.00731287,  0.00506933,  0.00676219,  0.00076189,  0.00634605,\n",
       "        -0.00340157, -0.00094943,  0.00576983, -0.00751381, -0.0039374 ,\n",
       "        -0.0075127 , -0.00093124,  0.00954387, -0.00731466, -0.00233048,\n",
       "        -0.00193676,  0.00808468, -0.00593122,  0.00004186, -0.00475079,\n",
       "        -0.00960139,  0.00500511, -0.00876237, -0.00438649, -0.00003868,\n",
       "        -0.00029575, -0.00766406,  0.00961257,  0.00498083,  0.00923122,\n",
       "        -0.00816112,  0.00449227, -0.00413935,  0.00082709,  0.00849971,\n",
       "        -0.00445931,  0.00451762, -0.00678583, -0.00353982,  0.00940205,\n",
       "        -0.00158375,  0.00032194, -0.00413758, -0.00768055, -0.00151129,\n",
       "         0.00247213, -0.00089148,  0.00553118, -0.00274054,  0.00226044,\n",
       "         0.005455  ,  0.00834704, -0.00145317, -0.00920385,  0.00437388,\n",
       "         0.00057305,  0.00744097, -0.00081716, -0.00264176, -0.00875302,\n",
       "        -0.0008586 ,  0.00283103,  0.0054063 ,  0.00705656, -0.00570774,\n",
       "         0.00186162,  0.00608684, -0.00479632, -0.0031075 ,  0.00679664,\n",
       "         0.00163663,  0.00018893,  0.00347234,  0.00022199,  0.00961403,\n",
       "         0.00505904, -0.00890995, -0.00704142,  0.00090108,  0.00639393],\n",
       "       dtype=float32),\n",
       " (100,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['sky'], w2v_model.wv['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "3INpkJK4Riqs",
    "outputId": "bebb8557-724d-40c7-bf92-bbeb5e0bf737"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>-0.000533</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>0.009006</td>\n",
       "      <td>-0.009305</td>\n",
       "      <td>-0.007123</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.008979</td>\n",
       "      <td>-0.005015</td>\n",
       "      <td>-0.003769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>-0.008910</td>\n",
       "      <td>-0.007041</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.006394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>-0.008618</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>-0.006173</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>-0.002838</td>\n",
       "      <td>-0.006180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>-0.001575</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>-0.007882</td>\n",
       "      <td>-0.002711</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>-0.002390</td>\n",
       "      <td>-0.009514</td>\n",
       "      <td>0.004512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>-0.006810</td>\n",
       "      <td>-0.001383</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>-0.003669</td>\n",
       "      <td>0.002650</td>\n",
       "      <td>-0.008329</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004499</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>0.009187</td>\n",
       "      <td>-0.004101</td>\n",
       "      <td>0.007971</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.008220</td>\n",
       "      <td>-0.007033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>-0.008245</td>\n",
       "      <td>0.009298</td>\n",
       "      <td>-0.000197</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>-0.004096</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007427</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>-0.000795</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>0.009684</td>\n",
       "      <td>-0.000456</td>\n",
       "      <td>0.005872</td>\n",
       "      <td>-0.007450</td>\n",
       "      <td>-0.002506</td>\n",
       "      <td>-0.005550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>-0.007133</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>-0.007173</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.005829</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>-0.004113</td>\n",
       "      <td>0.007220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>-0.004713</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>-0.004228</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>-0.008046</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.003012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>-0.008727</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>-0.000872</td>\n",
       "      <td>-0.009319</td>\n",
       "      <td>-0.009427</td>\n",
       "      <td>-0.001411</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>-0.006501</td>\n",
       "      <td>-0.006875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.008939</td>\n",
       "      <td>-0.008208</td>\n",
       "      <td>-0.003013</td>\n",
       "      <td>0.009890</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>-0.001589</td>\n",
       "      <td>-0.008696</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>-0.006679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0.008133</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>-0.001070</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.003248</td>\n",
       "      <td>-0.001512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002699</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>-0.003533</td>\n",
       "      <td>-0.000422</td>\n",
       "      <td>-0.000705</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>-0.005735</td>\n",
       "      <td>-0.001659</td>\n",
       "      <td>0.005570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.008170</td>\n",
       "      <td>-0.004445</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.008263</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>-0.003926</td>\n",
       "      <td>-0.005566</td>\n",
       "      <td>-0.006509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>-0.004005</td>\n",
       "      <td>-0.008242</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>-0.001943</td>\n",
       "      <td>-0.000667</td>\n",
       "      <td>-0.001773</td>\n",
       "      <td>-0.004537</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>-0.004270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>-0.009579</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>0.006644</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>-0.004425</td>\n",
       "      <td>-0.006803</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005085</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>-0.001536</td>\n",
       "      <td>0.009932</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>0.005891</td>\n",
       "      <td>-0.005581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>-0.005155</td>\n",
       "      <td>-0.006665</td>\n",
       "      <td>-0.007776</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>-0.001977</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>-0.004149</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>-0.002873</td>\n",
       "      <td>-0.003756</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008972</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>-0.007287</td>\n",
       "      <td>-0.009038</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.009391</td>\n",
       "      <td>0.003507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.007097</td>\n",
       "      <td>-0.001560</td>\n",
       "      <td>0.007953</td>\n",
       "      <td>-0.009501</td>\n",
       "      <td>-0.008024</td>\n",
       "      <td>-0.006648</td>\n",
       "      <td>-0.003998</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.003811</td>\n",
       "      <td>-0.008331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>-0.001259</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>-0.005638</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.009460</td>\n",
       "      <td>-0.005479</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>-0.008114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>-0.006455</td>\n",
       "      <td>-0.001426</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>-0.004620</td>\n",
       "      <td>-0.003995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>-0.003261</td>\n",
       "      <td>-0.009266</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>-0.005632</td>\n",
       "      <td>-0.007864</td>\n",
       "      <td>-0.002973</td>\n",
       "      <td>-0.004928</td>\n",
       "      <td>-0.002319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>-0.001944</td>\n",
       "      <td>-0.005268</td>\n",
       "      <td>0.009447</td>\n",
       "      <td>-0.009299</td>\n",
       "      <td>0.004504</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>-0.001409</td>\n",
       "      <td>0.009007</td>\n",
       "      <td>0.009885</td>\n",
       "      <td>-0.005475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>-0.002565</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>-0.007660</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.008732</td>\n",
       "      <td>0.005983</td>\n",
       "      <td>0.006815</td>\n",
       "      <td>0.007823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>-0.009500</td>\n",
       "      <td>0.009562</td>\n",
       "      <td>-0.007771</td>\n",
       "      <td>-0.002646</td>\n",
       "      <td>-0.004906</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.008024</td>\n",
       "      <td>-0.007784</td>\n",
       "      <td>-0.004553</td>\n",
       "      <td>-0.001275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.007234</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>-0.001347</td>\n",
       "      <td>-0.005890</td>\n",
       "      <td>-0.004533</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>-0.003135</td>\n",
       "      <td>-0.006339</td>\n",
       "      <td>0.009870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>0.007699</td>\n",
       "      <td>0.009124</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>-0.008330</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>-0.003700</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.009690</td>\n",
       "      <td>-0.009298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007101</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.006382</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>-0.006126</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>-0.006098</td>\n",
       "      <td>0.009436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>-0.007188</td>\n",
       "      <td>0.004236</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>-0.004882</td>\n",
       "      <td>-0.004570</td>\n",
       "      <td>-0.006096</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>-0.004498</td>\n",
       "      <td>0.008516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>-0.005791</td>\n",
       "      <td>-0.008747</td>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.006747</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.006761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>0.001300</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>-0.003130</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007012</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>0.007094</td>\n",
       "      <td>-0.005694</td>\n",
       "      <td>0.007241</td>\n",
       "      <td>-0.009295</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>-0.007757</td>\n",
       "      <td>0.004193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.007055</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>-0.007001</td>\n",
       "      <td>0.007711</td>\n",
       "      <td>-0.006006</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>-0.004019</td>\n",
       "      <td>-0.004710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>-0.003604</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>-0.001865</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>-0.007048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>0.009739</td>\n",
       "      <td>-0.009770</td>\n",
       "      <td>-0.006493</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>-0.005377</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>0.009129</td>\n",
       "      <td>-0.006816</td>\n",
       "      <td>-0.006113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007453</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>-0.002702</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.005627</td>\n",
       "      <td>0.005497</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>0.006559</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>-0.004207</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>-0.005234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000959</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>-0.008595</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>-0.009208</td>\n",
       "      <td>-0.009625</td>\n",
       "      <td>-0.008512</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.009249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5   \\\n",
       "sky       -0.000533  0.000235  0.005106  0.009006 -0.009305 -0.007123   \n",
       "blue      -0.008618  0.003673  0.005192  0.005741  0.007473 -0.006173   \n",
       "lazy       0.000100  0.003081 -0.006810 -0.001383  0.007674  0.007340   \n",
       "beautiful -0.008245  0.009298 -0.000197 -0.001961  0.004605 -0.004096   \n",
       "quick     -0.007133  0.001239 -0.007173 -0.002247  0.003712  0.005829   \n",
       "brown     -0.008727  0.002131 -0.000872 -0.009319 -0.009427 -0.001411   \n",
       "fox        0.008133 -0.004452 -0.001070  0.001003 -0.000184  0.001146   \n",
       "dog        0.008170 -0.004445  0.008983  0.008263 -0.004437  0.000309   \n",
       "sausages  -0.009579  0.008943  0.004165  0.009235  0.006644  0.002925   \n",
       "ham       -0.005155 -0.006665 -0.007776  0.008309 -0.001977 -0.006860   \n",
       "bacon      0.007097 -0.001560  0.007953 -0.009501 -0.008024 -0.006648   \n",
       "eggs       0.009774  0.008167  0.001282  0.005096  0.001410 -0.006455   \n",
       "love      -0.001944 -0.005268  0.009447 -0.009299  0.004504  0.005404   \n",
       "breakfast -0.009500  0.009562 -0.007771 -0.002646 -0.004906 -0.004967   \n",
       "kings      0.007699  0.009124  0.001138 -0.008330  0.008427 -0.003700   \n",
       "green     -0.007188  0.004236  0.002163  0.007438 -0.004882 -0.004570   \n",
       "jumps      0.001300 -0.009804  0.004588 -0.000538  0.006332  0.001783   \n",
       "toast      0.001815  0.007055  0.002960 -0.007001  0.007711 -0.006006   \n",
       "beans      0.009739 -0.009770 -0.006493  0.002774  0.006442 -0.005377   \n",
       "today      0.005627  0.005497  0.001829  0.005749 -0.008968  0.006559   \n",
       "\n",
       "                 6         7         8         9   ...        90        91  \\\n",
       "sky        0.006457  0.008979 -0.005015 -0.003769  ...  0.001637  0.000189   \n",
       "blue       0.001114  0.006053 -0.002838 -0.006180  ...  0.001093 -0.001575   \n",
       "lazy      -0.003669  0.002650 -0.008329  0.006200  ... -0.004499  0.005705   \n",
       "beautiful  0.002745  0.006941  0.006061 -0.007511  ... -0.007427 -0.001066   \n",
       "quick      0.001199  0.002107 -0.004113  0.007220  ...  0.003142 -0.004713   \n",
       "brown      0.004435  0.003705 -0.006501 -0.006875  ...  0.009072  0.008939   \n",
       "fox        0.006115 -0.000016 -0.003248 -0.001512  ... -0.002699  0.000448   \n",
       "dog        0.004274 -0.003926 -0.005566 -0.006509  ...  0.002053 -0.004005   \n",
       "sausages   0.009804 -0.004425 -0.006803  0.004227  ... -0.005085  0.001131   \n",
       "ham       -0.004149  0.005151 -0.002873 -0.003756  ... -0.008972  0.008593   \n",
       "bacon     -0.003998  0.004995 -0.003811 -0.008331  ...  0.007524  0.001501   \n",
       "eggs      -0.001426  0.006450 -0.004620 -0.003995  ...  0.004776 -0.003261   \n",
       "love      -0.001409  0.009007  0.009885 -0.005475  ...  0.002651 -0.002565   \n",
       "breakfast -0.008024 -0.007784 -0.004553 -0.001275  ...  0.008380  0.007234   \n",
       "kings      0.005745  0.004394  0.009690 -0.009298  ...  0.007101  0.001903   \n",
       "green     -0.006096  0.003307 -0.004498  0.008516  ... -0.003475  0.003491   \n",
       "jumps     -0.003130  0.007760  0.001555  0.000055  ...  0.007012  0.004829   \n",
       "toast      0.009003  0.002971 -0.004019 -0.004710  ...  0.009149  0.003593   \n",
       "beans      0.002761  0.009129 -0.006816 -0.006113  ...  0.007453 -0.004294   \n",
       "today      0.009226 -0.004207  0.001608 -0.005234  ... -0.000959  0.001310   \n",
       "\n",
       "                 92        93        94        95        96        97  \\\n",
       "sky        0.003472  0.000222  0.009614  0.005059 -0.008910 -0.007041   \n",
       "blue       0.002202 -0.007882 -0.002711  0.002665  0.005343 -0.002390   \n",
       "lazy       0.009187 -0.004101  0.007971  0.005383  0.005884  0.000506   \n",
       "beautiful -0.000795 -0.002566  0.009684 -0.000456  0.005872 -0.007450   \n",
       "quick      0.005281 -0.004228  0.002642 -0.008046  0.006211  0.004815   \n",
       "brown     -0.008208 -0.003013  0.009890  0.005106 -0.001589 -0.008696   \n",
       "fox       -0.003533 -0.000422 -0.000705  0.000826  0.008197 -0.005735   \n",
       "dog       -0.008242  0.006279 -0.001943 -0.000667 -0.001773 -0.004537   \n",
       "sausages   0.002883 -0.001536  0.009932  0.008350  0.002416  0.007118   \n",
       "ham        0.004052  0.007469  0.009752 -0.007287 -0.009038  0.005833   \n",
       "bacon     -0.001259  0.005773 -0.005638  0.000038  0.009460 -0.005479   \n",
       "eggs      -0.009266  0.003788  0.007163 -0.005632 -0.007864 -0.002973   \n",
       "love       0.006448 -0.007660  0.003394  0.000490  0.008732  0.005983   \n",
       "breakfast  0.001730 -0.001347 -0.005890 -0.004533  0.008648 -0.003135   \n",
       "kings      0.005202  0.006382  0.001914 -0.006126 -0.000006  0.008268   \n",
       "green     -0.005791 -0.008747 -0.005515  0.006747  0.006422  0.009441   \n",
       "jumps      0.008683  0.007094 -0.005694  0.007241 -0.009295 -0.002588   \n",
       "toast      0.006569 -0.003604  0.006792  0.007247 -0.002123 -0.001865   \n",
       "beans      0.004584  0.009089  0.003047  0.003142  0.004065 -0.002702   \n",
       "today     -0.008595  0.008749 -0.009208 -0.009625 -0.008512  0.007313   \n",
       "\n",
       "                 98        99  \n",
       "sky        0.000901  0.006394  \n",
       "blue      -0.009514  0.004512  \n",
       "lazy       0.008220 -0.007033  \n",
       "beautiful -0.002506 -0.005550  \n",
       "quick      0.000790  0.003012  \n",
       "brown      0.002962 -0.006679  \n",
       "fox       -0.001659  0.005570  \n",
       "dog        0.004066 -0.004270  \n",
       "sausages   0.005891 -0.005581  \n",
       "ham        0.009391  0.003507  \n",
       "bacon      0.003814 -0.008114  \n",
       "eggs      -0.004928 -0.002319  \n",
       "love       0.006815  0.007823  \n",
       "breakfast -0.006339  0.009870  \n",
       "kings     -0.006098  0.009436  \n",
       "green      0.007051  0.006761  \n",
       "jumps     -0.007757  0.004193  \n",
       "toast      0.003616 -0.007048  \n",
       "beans      0.003822  0.000336  \n",
       "today      0.005466  0.009249  \n",
       "\n",
       "[20 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df = pd.DataFrame(wvs, index=words)\n",
    "vec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTCiDguVRiqu"
   },
   "source": [
    "### Looking at term semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "id": "S63FJhi2Riqv",
    "outputId": "ca368c74-ffbc-4b7a-9d48-25c2b3aa9358"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sky</th>\n",
       "      <th>blue</th>\n",
       "      <th>lazy</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>quick</th>\n",
       "      <th>brown</th>\n",
       "      <th>fox</th>\n",
       "      <th>dog</th>\n",
       "      <th>sausages</th>\n",
       "      <th>ham</th>\n",
       "      <th>bacon</th>\n",
       "      <th>eggs</th>\n",
       "      <th>love</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>kings</th>\n",
       "      <th>green</th>\n",
       "      <th>jumps</th>\n",
       "      <th>toast</th>\n",
       "      <th>beans</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010486</td>\n",
       "      <td>-0.052628</td>\n",
       "      <td>-0.111528</td>\n",
       "      <td>-0.027551</td>\n",
       "      <td>-0.059903</td>\n",
       "      <td>0.016010</td>\n",
       "      <td>0.093210</td>\n",
       "      <td>0.027032</td>\n",
       "      <td>0.216277</td>\n",
       "      <td>0.063204</td>\n",
       "      <td>0.079641</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>-0.041206</td>\n",
       "      <td>-0.037588</td>\n",
       "      <td>0.054451</td>\n",
       "      <td>0.218867</td>\n",
       "      <td>-0.113870</td>\n",
       "      <td>-0.014342</td>\n",
       "      <td>0.002472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>-0.010486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023673</td>\n",
       "      <td>0.068212</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.009366</td>\n",
       "      <td>-0.113922</td>\n",
       "      <td>-0.115575</td>\n",
       "      <td>0.033609</td>\n",
       "      <td>-0.095451</td>\n",
       "      <td>-0.134269</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>-0.003616</td>\n",
       "      <td>0.137272</td>\n",
       "      <td>0.161227</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.085752</td>\n",
       "      <td>0.159690</td>\n",
       "      <td>-0.159853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>-0.052628</td>\n",
       "      <td>-0.023673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013439</td>\n",
       "      <td>0.170080</td>\n",
       "      <td>0.064290</td>\n",
       "      <td>0.146087</td>\n",
       "      <td>-0.002024</td>\n",
       "      <td>0.199144</td>\n",
       "      <td>-0.032889</td>\n",
       "      <td>-0.101577</td>\n",
       "      <td>0.173161</td>\n",
       "      <td>0.046518</td>\n",
       "      <td>-0.135233</td>\n",
       "      <td>-0.185223</td>\n",
       "      <td>-0.019798</td>\n",
       "      <td>-0.017042</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.152968</td>\n",
       "      <td>-0.035183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>-0.111528</td>\n",
       "      <td>0.068212</td>\n",
       "      <td>-0.013439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.044538</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.041589</td>\n",
       "      <td>-0.013624</td>\n",
       "      <td>0.074971</td>\n",
       "      <td>-0.169292</td>\n",
       "      <td>0.012924</td>\n",
       "      <td>0.041348</td>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>0.178155</td>\n",
       "      <td>-0.153942</td>\n",
       "      <td>-0.260758</td>\n",
       "      <td>0.041376</td>\n",
       "      <td>-0.041950</td>\n",
       "      <td>-0.001830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>-0.027551</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.170080</td>\n",
       "      <td>-0.044538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.138832</td>\n",
       "      <td>0.034770</td>\n",
       "      <td>-0.028082</td>\n",
       "      <td>-0.069024</td>\n",
       "      <td>-0.173125</td>\n",
       "      <td>-0.258496</td>\n",
       "      <td>-0.005777</td>\n",
       "      <td>0.150277</td>\n",
       "      <td>0.252918</td>\n",
       "      <td>-0.086099</td>\n",
       "      <td>0.016038</td>\n",
       "      <td>0.108613</td>\n",
       "      <td>-0.029504</td>\n",
       "      <td>0.032947</td>\n",
       "      <td>-0.276838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>-0.059903</td>\n",
       "      <td>0.009366</td>\n",
       "      <td>0.064290</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.138832</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019214</td>\n",
       "      <td>-0.057632</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>-0.105102</td>\n",
       "      <td>0.019992</td>\n",
       "      <td>0.166978</td>\n",
       "      <td>-0.145121</td>\n",
       "      <td>0.044112</td>\n",
       "      <td>-0.109112</td>\n",
       "      <td>-0.026460</td>\n",
       "      <td>0.047645</td>\n",
       "      <td>0.071818</td>\n",
       "      <td>-0.140769</td>\n",
       "      <td>-0.126858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0.016010</td>\n",
       "      <td>-0.113922</td>\n",
       "      <td>0.146087</td>\n",
       "      <td>0.041589</td>\n",
       "      <td>0.034770</td>\n",
       "      <td>0.019214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>-0.083685</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.012811</td>\n",
       "      <td>0.007866</td>\n",
       "      <td>-0.146393</td>\n",
       "      <td>0.163691</td>\n",
       "      <td>-0.107197</td>\n",
       "      <td>0.074837</td>\n",
       "      <td>0.110690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.093210</td>\n",
       "      <td>-0.115575</td>\n",
       "      <td>-0.002024</td>\n",
       "      <td>-0.013624</td>\n",
       "      <td>-0.028082</td>\n",
       "      <td>-0.057632</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144459</td>\n",
       "      <td>-0.093106</td>\n",
       "      <td>0.108749</td>\n",
       "      <td>0.111620</td>\n",
       "      <td>-0.040845</td>\n",
       "      <td>-0.025583</td>\n",
       "      <td>0.105429</td>\n",
       "      <td>0.080558</td>\n",
       "      <td>0.174864</td>\n",
       "      <td>0.119101</td>\n",
       "      <td>0.079293</td>\n",
       "      <td>0.318992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>0.027032</td>\n",
       "      <td>0.033609</td>\n",
       "      <td>0.199144</td>\n",
       "      <td>0.074971</td>\n",
       "      <td>-0.069024</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>-0.144459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>0.026763</td>\n",
       "      <td>0.037713</td>\n",
       "      <td>-0.040441</td>\n",
       "      <td>-0.122462</td>\n",
       "      <td>-0.076904</td>\n",
       "      <td>-0.014549</td>\n",
       "      <td>-0.120705</td>\n",
       "      <td>0.012425</td>\n",
       "      <td>0.015794</td>\n",
       "      <td>0.096739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.216277</td>\n",
       "      <td>-0.095451</td>\n",
       "      <td>-0.032889</td>\n",
       "      <td>-0.169292</td>\n",
       "      <td>-0.173125</td>\n",
       "      <td>-0.105102</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>-0.093106</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>-0.074221</td>\n",
       "      <td>-0.045663</td>\n",
       "      <td>-0.106110</td>\n",
       "      <td>-0.045302</td>\n",
       "      <td>0.010598</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>-0.160124</td>\n",
       "      <td>0.020276</td>\n",
       "      <td>-0.037992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.063204</td>\n",
       "      <td>-0.134269</td>\n",
       "      <td>-0.101577</td>\n",
       "      <td>0.012924</td>\n",
       "      <td>-0.258496</td>\n",
       "      <td>0.019992</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>0.108749</td>\n",
       "      <td>0.026763</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109504</td>\n",
       "      <td>0.128159</td>\n",
       "      <td>-0.001125</td>\n",
       "      <td>-0.045570</td>\n",
       "      <td>-0.042557</td>\n",
       "      <td>-0.012008</td>\n",
       "      <td>0.098029</td>\n",
       "      <td>-0.049844</td>\n",
       "      <td>0.086437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.079641</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>0.173161</td>\n",
       "      <td>0.041348</td>\n",
       "      <td>-0.005777</td>\n",
       "      <td>0.166978</td>\n",
       "      <td>-0.083685</td>\n",
       "      <td>0.111620</td>\n",
       "      <td>0.037713</td>\n",
       "      <td>-0.074221</td>\n",
       "      <td>0.109504</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030283</td>\n",
       "      <td>-0.162883</td>\n",
       "      <td>-0.058880</td>\n",
       "      <td>0.013226</td>\n",
       "      <td>-0.148470</td>\n",
       "      <td>0.060082</td>\n",
       "      <td>0.132717</td>\n",
       "      <td>0.097388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.093197</td>\n",
       "      <td>-0.003616</td>\n",
       "      <td>0.046518</td>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.150277</td>\n",
       "      <td>-0.145121</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>-0.040845</td>\n",
       "      <td>-0.040441</td>\n",
       "      <td>-0.045663</td>\n",
       "      <td>0.128159</td>\n",
       "      <td>-0.030283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.076390</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>-0.003628</td>\n",
       "      <td>-0.116366</td>\n",
       "      <td>-0.067320</td>\n",
       "      <td>0.035484</td>\n",
       "      <td>-0.001872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>-0.041206</td>\n",
       "      <td>0.137272</td>\n",
       "      <td>-0.135233</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>0.252918</td>\n",
       "      <td>0.044112</td>\n",
       "      <td>0.012811</td>\n",
       "      <td>-0.025583</td>\n",
       "      <td>-0.122462</td>\n",
       "      <td>-0.106110</td>\n",
       "      <td>-0.001125</td>\n",
       "      <td>-0.162883</td>\n",
       "      <td>-0.076390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027042</td>\n",
       "      <td>-0.144852</td>\n",
       "      <td>-0.032478</td>\n",
       "      <td>0.116573</td>\n",
       "      <td>0.142687</td>\n",
       "      <td>-0.049997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>-0.037588</td>\n",
       "      <td>0.161227</td>\n",
       "      <td>-0.185223</td>\n",
       "      <td>0.178155</td>\n",
       "      <td>-0.086099</td>\n",
       "      <td>-0.109112</td>\n",
       "      <td>0.007866</td>\n",
       "      <td>0.105429</td>\n",
       "      <td>-0.076904</td>\n",
       "      <td>-0.045302</td>\n",
       "      <td>-0.045570</td>\n",
       "      <td>-0.058880</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.027042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.097812</td>\n",
       "      <td>0.048859</td>\n",
       "      <td>0.247344</td>\n",
       "      <td>-0.005284</td>\n",
       "      <td>0.162057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.054451</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>-0.019798</td>\n",
       "      <td>-0.153942</td>\n",
       "      <td>0.016038</td>\n",
       "      <td>-0.026460</td>\n",
       "      <td>-0.146393</td>\n",
       "      <td>0.080558</td>\n",
       "      <td>-0.014549</td>\n",
       "      <td>0.010598</td>\n",
       "      <td>-0.042557</td>\n",
       "      <td>0.013226</td>\n",
       "      <td>-0.003628</td>\n",
       "      <td>-0.144852</td>\n",
       "      <td>-0.097812</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.065524</td>\n",
       "      <td>-0.065974</td>\n",
       "      <td>-0.064708</td>\n",
       "      <td>0.000697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>0.218867</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>-0.017042</td>\n",
       "      <td>-0.260758</td>\n",
       "      <td>0.108613</td>\n",
       "      <td>0.047645</td>\n",
       "      <td>0.163691</td>\n",
       "      <td>0.174864</td>\n",
       "      <td>-0.120705</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>-0.012008</td>\n",
       "      <td>-0.148470</td>\n",
       "      <td>-0.116366</td>\n",
       "      <td>-0.032478</td>\n",
       "      <td>0.048859</td>\n",
       "      <td>0.065524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.148985</td>\n",
       "      <td>0.059518</td>\n",
       "      <td>-0.038573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>-0.113870</td>\n",
       "      <td>0.085752</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.041376</td>\n",
       "      <td>-0.029504</td>\n",
       "      <td>0.071818</td>\n",
       "      <td>-0.107197</td>\n",
       "      <td>0.119101</td>\n",
       "      <td>0.012425</td>\n",
       "      <td>-0.160124</td>\n",
       "      <td>0.098029</td>\n",
       "      <td>0.060082</td>\n",
       "      <td>-0.067320</td>\n",
       "      <td>0.116573</td>\n",
       "      <td>0.247344</td>\n",
       "      <td>-0.065974</td>\n",
       "      <td>-0.148985</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120099</td>\n",
       "      <td>-0.046950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>-0.014342</td>\n",
       "      <td>0.159690</td>\n",
       "      <td>0.152968</td>\n",
       "      <td>-0.041950</td>\n",
       "      <td>0.032947</td>\n",
       "      <td>-0.140769</td>\n",
       "      <td>0.074837</td>\n",
       "      <td>0.079293</td>\n",
       "      <td>0.015794</td>\n",
       "      <td>0.020276</td>\n",
       "      <td>-0.049844</td>\n",
       "      <td>0.132717</td>\n",
       "      <td>0.035484</td>\n",
       "      <td>0.142687</td>\n",
       "      <td>-0.005284</td>\n",
       "      <td>-0.064708</td>\n",
       "      <td>0.059518</td>\n",
       "      <td>0.120099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.002472</td>\n",
       "      <td>-0.159853</td>\n",
       "      <td>-0.035183</td>\n",
       "      <td>-0.001830</td>\n",
       "      <td>-0.276838</td>\n",
       "      <td>-0.126858</td>\n",
       "      <td>0.110690</td>\n",
       "      <td>0.318992</td>\n",
       "      <td>0.096739</td>\n",
       "      <td>-0.037992</td>\n",
       "      <td>0.086437</td>\n",
       "      <td>0.097388</td>\n",
       "      <td>-0.001872</td>\n",
       "      <td>-0.049997</td>\n",
       "      <td>0.162057</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>-0.038573</td>\n",
       "      <td>-0.046950</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sky      blue      lazy  beautiful     quick     brown  \\\n",
       "sky        1.000000 -0.010486 -0.052628  -0.111528 -0.027551 -0.059903   \n",
       "blue      -0.010486  1.000000 -0.023673   0.068212  0.004253  0.009366   \n",
       "lazy      -0.052628 -0.023673  1.000000  -0.013439  0.170080  0.064290   \n",
       "beautiful -0.111528  0.068212 -0.013439   1.000000 -0.044538  0.131579   \n",
       "quick     -0.027551  0.004253  0.170080  -0.044538  1.000000  0.138832   \n",
       "brown     -0.059903  0.009366  0.064290   0.131579  0.138832  1.000000   \n",
       "fox        0.016010 -0.113922  0.146087   0.041589  0.034770  0.019214   \n",
       "dog        0.093210 -0.115575 -0.002024  -0.013624 -0.028082 -0.057632   \n",
       "sausages   0.027032  0.033609  0.199144   0.074971 -0.069024  0.060613   \n",
       "ham        0.216277 -0.095451 -0.032889  -0.169292 -0.173125 -0.105102   \n",
       "bacon      0.063204 -0.134269 -0.101577   0.012924 -0.258496  0.019992   \n",
       "eggs       0.079641  0.008203  0.173161   0.041348 -0.005777  0.166978   \n",
       "love       0.093197 -0.003616  0.046518  -0.009290  0.150277 -0.145121   \n",
       "breakfast -0.041206  0.137272 -0.135233   0.006648  0.252918  0.044112   \n",
       "kings     -0.037588  0.161227 -0.185223   0.178155 -0.086099 -0.109112   \n",
       "green      0.054451  0.123320 -0.019798  -0.153942  0.016038 -0.026460   \n",
       "jumps      0.218867  0.022355 -0.017042  -0.260758  0.108613  0.047645   \n",
       "toast     -0.113870  0.085752  0.002146   0.041376 -0.029504  0.071818   \n",
       "beans     -0.014342  0.159690  0.152968  -0.041950  0.032947 -0.140769   \n",
       "today      0.002472 -0.159853 -0.035183  -0.001830 -0.276838 -0.126858   \n",
       "\n",
       "                fox       dog  sausages       ham     bacon      eggs  \\\n",
       "sky        0.016010  0.093210  0.027032  0.216277  0.063204  0.079641   \n",
       "blue      -0.113922 -0.115575  0.033609 -0.095451 -0.134269  0.008203   \n",
       "lazy       0.146087 -0.002024  0.199144 -0.032889 -0.101577  0.173161   \n",
       "beautiful  0.041589 -0.013624  0.074971 -0.169292  0.012924  0.041348   \n",
       "quick      0.034770 -0.028082 -0.069024 -0.173125 -0.258496 -0.005777   \n",
       "brown      0.019214 -0.057632  0.060613 -0.105102  0.019992  0.166978   \n",
       "fox        1.000000  0.004815  0.008904  0.001921  0.050417 -0.083685   \n",
       "dog        0.004815  1.000000 -0.144459 -0.093106  0.108749  0.111620   \n",
       "sausages   0.008904 -0.144459  1.000000  0.044722  0.026763  0.037713   \n",
       "ham        0.001921 -0.093106  0.044722  1.000000  0.014945 -0.074221   \n",
       "bacon      0.050417  0.108749  0.026763  0.014945  1.000000  0.109504   \n",
       "eggs      -0.083685  0.111620  0.037713 -0.074221  0.109504  1.000000   \n",
       "love       0.000719 -0.040845 -0.040441 -0.045663  0.128159 -0.030283   \n",
       "breakfast  0.012811 -0.025583 -0.122462 -0.106110 -0.001125 -0.162883   \n",
       "kings      0.007866  0.105429 -0.076904 -0.045302 -0.045570 -0.058880   \n",
       "green     -0.146393  0.080558 -0.014549  0.010598 -0.042557  0.013226   \n",
       "jumps      0.163691  0.174864 -0.120705  0.006430 -0.012008 -0.148470   \n",
       "toast     -0.107197  0.119101  0.012425 -0.160124  0.098029  0.060082   \n",
       "beans      0.074837  0.079293  0.015794  0.020276 -0.049844  0.132717   \n",
       "today      0.110690  0.318992  0.096739 -0.037992  0.086437  0.097388   \n",
       "\n",
       "               love  breakfast     kings     green     jumps     toast  \\\n",
       "sky        0.093197  -0.041206 -0.037588  0.054451  0.218867 -0.113870   \n",
       "blue      -0.003616   0.137272  0.161227  0.123320  0.022355  0.085752   \n",
       "lazy       0.046518  -0.135233 -0.185223 -0.019798 -0.017042  0.002146   \n",
       "beautiful -0.009290   0.006648  0.178155 -0.153942 -0.260758  0.041376   \n",
       "quick      0.150277   0.252918 -0.086099  0.016038  0.108613 -0.029504   \n",
       "brown     -0.145121   0.044112 -0.109112 -0.026460  0.047645  0.071818   \n",
       "fox        0.000719   0.012811  0.007866 -0.146393  0.163691 -0.107197   \n",
       "dog       -0.040845  -0.025583  0.105429  0.080558  0.174864  0.119101   \n",
       "sausages  -0.040441  -0.122462 -0.076904 -0.014549 -0.120705  0.012425   \n",
       "ham       -0.045663  -0.106110 -0.045302  0.010598  0.006430 -0.160124   \n",
       "bacon      0.128159  -0.001125 -0.045570 -0.042557 -0.012008  0.098029   \n",
       "eggs      -0.030283  -0.162883 -0.058880  0.013226 -0.148470  0.060082   \n",
       "love       1.000000  -0.076390  0.092150 -0.003628 -0.116366 -0.067320   \n",
       "breakfast -0.076390   1.000000  0.027042 -0.144852 -0.032478  0.116573   \n",
       "kings      0.092150   0.027042  1.000000 -0.097812  0.048859  0.247344   \n",
       "green     -0.003628  -0.144852 -0.097812  1.000000  0.065524 -0.065974   \n",
       "jumps     -0.116366  -0.032478  0.048859  0.065524  1.000000 -0.148985   \n",
       "toast     -0.067320   0.116573  0.247344 -0.065974 -0.148985  1.000000   \n",
       "beans      0.035484   0.142687 -0.005284 -0.064708  0.059518  0.120099   \n",
       "today     -0.001872  -0.049997  0.162057  0.000697 -0.038573 -0.046950   \n",
       "\n",
       "              beans     today  \n",
       "sky       -0.014342  0.002472  \n",
       "blue       0.159690 -0.159853  \n",
       "lazy       0.152968 -0.035183  \n",
       "beautiful -0.041950 -0.001830  \n",
       "quick      0.032947 -0.276838  \n",
       "brown     -0.140769 -0.126858  \n",
       "fox        0.074837  0.110690  \n",
       "dog        0.079293  0.318992  \n",
       "sausages   0.015794  0.096739  \n",
       "ham        0.020276 -0.037992  \n",
       "bacon     -0.049844  0.086437  \n",
       "eggs       0.132717  0.097388  \n",
       "love       0.035484 -0.001872  \n",
       "breakfast  0.142687 -0.049997  \n",
       "kings     -0.005284  0.162057  \n",
       "green     -0.064708  0.000697  \n",
       "jumps      0.059518 -0.038573  \n",
       "toast      0.120099 -0.046950  \n",
       "beans      1.000000  0.000495  \n",
       "today      0.000495  1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(vec_df.values)\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "ngIHnn__Riqx",
    "outputId": "d05dd28c-5955-4313-ce19-029b87b8dfba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sky                  [jumps, ham, dog]\n",
       "blue         [kings, beans, breakfast]\n",
       "lazy           [sausages, eggs, quick]\n",
       "beautiful     [kings, brown, sausages]\n",
       "quick          [breakfast, lazy, love]\n",
       "brown         [eggs, quick, beautiful]\n",
       "fox               [jumps, lazy, today]\n",
       "dog              [today, jumps, toast]\n",
       "sausages      [lazy, today, beautiful]\n",
       "ham             [sky, sausages, beans]\n",
       "bacon                [love, eggs, dog]\n",
       "eggs              [lazy, brown, beans]\n",
       "love               [quick, bacon, sky]\n",
       "breakfast         [quick, beans, blue]\n",
       "kings        [toast, beautiful, today]\n",
       "green               [blue, dog, jumps]\n",
       "jumps                  [sky, dog, fox]\n",
       "toast              [kings, beans, dog]\n",
       "beans          [blue, lazy, breakfast]\n",
       "today                [dog, kings, fox]\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(words)\n",
    "similarity_df.apply(lambda row: feature_names[np.argsort(-row.values)[1:4]], \n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTUAkYxQRiqz"
   },
   "source": [
    "# The GloVe Model\n",
    "\n",
    "The GloVe model stands for Global Vectors which is an unsupervised learning model which can be used to obtain dense word vectors similar to Word2Vec. However the technique is different and training is performed on an aggregated global word-word co-occurrence matrix, giving us a vector space with meaningful sub-structures. This method was invented in Stanford by Pennington et al. and I recommend you to read the original paper on GloVe, _[‘GloVe: Global Vectors for Word Representation’ by Pennington et al.](https://nlp.stanford.edu/pubs/glove.pdf)_ which is an excellent read to get some perspective on how this model works.\n",
    "\n",
    "The basic methodology of the GloVe model is to first create a huge word-context co-occurence matrix consisting of (word, context) pairs such that each element in this matrix represents how often a word occurs with the context (which can be a sequence of words). The idea then is to apply matrix factorization to approximate this matrix as depicted in the following figure.\n",
    "\n",
    "![](https://github.com/dipanjanS/nlp_workshop_odsc19/blob/master/Module04%20-%20Text%20Representation/glove_arch.png?raw=1)\n",
    "\n",
    "Considering the __Word-Context (WC)__ matrix, __Word-Feature (WF)__ matrix and __Feature-Context (FC)__ matrix, we try to factorize __WC = WF x FC__\n",
    "\n",
    "Such that we we aim to reconstruct __WC__ from __WF__ and __FC__ by multiplying them. For this, we typically initialize __WF__ and __FC__ with some random weights and attempt to multiply them to get __WC'__ (an approximation of __WC__) and measure how close it is to __WC__. We do this multiple times using Stochastic Gradient Descent (SGD) to minimize the error. Finally, the __Word-Feature matrix (WF)__ gives us the word embeddings for each word where __F__ can be preset to a specific number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_UFyeFqfRiq0"
   },
   "source": [
    "# Robust Glove Model with SpaCy\n",
    "\n",
    "Let’s try and leverage GloVe based embeddings for our document clustering task. The very popular spacy framework comes with capabilities to leverage GloVe embeddings based on different language models. You can also get pre-trained word vectors and load them up as needed using gensim or spacy.\n",
    "\n",
    "If you have spacy installed, we will be using the __[`en_vectors_web_lg`](https://spacy.io/models/en#en_vectors_web_lg)__ model which consists of 300-dimensional word vectors trained on [Common Crawl](http://commoncrawl.org) with GloVe.\n",
    "\n",
    "__Install Instructions:__\n",
    "\n",
    "```\n",
    "# Use the following command to install spaCy\n",
    "> pip install -U spacy\n",
    "OR\n",
    "> conda install -c conda-forge spacy\n",
    "\n",
    "C:\\WINDOWS\\system32>python -m spacy download en_vectors_web_lg\n",
    "Collecting en_vectors_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.0.0/en_vectors_web_lg-2.0.0.tar.gz#egg=en_vectors_web_lg==2.0.0\n",
    "  Downloading https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.0.0/en_vectors_web_lg-2.0.0.tar.gz (661.8MB)\n",
    "    100% |████████████████████████████████| 661.8MB 392kB/s\n",
    "Installing collected packages: en-vectors-web-lg\n",
    "  Running setup.py install for en-vectors-web-lg ... done\n",
    "Successfully installed en-vectors-web-lg-2.0.0\n",
    "You are using pip version 10.0.1, however version 18.0 is available.\n",
    "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
    "\n",
    "    Linking successful\n",
    "    C:\\Anaconda3\\lib\\site-packages\\en_vectors_web_lg -->\n",
    "    C:\\Anaconda3\\lib\\site-packages\\spacy\\data\\en_vectors_web_lg\n",
    "\n",
    "    You can now load the model via spacy.load('en_vectors_web_lg')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "geEhPWzgTA7K",
    "outputId": "925b33eb-8de8-417f-d596-da42af328273"
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "sz4gggzKTkYx",
    "outputId": "1b8aa213-261e-46d5-96a6-3ab1d2ac32d2"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xCfnNf5fRiq0",
    "outputId": "32218af6-78f9-42f1-fcb4-d46673e47c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word vectors: 0\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "total_vectors = len(nlp.vocab.vectors)\n",
    "\n",
    "print('Total word vectors:', total_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Gf4dcBuRiq2"
   },
   "source": [
    "This validates that everything is working and in order. Let’s get the GloVe embeddings for each of our words now in our toy corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "colab_type": "code",
    "id": "sd68_DV0Riq3",
    "outputId": "0beef2e4-3e0e-4290-deee-4163bb2681bc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>-0.550694</td>\n",
       "      <td>0.323250</td>\n",
       "      <td>0.170574</td>\n",
       "      <td>-0.989460</td>\n",
       "      <td>-1.077588</td>\n",
       "      <td>0.651153</td>\n",
       "      <td>1.082611</td>\n",
       "      <td>2.119812</td>\n",
       "      <td>1.009088</td>\n",
       "      <td>-0.445817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543705</td>\n",
       "      <td>-0.307529</td>\n",
       "      <td>-0.447302</td>\n",
       "      <td>-0.414739</td>\n",
       "      <td>-1.110210</td>\n",
       "      <td>0.758873</td>\n",
       "      <td>-0.181222</td>\n",
       "      <td>0.907776</td>\n",
       "      <td>0.149283</td>\n",
       "      <td>-0.602277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>-0.517980</td>\n",
       "      <td>0.543829</td>\n",
       "      <td>1.088808</td>\n",
       "      <td>-0.534416</td>\n",
       "      <td>-0.247349</td>\n",
       "      <td>-0.376343</td>\n",
       "      <td>0.254976</td>\n",
       "      <td>0.110342</td>\n",
       "      <td>1.277124</td>\n",
       "      <td>-0.673650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037811</td>\n",
       "      <td>0.052443</td>\n",
       "      <td>0.367050</td>\n",
       "      <td>0.974690</td>\n",
       "      <td>-0.040317</td>\n",
       "      <td>1.158053</td>\n",
       "      <td>0.360902</td>\n",
       "      <td>0.058626</td>\n",
       "      <td>1.315408</td>\n",
       "      <td>-0.142169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>0.080789</td>\n",
       "      <td>0.473251</td>\n",
       "      <td>0.923093</td>\n",
       "      <td>-0.691717</td>\n",
       "      <td>-0.841625</td>\n",
       "      <td>-0.134446</td>\n",
       "      <td>0.808166</td>\n",
       "      <td>1.263736</td>\n",
       "      <td>1.184073</td>\n",
       "      <td>-0.756468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131687</td>\n",
       "      <td>-0.776639</td>\n",
       "      <td>-0.466794</td>\n",
       "      <td>0.446034</td>\n",
       "      <td>-0.906325</td>\n",
       "      <td>1.206510</td>\n",
       "      <td>-0.242366</td>\n",
       "      <td>0.080054</td>\n",
       "      <td>-0.124095</td>\n",
       "      <td>0.504510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>0.412770</td>\n",
       "      <td>1.091174</td>\n",
       "      <td>0.963708</td>\n",
       "      <td>0.100525</td>\n",
       "      <td>-1.172844</td>\n",
       "      <td>0.017630</td>\n",
       "      <td>0.444518</td>\n",
       "      <td>1.085187</td>\n",
       "      <td>0.581613</td>\n",
       "      <td>-0.486137</td>\n",
       "      <td>...</td>\n",
       "      <td>1.156674</td>\n",
       "      <td>-0.457433</td>\n",
       "      <td>-1.288691</td>\n",
       "      <td>-0.060642</td>\n",
       "      <td>0.328792</td>\n",
       "      <td>0.728755</td>\n",
       "      <td>0.160402</td>\n",
       "      <td>0.149435</td>\n",
       "      <td>-0.311282</td>\n",
       "      <td>0.352830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>1.434052</td>\n",
       "      <td>1.557653</td>\n",
       "      <td>2.167359</td>\n",
       "      <td>-0.582986</td>\n",
       "      <td>-0.840191</td>\n",
       "      <td>0.385024</td>\n",
       "      <td>1.174704</td>\n",
       "      <td>-1.590813</td>\n",
       "      <td>0.057374</td>\n",
       "      <td>1.757742</td>\n",
       "      <td>...</td>\n",
       "      <td>1.180915</td>\n",
       "      <td>0.226474</td>\n",
       "      <td>1.535666</td>\n",
       "      <td>0.926358</td>\n",
       "      <td>-2.009713</td>\n",
       "      <td>0.148794</td>\n",
       "      <td>-1.534109</td>\n",
       "      <td>1.301348</td>\n",
       "      <td>0.307540</td>\n",
       "      <td>1.235014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0.358239</td>\n",
       "      <td>0.477660</td>\n",
       "      <td>0.435286</td>\n",
       "      <td>-0.386845</td>\n",
       "      <td>-0.432213</td>\n",
       "      <td>-0.005646</td>\n",
       "      <td>1.222262</td>\n",
       "      <td>0.408084</td>\n",
       "      <td>1.261542</td>\n",
       "      <td>-0.288424</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.009536</td>\n",
       "      <td>0.156002</td>\n",
       "      <td>0.658086</td>\n",
       "      <td>-0.638026</td>\n",
       "      <td>-0.759796</td>\n",
       "      <td>0.984840</td>\n",
       "      <td>0.044529</td>\n",
       "      <td>0.300104</td>\n",
       "      <td>0.784876</td>\n",
       "      <td>0.116263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.531599</td>\n",
       "      <td>-0.016684</td>\n",
       "      <td>1.700254</td>\n",
       "      <td>0.014643</td>\n",
       "      <td>-0.882773</td>\n",
       "      <td>1.170777</td>\n",
       "      <td>1.542471</td>\n",
       "      <td>0.884013</td>\n",
       "      <td>0.437212</td>\n",
       "      <td>-0.185326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180476</td>\n",
       "      <td>-1.184617</td>\n",
       "      <td>-0.357410</td>\n",
       "      <td>-0.408170</td>\n",
       "      <td>-0.704769</td>\n",
       "      <td>1.675661</td>\n",
       "      <td>-0.032622</td>\n",
       "      <td>-0.654783</td>\n",
       "      <td>0.351540</td>\n",
       "      <td>0.376188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>-0.283319</td>\n",
       "      <td>0.445146</td>\n",
       "      <td>0.810541</td>\n",
       "      <td>-0.707892</td>\n",
       "      <td>-0.938900</td>\n",
       "      <td>0.361050</td>\n",
       "      <td>0.539060</td>\n",
       "      <td>0.217482</td>\n",
       "      <td>0.558082</td>\n",
       "      <td>-0.552150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.481538</td>\n",
       "      <td>-0.459132</td>\n",
       "      <td>0.769351</td>\n",
       "      <td>1.009885</td>\n",
       "      <td>0.485906</td>\n",
       "      <td>1.256341</td>\n",
       "      <td>-0.038165</td>\n",
       "      <td>-0.132174</td>\n",
       "      <td>1.125373</td>\n",
       "      <td>-0.282562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>0.042796</td>\n",
       "      <td>0.631259</td>\n",
       "      <td>1.041934</td>\n",
       "      <td>0.154134</td>\n",
       "      <td>0.690831</td>\n",
       "      <td>-0.456977</td>\n",
       "      <td>-0.403841</td>\n",
       "      <td>0.771655</td>\n",
       "      <td>0.319993</td>\n",
       "      <td>-0.621413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279227</td>\n",
       "      <td>0.625695</td>\n",
       "      <td>0.562677</td>\n",
       "      <td>-0.230811</td>\n",
       "      <td>0.557645</td>\n",
       "      <td>0.889145</td>\n",
       "      <td>-0.023089</td>\n",
       "      <td>-0.163384</td>\n",
       "      <td>0.272499</td>\n",
       "      <td>0.327672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>1.156161</td>\n",
       "      <td>0.369599</td>\n",
       "      <td>0.121756</td>\n",
       "      <td>0.339679</td>\n",
       "      <td>-0.438924</td>\n",
       "      <td>-0.538778</td>\n",
       "      <td>0.628702</td>\n",
       "      <td>0.678033</td>\n",
       "      <td>0.330472</td>\n",
       "      <td>-0.647238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861731</td>\n",
       "      <td>0.198540</td>\n",
       "      <td>0.782854</td>\n",
       "      <td>-0.638970</td>\n",
       "      <td>-0.671303</td>\n",
       "      <td>0.027396</td>\n",
       "      <td>0.818133</td>\n",
       "      <td>0.192981</td>\n",
       "      <td>1.141913</td>\n",
       "      <td>0.476875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>1.515050</td>\n",
       "      <td>0.526029</td>\n",
       "      <td>0.374019</td>\n",
       "      <td>-0.206644</td>\n",
       "      <td>-0.390367</td>\n",
       "      <td>-0.049924</td>\n",
       "      <td>1.175906</td>\n",
       "      <td>0.517972</td>\n",
       "      <td>0.649229</td>\n",
       "      <td>-0.800090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390081</td>\n",
       "      <td>-0.216372</td>\n",
       "      <td>0.854681</td>\n",
       "      <td>-0.185864</td>\n",
       "      <td>-1.260741</td>\n",
       "      <td>0.149061</td>\n",
       "      <td>-0.476886</td>\n",
       "      <td>0.608217</td>\n",
       "      <td>0.500778</td>\n",
       "      <td>0.432574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>1.067028</td>\n",
       "      <td>1.186957</td>\n",
       "      <td>0.986857</td>\n",
       "      <td>-0.500639</td>\n",
       "      <td>-0.808459</td>\n",
       "      <td>-0.371323</td>\n",
       "      <td>1.161645</td>\n",
       "      <td>0.350924</td>\n",
       "      <td>0.318936</td>\n",
       "      <td>0.097112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567409</td>\n",
       "      <td>-0.626836</td>\n",
       "      <td>0.731561</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>-0.767194</td>\n",
       "      <td>0.711032</td>\n",
       "      <td>-0.562781</td>\n",
       "      <td>-0.126742</td>\n",
       "      <td>0.378301</td>\n",
       "      <td>1.171454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>0.084711</td>\n",
       "      <td>0.557376</td>\n",
       "      <td>0.451070</td>\n",
       "      <td>-1.189455</td>\n",
       "      <td>-0.549629</td>\n",
       "      <td>0.035492</td>\n",
       "      <td>0.785811</td>\n",
       "      <td>-0.249312</td>\n",
       "      <td>0.707860</td>\n",
       "      <td>-0.870797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081167</td>\n",
       "      <td>0.552215</td>\n",
       "      <td>0.384991</td>\n",
       "      <td>-0.009349</td>\n",
       "      <td>-0.627743</td>\n",
       "      <td>1.666673</td>\n",
       "      <td>0.260842</td>\n",
       "      <td>-0.206036</td>\n",
       "      <td>1.338707</td>\n",
       "      <td>-0.288019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>1.050502</td>\n",
       "      <td>0.534011</td>\n",
       "      <td>0.425622</td>\n",
       "      <td>-0.123910</td>\n",
       "      <td>-1.261729</td>\n",
       "      <td>-0.091073</td>\n",
       "      <td>0.937743</td>\n",
       "      <td>1.937860</td>\n",
       "      <td>0.867035</td>\n",
       "      <td>-0.499744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415657</td>\n",
       "      <td>-0.102161</td>\n",
       "      <td>-0.997894</td>\n",
       "      <td>-0.041619</td>\n",
       "      <td>-0.055924</td>\n",
       "      <td>0.898503</td>\n",
       "      <td>0.193383</td>\n",
       "      <td>-0.018402</td>\n",
       "      <td>-0.609755</td>\n",
       "      <td>0.526499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>1.200785</td>\n",
       "      <td>0.835411</td>\n",
       "      <td>0.215626</td>\n",
       "      <td>-0.142062</td>\n",
       "      <td>-0.085585</td>\n",
       "      <td>-0.229894</td>\n",
       "      <td>1.612361</td>\n",
       "      <td>0.411321</td>\n",
       "      <td>0.221453</td>\n",
       "      <td>-0.025517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980479</td>\n",
       "      <td>0.283119</td>\n",
       "      <td>0.447617</td>\n",
       "      <td>0.301654</td>\n",
       "      <td>-0.905684</td>\n",
       "      <td>1.575168</td>\n",
       "      <td>-0.171315</td>\n",
       "      <td>0.115446</td>\n",
       "      <td>0.376293</td>\n",
       "      <td>0.837159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>-0.518940</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>1.507880</td>\n",
       "      <td>-0.344073</td>\n",
       "      <td>0.475136</td>\n",
       "      <td>0.407919</td>\n",
       "      <td>0.437827</td>\n",
       "      <td>-0.107507</td>\n",
       "      <td>0.166038</td>\n",
       "      <td>-1.046085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293574</td>\n",
       "      <td>-0.095486</td>\n",
       "      <td>0.096220</td>\n",
       "      <td>1.339754</td>\n",
       "      <td>1.017567</td>\n",
       "      <td>2.334682</td>\n",
       "      <td>-0.453960</td>\n",
       "      <td>-0.124507</td>\n",
       "      <td>0.790154</td>\n",
       "      <td>0.202871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.837651</td>\n",
       "      <td>0.885507</td>\n",
       "      <td>0.719781</td>\n",
       "      <td>-0.277150</td>\n",
       "      <td>-0.107676</td>\n",
       "      <td>-0.423091</td>\n",
       "      <td>1.057672</td>\n",
       "      <td>0.069846</td>\n",
       "      <td>0.187004</td>\n",
       "      <td>-0.731239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353498</td>\n",
       "      <td>-0.676991</td>\n",
       "      <td>0.855486</td>\n",
       "      <td>-0.299651</td>\n",
       "      <td>-0.919849</td>\n",
       "      <td>0.400487</td>\n",
       "      <td>0.413089</td>\n",
       "      <td>0.133089</td>\n",
       "      <td>0.463161</td>\n",
       "      <td>0.271499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>0.828953</td>\n",
       "      <td>1.179940</td>\n",
       "      <td>0.499709</td>\n",
       "      <td>-0.534191</td>\n",
       "      <td>-0.454976</td>\n",
       "      <td>-0.245987</td>\n",
       "      <td>0.365166</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>-0.338075</td>\n",
       "      <td>0.130613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365480</td>\n",
       "      <td>0.757720</td>\n",
       "      <td>1.279614</td>\n",
       "      <td>-0.275212</td>\n",
       "      <td>-1.554348</td>\n",
       "      <td>0.180623</td>\n",
       "      <td>0.300420</td>\n",
       "      <td>0.335078</td>\n",
       "      <td>0.694919</td>\n",
       "      <td>-0.467568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>-0.191471</td>\n",
       "      <td>0.360831</td>\n",
       "      <td>1.030212</td>\n",
       "      <td>-0.554234</td>\n",
       "      <td>0.526235</td>\n",
       "      <td>0.359082</td>\n",
       "      <td>0.570276</td>\n",
       "      <td>-0.087211</td>\n",
       "      <td>-0.129177</td>\n",
       "      <td>-1.029631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556465</td>\n",
       "      <td>0.677949</td>\n",
       "      <td>0.513101</td>\n",
       "      <td>1.785194</td>\n",
       "      <td>0.798586</td>\n",
       "      <td>1.634997</td>\n",
       "      <td>-0.372465</td>\n",
       "      <td>0.069222</td>\n",
       "      <td>0.774663</td>\n",
       "      <td>0.505097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.221221</td>\n",
       "      <td>0.777529</td>\n",
       "      <td>0.795845</td>\n",
       "      <td>0.069223</td>\n",
       "      <td>0.551329</td>\n",
       "      <td>0.734856</td>\n",
       "      <td>0.800853</td>\n",
       "      <td>0.738447</td>\n",
       "      <td>0.476999</td>\n",
       "      <td>-0.563477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.821926</td>\n",
       "      <td>0.020088</td>\n",
       "      <td>1.469363</td>\n",
       "      <td>-0.462293</td>\n",
       "      <td>-1.382615</td>\n",
       "      <td>-0.073822</td>\n",
       "      <td>0.582372</td>\n",
       "      <td>0.358298</td>\n",
       "      <td>0.438726</td>\n",
       "      <td>-0.001920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5   \\\n",
       "jumps     -0.550694  0.323250  0.170574 -0.989460 -1.077588  0.651153   \n",
       "blue      -0.517980  0.543829  1.088808 -0.534416 -0.247349 -0.376343   \n",
       "sausages   0.080789  0.473251  0.923093 -0.691717 -0.841625 -0.134446   \n",
       "kings      0.412770  1.091174  0.963708  0.100525 -1.172844  0.017630   \n",
       "today      1.434052  1.557653  2.167359 -0.582986 -0.840191  0.385024   \n",
       "fox        0.358239  0.477660  0.435286 -0.386845 -0.432213 -0.005646   \n",
       "eggs       0.531599 -0.016684  1.700254  0.014643 -0.882773  1.170777   \n",
       "green     -0.283319  0.445146  0.810541 -0.707892 -0.938900  0.361050   \n",
       "quick      0.042796  0.631259  1.041934  0.154134  0.690831 -0.456977   \n",
       "toast      1.156161  0.369599  0.121756  0.339679 -0.438924 -0.538778   \n",
       "dog        1.515050  0.526029  0.374019 -0.206644 -0.390367 -0.049924   \n",
       "ham        1.067028  1.186957  0.986857 -0.500639 -0.808459 -0.371323   \n",
       "brown      0.084711  0.557376  0.451070 -1.189455 -0.549629  0.035492   \n",
       "beans      1.050502  0.534011  0.425622 -0.123910 -1.261729 -0.091073   \n",
       "sky        1.200785  0.835411  0.215626 -0.142062 -0.085585 -0.229894   \n",
       "lazy      -0.518940  0.005532  1.507880 -0.344073  0.475136  0.407919   \n",
       "bacon      0.837651  0.885507  0.719781 -0.277150 -0.107676 -0.423091   \n",
       "breakfast  0.828953  1.179940  0.499709 -0.534191 -0.454976 -0.245987   \n",
       "beautiful -0.191471  0.360831  1.030212 -0.554234  0.526235  0.359082   \n",
       "love       0.221221  0.777529  0.795845  0.069223  0.551329  0.734856   \n",
       "\n",
       "                 6         7         8         9   ...        86        87  \\\n",
       "jumps      1.082611  2.119812  1.009088 -0.445817  ...  0.543705 -0.307529   \n",
       "blue       0.254976  0.110342  1.277124 -0.673650  ...  0.037811  0.052443   \n",
       "sausages   0.808166  1.263736  1.184073 -0.756468  ...  0.131687 -0.776639   \n",
       "kings      0.444518  1.085187  0.581613 -0.486137  ...  1.156674 -0.457433   \n",
       "today      1.174704 -1.590813  0.057374  1.757742  ...  1.180915  0.226474   \n",
       "fox        1.222262  0.408084  1.261542 -0.288424  ... -1.009536  0.156002   \n",
       "eggs       1.542471  0.884013  0.437212 -0.185326  ...  0.180476 -1.184617   \n",
       "green      0.539060  0.217482  0.558082 -0.552150  ... -0.481538 -0.459132   \n",
       "quick     -0.403841  0.771655  0.319993 -0.621413  ...  0.279227  0.625695   \n",
       "toast      0.628702  0.678033  0.330472 -0.647238  ...  0.861731  0.198540   \n",
       "dog        1.175906  0.517972  0.649229 -0.800090  ... -0.390081 -0.216372   \n",
       "ham        1.161645  0.350924  0.318936  0.097112  ...  0.567409 -0.626836   \n",
       "brown      0.785811 -0.249312  0.707860 -0.870797  ... -0.081167  0.552215   \n",
       "beans      0.937743  1.937860  0.867035 -0.499744  ...  0.415657 -0.102161   \n",
       "sky        1.612361  0.411321  0.221453 -0.025517  ...  0.980479  0.283119   \n",
       "lazy       0.437827 -0.107507  0.166038 -1.046085  ...  0.293574 -0.095486   \n",
       "bacon      1.057672  0.069846  0.187004 -0.731239  ...  0.353498 -0.676991   \n",
       "breakfast  0.365166  0.002383 -0.338075  0.130613  ...  0.365480  0.757720   \n",
       "beautiful  0.570276 -0.087211 -0.129177 -1.029631  ...  0.556465  0.677949   \n",
       "love       0.800853  0.738447  0.476999 -0.563477  ... -0.821926  0.020088   \n",
       "\n",
       "                 88        89        90        91        92        93  \\\n",
       "jumps     -0.447302 -0.414739 -1.110210  0.758873 -0.181222  0.907776   \n",
       "blue       0.367050  0.974690 -0.040317  1.158053  0.360902  0.058626   \n",
       "sausages  -0.466794  0.446034 -0.906325  1.206510 -0.242366  0.080054   \n",
       "kings     -1.288691 -0.060642  0.328792  0.728755  0.160402  0.149435   \n",
       "today      1.535666  0.926358 -2.009713  0.148794 -1.534109  1.301348   \n",
       "fox        0.658086 -0.638026 -0.759796  0.984840  0.044529  0.300104   \n",
       "eggs      -0.357410 -0.408170 -0.704769  1.675661 -0.032622 -0.654783   \n",
       "green      0.769351  1.009885  0.485906  1.256341 -0.038165 -0.132174   \n",
       "quick      0.562677 -0.230811  0.557645  0.889145 -0.023089 -0.163384   \n",
       "toast      0.782854 -0.638970 -0.671303  0.027396  0.818133  0.192981   \n",
       "dog        0.854681 -0.185864 -1.260741  0.149061 -0.476886  0.608217   \n",
       "ham        0.731561  0.003806 -0.767194  0.711032 -0.562781 -0.126742   \n",
       "brown      0.384991 -0.009349 -0.627743  1.666673  0.260842 -0.206036   \n",
       "beans     -0.997894 -0.041619 -0.055924  0.898503  0.193383 -0.018402   \n",
       "sky        0.447617  0.301654 -0.905684  1.575168 -0.171315  0.115446   \n",
       "lazy       0.096220  1.339754  1.017567  2.334682 -0.453960 -0.124507   \n",
       "bacon      0.855486 -0.299651 -0.919849  0.400487  0.413089  0.133089   \n",
       "breakfast  1.279614 -0.275212 -1.554348  0.180623  0.300420  0.335078   \n",
       "beautiful  0.513101  1.785194  0.798586  1.634997 -0.372465  0.069222   \n",
       "love       1.469363 -0.462293 -1.382615 -0.073822  0.582372  0.358298   \n",
       "\n",
       "                 94        95  \n",
       "jumps      0.149283 -0.602277  \n",
       "blue       1.315408 -0.142169  \n",
       "sausages  -0.124095  0.504510  \n",
       "kings     -0.311282  0.352830  \n",
       "today      0.307540  1.235014  \n",
       "fox        0.784876  0.116263  \n",
       "eggs       0.351540  0.376188  \n",
       "green      1.125373 -0.282562  \n",
       "quick      0.272499  0.327672  \n",
       "toast      1.141913  0.476875  \n",
       "dog        0.500778  0.432574  \n",
       "ham        0.378301  1.171454  \n",
       "brown      1.338707 -0.288019  \n",
       "beans     -0.609755  0.526499  \n",
       "sky        0.376293  0.837159  \n",
       "lazy       0.790154  0.202871  \n",
       "bacon      0.463161  0.271499  \n",
       "breakfast  0.694919 -0.467568  \n",
       "beautiful  0.774663  0.505097  \n",
       "love       0.438726 -0.001920  \n",
       "\n",
       "[20 rows x 96 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set([word for sublist in tokenized_corpus for word in sublist]))\n",
    "\n",
    "word_glove_vectors = np.array([nlp(word).vector for word in unique_words])\n",
    "vec_df = pd.DataFrame(word_glove_vectors, index=unique_words)\n",
    "vec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSKGVbkeRiq5"
   },
   "source": [
    "We can now use t-SNE to visualize these embeddings similar to what we did using our Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "j6aSesHGRiq5",
    "outputId": "50fb644e-cbd4-4937-ac07-25b83d2f331c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(word_glove_vectors)\n",
    "labels = unique_words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='red', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ce6BV5bFRiq7"
   },
   "source": [
    "### Looking at term semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "id": "fyO3JzS2Riq8",
    "outputId": "9b88675d-eb0f-4693-b318-a43bddcaeaf2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jumps</th>\n",
       "      <th>blue</th>\n",
       "      <th>sausages</th>\n",
       "      <th>kings</th>\n",
       "      <th>today</th>\n",
       "      <th>fox</th>\n",
       "      <th>eggs</th>\n",
       "      <th>green</th>\n",
       "      <th>quick</th>\n",
       "      <th>toast</th>\n",
       "      <th>dog</th>\n",
       "      <th>ham</th>\n",
       "      <th>brown</th>\n",
       "      <th>beans</th>\n",
       "      <th>sky</th>\n",
       "      <th>lazy</th>\n",
       "      <th>bacon</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.181924</td>\n",
       "      <td>0.314844</td>\n",
       "      <td>0.317616</td>\n",
       "      <td>-0.132332</td>\n",
       "      <td>0.248342</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.149521</td>\n",
       "      <td>0.046726</td>\n",
       "      <td>0.069761</td>\n",
       "      <td>0.145771</td>\n",
       "      <td>0.019794</td>\n",
       "      <td>0.139369</td>\n",
       "      <td>0.482578</td>\n",
       "      <td>0.125971</td>\n",
       "      <td>0.010118</td>\n",
       "      <td>0.122776</td>\n",
       "      <td>-0.025116</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.149058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>0.181924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149752</td>\n",
       "      <td>0.101271</td>\n",
       "      <td>0.096345</td>\n",
       "      <td>0.506338</td>\n",
       "      <td>0.078029</td>\n",
       "      <td>0.761523</td>\n",
       "      <td>0.568334</td>\n",
       "      <td>0.439097</td>\n",
       "      <td>0.354846</td>\n",
       "      <td>0.498772</td>\n",
       "      <td>0.637522</td>\n",
       "      <td>0.150440</td>\n",
       "      <td>0.339026</td>\n",
       "      <td>0.644678</td>\n",
       "      <td>0.503484</td>\n",
       "      <td>0.240265</td>\n",
       "      <td>0.642956</td>\n",
       "      <td>0.368661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>0.314844</td>\n",
       "      <td>0.149752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.616344</td>\n",
       "      <td>0.212717</td>\n",
       "      <td>0.363892</td>\n",
       "      <td>0.722916</td>\n",
       "      <td>0.209037</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.296893</td>\n",
       "      <td>0.475969</td>\n",
       "      <td>0.459501</td>\n",
       "      <td>0.210193</td>\n",
       "      <td>0.772670</td>\n",
       "      <td>0.409535</td>\n",
       "      <td>0.102254</td>\n",
       "      <td>0.481139</td>\n",
       "      <td>0.213103</td>\n",
       "      <td>0.048913</td>\n",
       "      <td>0.220515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>0.317616</td>\n",
       "      <td>0.101271</td>\n",
       "      <td>0.616344</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116353</td>\n",
       "      <td>0.146779</td>\n",
       "      <td>0.650837</td>\n",
       "      <td>0.042585</td>\n",
       "      <td>-0.098122</td>\n",
       "      <td>0.184567</td>\n",
       "      <td>0.256338</td>\n",
       "      <td>0.234385</td>\n",
       "      <td>0.147045</td>\n",
       "      <td>0.724883</td>\n",
       "      <td>0.245350</td>\n",
       "      <td>0.045048</td>\n",
       "      <td>0.334983</td>\n",
       "      <td>0.102142</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>0.072757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>-0.132332</td>\n",
       "      <td>0.096345</td>\n",
       "      <td>0.212717</td>\n",
       "      <td>0.116353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.148346</td>\n",
       "      <td>0.219434</td>\n",
       "      <td>-0.026154</td>\n",
       "      <td>-0.093929</td>\n",
       "      <td>0.343121</td>\n",
       "      <td>0.346048</td>\n",
       "      <td>0.391790</td>\n",
       "      <td>0.051571</td>\n",
       "      <td>0.049565</td>\n",
       "      <td>0.458495</td>\n",
       "      <td>-0.050498</td>\n",
       "      <td>0.176931</td>\n",
       "      <td>0.391791</td>\n",
       "      <td>0.058541</td>\n",
       "      <td>0.203853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0.248342</td>\n",
       "      <td>0.506338</td>\n",
       "      <td>0.363892</td>\n",
       "      <td>0.146779</td>\n",
       "      <td>0.148346</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300774</td>\n",
       "      <td>0.489812</td>\n",
       "      <td>0.241975</td>\n",
       "      <td>0.459260</td>\n",
       "      <td>0.500825</td>\n",
       "      <td>0.554450</td>\n",
       "      <td>0.587346</td>\n",
       "      <td>0.381149</td>\n",
       "      <td>0.479465</td>\n",
       "      <td>0.253543</td>\n",
       "      <td>0.642072</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.269856</td>\n",
       "      <td>0.653546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.078029</td>\n",
       "      <td>0.722916</td>\n",
       "      <td>0.650837</td>\n",
       "      <td>0.219434</td>\n",
       "      <td>0.300774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.114935</td>\n",
       "      <td>0.038423</td>\n",
       "      <td>0.284644</td>\n",
       "      <td>0.386576</td>\n",
       "      <td>0.384320</td>\n",
       "      <td>0.203709</td>\n",
       "      <td>0.568352</td>\n",
       "      <td>0.387275</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>0.475107</td>\n",
       "      <td>0.284669</td>\n",
       "      <td>0.084605</td>\n",
       "      <td>0.292152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.149521</td>\n",
       "      <td>0.761523</td>\n",
       "      <td>0.209037</td>\n",
       "      <td>0.042585</td>\n",
       "      <td>-0.026154</td>\n",
       "      <td>0.489812</td>\n",
       "      <td>0.114935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.528082</td>\n",
       "      <td>0.319099</td>\n",
       "      <td>0.306343</td>\n",
       "      <td>0.412775</td>\n",
       "      <td>0.721183</td>\n",
       "      <td>0.191504</td>\n",
       "      <td>0.160340</td>\n",
       "      <td>0.719814</td>\n",
       "      <td>0.402737</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>0.657476</td>\n",
       "      <td>0.275326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>0.046726</td>\n",
       "      <td>0.568334</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>-0.098122</td>\n",
       "      <td>-0.093929</td>\n",
       "      <td>0.241975</td>\n",
       "      <td>0.038423</td>\n",
       "      <td>0.528082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.369648</td>\n",
       "      <td>0.164281</td>\n",
       "      <td>0.289204</td>\n",
       "      <td>0.398579</td>\n",
       "      <td>0.071079</td>\n",
       "      <td>0.276017</td>\n",
       "      <td>0.591375</td>\n",
       "      <td>0.299811</td>\n",
       "      <td>0.262249</td>\n",
       "      <td>0.716110</td>\n",
       "      <td>0.282164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>0.069761</td>\n",
       "      <td>0.439097</td>\n",
       "      <td>0.296893</td>\n",
       "      <td>0.184567</td>\n",
       "      <td>0.343121</td>\n",
       "      <td>0.459260</td>\n",
       "      <td>0.284644</td>\n",
       "      <td>0.319099</td>\n",
       "      <td>0.369648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.594936</td>\n",
       "      <td>0.572171</td>\n",
       "      <td>0.402966</td>\n",
       "      <td>0.241999</td>\n",
       "      <td>0.629639</td>\n",
       "      <td>0.308049</td>\n",
       "      <td>0.624400</td>\n",
       "      <td>0.583615</td>\n",
       "      <td>0.362859</td>\n",
       "      <td>0.579866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.145771</td>\n",
       "      <td>0.354846</td>\n",
       "      <td>0.475969</td>\n",
       "      <td>0.256338</td>\n",
       "      <td>0.346048</td>\n",
       "      <td>0.500825</td>\n",
       "      <td>0.386576</td>\n",
       "      <td>0.306343</td>\n",
       "      <td>0.164281</td>\n",
       "      <td>0.594936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.530753</td>\n",
       "      <td>0.309427</td>\n",
       "      <td>0.329620</td>\n",
       "      <td>0.658686</td>\n",
       "      <td>0.179627</td>\n",
       "      <td>0.655660</td>\n",
       "      <td>0.606019</td>\n",
       "      <td>0.178229</td>\n",
       "      <td>0.577704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.019794</td>\n",
       "      <td>0.498772</td>\n",
       "      <td>0.459501</td>\n",
       "      <td>0.234385</td>\n",
       "      <td>0.391790</td>\n",
       "      <td>0.554450</td>\n",
       "      <td>0.384320</td>\n",
       "      <td>0.412775</td>\n",
       "      <td>0.289204</td>\n",
       "      <td>0.572171</td>\n",
       "      <td>0.530753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.345543</td>\n",
       "      <td>0.314201</td>\n",
       "      <td>0.695027</td>\n",
       "      <td>0.188068</td>\n",
       "      <td>0.634326</td>\n",
       "      <td>0.432130</td>\n",
       "      <td>0.264724</td>\n",
       "      <td>0.389224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>0.139369</td>\n",
       "      <td>0.637522</td>\n",
       "      <td>0.210193</td>\n",
       "      <td>0.147045</td>\n",
       "      <td>0.051571</td>\n",
       "      <td>0.587346</td>\n",
       "      <td>0.203709</td>\n",
       "      <td>0.721183</td>\n",
       "      <td>0.398579</td>\n",
       "      <td>0.402966</td>\n",
       "      <td>0.309427</td>\n",
       "      <td>0.345543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.238603</td>\n",
       "      <td>0.283375</td>\n",
       "      <td>0.646367</td>\n",
       "      <td>0.418950</td>\n",
       "      <td>0.157745</td>\n",
       "      <td>0.588541</td>\n",
       "      <td>0.336810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>0.482578</td>\n",
       "      <td>0.150440</td>\n",
       "      <td>0.772670</td>\n",
       "      <td>0.724883</td>\n",
       "      <td>0.049565</td>\n",
       "      <td>0.381149</td>\n",
       "      <td>0.568352</td>\n",
       "      <td>0.191504</td>\n",
       "      <td>0.071079</td>\n",
       "      <td>0.241999</td>\n",
       "      <td>0.329620</td>\n",
       "      <td>0.314201</td>\n",
       "      <td>0.238603</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.313956</td>\n",
       "      <td>0.078852</td>\n",
       "      <td>0.410435</td>\n",
       "      <td>0.061709</td>\n",
       "      <td>0.093073</td>\n",
       "      <td>0.191778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>0.125971</td>\n",
       "      <td>0.339026</td>\n",
       "      <td>0.409535</td>\n",
       "      <td>0.245350</td>\n",
       "      <td>0.458495</td>\n",
       "      <td>0.479465</td>\n",
       "      <td>0.387275</td>\n",
       "      <td>0.160340</td>\n",
       "      <td>0.276017</td>\n",
       "      <td>0.629639</td>\n",
       "      <td>0.658686</td>\n",
       "      <td>0.695027</td>\n",
       "      <td>0.283375</td>\n",
       "      <td>0.313956</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170488</td>\n",
       "      <td>0.580788</td>\n",
       "      <td>0.605003</td>\n",
       "      <td>0.289573</td>\n",
       "      <td>0.490442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>0.010118</td>\n",
       "      <td>0.644678</td>\n",
       "      <td>0.102254</td>\n",
       "      <td>0.045048</td>\n",
       "      <td>-0.050498</td>\n",
       "      <td>0.253543</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>0.719814</td>\n",
       "      <td>0.591375</td>\n",
       "      <td>0.308049</td>\n",
       "      <td>0.179627</td>\n",
       "      <td>0.188068</td>\n",
       "      <td>0.646367</td>\n",
       "      <td>0.078852</td>\n",
       "      <td>0.170488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.273282</td>\n",
       "      <td>0.100425</td>\n",
       "      <td>0.738642</td>\n",
       "      <td>0.190721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.122776</td>\n",
       "      <td>0.503484</td>\n",
       "      <td>0.481139</td>\n",
       "      <td>0.334983</td>\n",
       "      <td>0.176931</td>\n",
       "      <td>0.642072</td>\n",
       "      <td>0.475107</td>\n",
       "      <td>0.402737</td>\n",
       "      <td>0.299811</td>\n",
       "      <td>0.624400</td>\n",
       "      <td>0.655660</td>\n",
       "      <td>0.634326</td>\n",
       "      <td>0.418950</td>\n",
       "      <td>0.410435</td>\n",
       "      <td>0.580788</td>\n",
       "      <td>0.273282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.565241</td>\n",
       "      <td>0.387486</td>\n",
       "      <td>0.670329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>-0.025116</td>\n",
       "      <td>0.240265</td>\n",
       "      <td>0.213103</td>\n",
       "      <td>0.102142</td>\n",
       "      <td>0.391791</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.284669</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>0.262249</td>\n",
       "      <td>0.583615</td>\n",
       "      <td>0.606019</td>\n",
       "      <td>0.432130</td>\n",
       "      <td>0.157745</td>\n",
       "      <td>0.061709</td>\n",
       "      <td>0.605003</td>\n",
       "      <td>0.100425</td>\n",
       "      <td>0.565241</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167038</td>\n",
       "      <td>0.532718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.642956</td>\n",
       "      <td>0.048913</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>0.058541</td>\n",
       "      <td>0.269856</td>\n",
       "      <td>0.084605</td>\n",
       "      <td>0.657476</td>\n",
       "      <td>0.716110</td>\n",
       "      <td>0.362859</td>\n",
       "      <td>0.178229</td>\n",
       "      <td>0.264724</td>\n",
       "      <td>0.588541</td>\n",
       "      <td>0.093073</td>\n",
       "      <td>0.289573</td>\n",
       "      <td>0.738642</td>\n",
       "      <td>0.387486</td>\n",
       "      <td>0.167038</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.292573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.149058</td>\n",
       "      <td>0.368661</td>\n",
       "      <td>0.220515</td>\n",
       "      <td>0.072757</td>\n",
       "      <td>0.203853</td>\n",
       "      <td>0.653546</td>\n",
       "      <td>0.292152</td>\n",
       "      <td>0.275326</td>\n",
       "      <td>0.282164</td>\n",
       "      <td>0.579866</td>\n",
       "      <td>0.577704</td>\n",
       "      <td>0.389224</td>\n",
       "      <td>0.336810</td>\n",
       "      <td>0.191778</td>\n",
       "      <td>0.490442</td>\n",
       "      <td>0.190721</td>\n",
       "      <td>0.670329</td>\n",
       "      <td>0.532718</td>\n",
       "      <td>0.292573</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              jumps      blue  sausages     kings     today       fox  \\\n",
       "jumps      1.000000  0.181924  0.314844  0.317616 -0.132332  0.248342   \n",
       "blue       0.181924  1.000000  0.149752  0.101271  0.096345  0.506338   \n",
       "sausages   0.314844  0.149752  1.000000  0.616344  0.212717  0.363892   \n",
       "kings      0.317616  0.101271  0.616344  1.000000  0.116353  0.146779   \n",
       "today     -0.132332  0.096345  0.212717  0.116353  1.000000  0.148346   \n",
       "fox        0.248342  0.506338  0.363892  0.146779  0.148346  1.000000   \n",
       "eggs       0.133100  0.078029  0.722916  0.650837  0.219434  0.300774   \n",
       "green      0.149521  0.761523  0.209037  0.042585 -0.026154  0.489812   \n",
       "quick      0.046726  0.568334  0.006799 -0.098122 -0.093929  0.241975   \n",
       "toast      0.069761  0.439097  0.296893  0.184567  0.343121  0.459260   \n",
       "dog        0.145771  0.354846  0.475969  0.256338  0.346048  0.500825   \n",
       "ham        0.019794  0.498772  0.459501  0.234385  0.391790  0.554450   \n",
       "brown      0.139369  0.637522  0.210193  0.147045  0.051571  0.587346   \n",
       "beans      0.482578  0.150440  0.772670  0.724883  0.049565  0.381149   \n",
       "sky        0.125971  0.339026  0.409535  0.245350  0.458495  0.479465   \n",
       "lazy       0.010118  0.644678  0.102254  0.045048 -0.050498  0.253543   \n",
       "bacon      0.122776  0.503484  0.481139  0.334983  0.176931  0.642072   \n",
       "breakfast -0.025116  0.240265  0.213103  0.102142  0.391791  0.293300   \n",
       "beautiful  0.061990  0.642956  0.048913  0.014945  0.058541  0.269856   \n",
       "love       0.149058  0.368661  0.220515  0.072757  0.203853  0.653546   \n",
       "\n",
       "               eggs     green     quick     toast       dog       ham  \\\n",
       "jumps      0.133100  0.149521  0.046726  0.069761  0.145771  0.019794   \n",
       "blue       0.078029  0.761523  0.568334  0.439097  0.354846  0.498772   \n",
       "sausages   0.722916  0.209037  0.006799  0.296893  0.475969  0.459501   \n",
       "kings      0.650837  0.042585 -0.098122  0.184567  0.256338  0.234385   \n",
       "today      0.219434 -0.026154 -0.093929  0.343121  0.346048  0.391790   \n",
       "fox        0.300774  0.489812  0.241975  0.459260  0.500825  0.554450   \n",
       "eggs       1.000000  0.114935  0.038423  0.284644  0.386576  0.384320   \n",
       "green      0.114935  1.000000  0.528082  0.319099  0.306343  0.412775   \n",
       "quick      0.038423  0.528082  1.000000  0.369648  0.164281  0.289204   \n",
       "toast      0.284644  0.319099  0.369648  1.000000  0.594936  0.572171   \n",
       "dog        0.386576  0.306343  0.164281  0.594936  1.000000  0.530753   \n",
       "ham        0.384320  0.412775  0.289204  0.572171  0.530753  1.000000   \n",
       "brown      0.203709  0.721183  0.398579  0.402966  0.309427  0.345543   \n",
       "beans      0.568352  0.191504  0.071079  0.241999  0.329620  0.314201   \n",
       "sky        0.387275  0.160340  0.276017  0.629639  0.658686  0.695027   \n",
       "lazy       0.134831  0.719814  0.591375  0.308049  0.179627  0.188068   \n",
       "bacon      0.475107  0.402737  0.299811  0.624400  0.655660  0.634326   \n",
       "breakfast  0.284669  0.089419  0.262249  0.583615  0.606019  0.432130   \n",
       "beautiful  0.084605  0.657476  0.716110  0.362859  0.178229  0.264724   \n",
       "love       0.292152  0.275326  0.282164  0.579866  0.577704  0.389224   \n",
       "\n",
       "              brown     beans       sky      lazy     bacon  breakfast  \\\n",
       "jumps      0.139369  0.482578  0.125971  0.010118  0.122776  -0.025116   \n",
       "blue       0.637522  0.150440  0.339026  0.644678  0.503484   0.240265   \n",
       "sausages   0.210193  0.772670  0.409535  0.102254  0.481139   0.213103   \n",
       "kings      0.147045  0.724883  0.245350  0.045048  0.334983   0.102142   \n",
       "today      0.051571  0.049565  0.458495 -0.050498  0.176931   0.391791   \n",
       "fox        0.587346  0.381149  0.479465  0.253543  0.642072   0.293300   \n",
       "eggs       0.203709  0.568352  0.387275  0.134831  0.475107   0.284669   \n",
       "green      0.721183  0.191504  0.160340  0.719814  0.402737   0.089419   \n",
       "quick      0.398579  0.071079  0.276017  0.591375  0.299811   0.262249   \n",
       "toast      0.402966  0.241999  0.629639  0.308049  0.624400   0.583615   \n",
       "dog        0.309427  0.329620  0.658686  0.179627  0.655660   0.606019   \n",
       "ham        0.345543  0.314201  0.695027  0.188068  0.634326   0.432130   \n",
       "brown      1.000000  0.238603  0.283375  0.646367  0.418950   0.157745   \n",
       "beans      0.238603  1.000000  0.313956  0.078852  0.410435   0.061709   \n",
       "sky        0.283375  0.313956  1.000000  0.170488  0.580788   0.605003   \n",
       "lazy       0.646367  0.078852  0.170488  1.000000  0.273282   0.100425   \n",
       "bacon      0.418950  0.410435  0.580788  0.273282  1.000000   0.565241   \n",
       "breakfast  0.157745  0.061709  0.605003  0.100425  0.565241   1.000000   \n",
       "beautiful  0.588541  0.093073  0.289573  0.738642  0.387486   0.167038   \n",
       "love       0.336810  0.191778  0.490442  0.190721  0.670329   0.532718   \n",
       "\n",
       "           beautiful      love  \n",
       "jumps       0.061990  0.149058  \n",
       "blue        0.642956  0.368661  \n",
       "sausages    0.048913  0.220515  \n",
       "kings       0.014945  0.072757  \n",
       "today       0.058541  0.203853  \n",
       "fox         0.269856  0.653546  \n",
       "eggs        0.084605  0.292152  \n",
       "green       0.657476  0.275326  \n",
       "quick       0.716110  0.282164  \n",
       "toast       0.362859  0.579866  \n",
       "dog         0.178229  0.577704  \n",
       "ham         0.264724  0.389224  \n",
       "brown       0.588541  0.336810  \n",
       "beans       0.093073  0.191778  \n",
       "sky         0.289573  0.490442  \n",
       "lazy        0.738642  0.190721  \n",
       "bacon       0.387486  0.670329  \n",
       "breakfast   0.167038  0.532718  \n",
       "beautiful   1.000000  0.292573  \n",
       "love        0.292573  1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(vec_df.values)\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=unique_words, columns=unique_words)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "JC-aQ1IERiq9",
    "outputId": "caead4fe-3923-423f-9b0f-fbe71bcc6097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jumps         [beans, kings, sausages]\n",
       "blue          [green, lazy, beautiful]\n",
       "sausages          [beans, eggs, kings]\n",
       "kings          [beans, eggs, sausages]\n",
       "today            [sky, breakfast, ham]\n",
       "fox               [love, bacon, brown]\n",
       "eggs          [sausages, kings, beans]\n",
       "green              [blue, brown, lazy]\n",
       "quick          [beautiful, lazy, blue]\n",
       "toast                [sky, bacon, dog]\n",
       "dog            [sky, bacon, breakfast]\n",
       "ham                [sky, bacon, toast]\n",
       "brown              [green, lazy, blue]\n",
       "beans          [sausages, kings, eggs]\n",
       "sky                  [ham, dog, toast]\n",
       "lazy         [beautiful, green, brown]\n",
       "bacon                 [love, dog, fox]\n",
       "breakfast            [dog, sky, toast]\n",
       "beautiful         [lazy, quick, green]\n",
       "love               [bacon, fox, toast]\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(unique_words)\n",
    "similarity_df.apply(lambda row: feature_names[np.argsort(-row.values)[1:4]], \n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pU1BAEUWRiq_"
   },
   "source": [
    "# The FastText Model\n",
    "\n",
    "The FastText model was first introduced by Facebook in 2016 as an extension and supposedly improvement of the vanilla Word2Vec model. Based on the original paper titled _[‘Enriching Word Vectors with Subword Information’](https://arxiv.org/pdf/1607.04606.pdf)_ by Mikolov et al. which is an excellent read to gain an in-depth understanding of how this model works. Overall, FastText is a framework for learning word representations and also performing robust, fast and accurate text classification. The framework is open-sourced by Facebook on [GitHub](https://github.com/facebookresearch/fastText) and claims to have the following.\n",
    "\n",
    "- Recent state-of-the-art English word vectors.\n",
    "- Word vectors for 157 languages trained on Wikipedia and Crawl.\n",
    "- Models for language identification and various supervised tasks.\n",
    "\n",
    "Though I haven't implemented this model from scratch, based on the research paper, following is what I learnt about how the model works. In general, predictive models like the Word2Vec model typically considers each word as a distinct entity (e.g. where) and generates a dense embedding for the word. However this poses to be a serious limitation with languages having massive vocabularies and many rare words which may not occur a lot in different corpora. \n",
    "\n",
    "The Word2Vec model typically ignores the morphological structure of each word and considers a word as a single entity. The FastText model ___considers each word as a Bag of Character n-grams___. This is also called as a ___subword model___ in the paper.\n",
    "\n",
    "We add special boundary symbols __<__ and __>__ at the beginning and end of words. This enables us to distinguish prefixes and suffixes from other character sequences. We also include the word __w__ itself in the set of its n-grams, to learn a representation for each word (in addition to its character n-grams). \n",
    "\n",
    "Taking the word where and __n=3 (tri-grams)__ as an example, it will be represented by the __character n-grams__: __<wh, whe, her, ere, re>__ and the special sequence __< where >__ representing the whole word. Note that the sequence , corresponding to the word __< her >__ is different from the tri-gram __her__ from the word __where__.\n",
    "\n",
    "In practice, the paper recommends in extracting all the n-grams for __n ≥ 3__ and __n ≤ 6__. This is a very simple approach, and different sets of n-grams could be considered, for example taking all prefixes and suffixes. We typically associate a vector representation (embedding) to each n-gram for a word. \n",
    "\n",
    "Thus, we can represent a word by the sum of the vector representations of its n-grams or the average of the embedding of these n-grams. Thus, due to this effect of leveraging n-grams from individual words based on their characters, there is a higher chance for rare words to get a good representation since their character based n-grams should occur across other words of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTpq6CyxRiq_"
   },
   "source": [
    "# Robust FastText Model with Gensim\n",
    "\n",
    "The __`gensim`__ package has nice wrappers providing us interfaces to leverage the FastText model available under the `gensim.models.fasttext` module. Let’s apply this once again on our toy corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qzrr2AiJRirA",
    "outputId": "953c4f62-98e4-42fc-f68d-dda2bc6c5989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.fasttext.FastText at 0x1e820e519c8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 15    # Word vector dimensionality  \n",
    "window_context = 20  # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "sg = 1               # skip-gram model\n",
    "\n",
    "ft_model = FastText(tokenized_corpus,  \n",
    "                     window=window_context, min_count = min_word_count,\n",
    "                     sg=sg, sample=sample)\n",
    "ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "iR6sYaASRirB",
    "outputId": "233613d4-3851-4b02-dc33-1ff52f081948"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = ft_model.wv.index_to_key\n",
    "wvs = ft_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=5)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='green', edgecolors='k')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzqhdlSpbY8s"
   },
   "source": [
    "## Embedding Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_LplYuHBRirE",
    "outputId": "00a2d159-159b-4e65-c398-464ffdd3f53c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00221835, -0.00319054,  0.00277566,  0.00102296, -0.00153656,\n",
       "        -0.00163024, -0.00231491,  0.00240707, -0.00048566,  0.00098691,\n",
       "        -0.00085169,  0.00126206, -0.00265305,  0.00327311,  0.00120383,\n",
       "        -0.00019749,  0.00075437,  0.00002203, -0.0023516 ,  0.00075234,\n",
       "         0.00159587, -0.0020277 , -0.00128701, -0.0006478 ,  0.00230742,\n",
       "        -0.00089073,  0.00187692, -0.00030547,  0.00170599,  0.00128906,\n",
       "        -0.00051703, -0.00038594,  0.00242666,  0.00100433, -0.00119173,\n",
       "         0.00170316,  0.00242299, -0.00012247, -0.00204472,  0.00158365,\n",
       "        -0.00056463,  0.00147082,  0.00026043, -0.00049712,  0.00064704,\n",
       "        -0.00022024, -0.00114704,  0.00118757,  0.000212  , -0.00046281,\n",
       "         0.00192766,  0.00272246, -0.0024699 , -0.00020753,  0.00364099,\n",
       "        -0.00227419,  0.00199629, -0.00278769,  0.00419264,  0.00065458,\n",
       "         0.00161865, -0.00176483, -0.0050681 ,  0.0039457 ,  0.00419691,\n",
       "        -0.00438108,  0.00179408, -0.00103201, -0.00248051,  0.00057169,\n",
       "         0.00197521,  0.00369702, -0.00132672, -0.00333336, -0.00037761,\n",
       "        -0.00063061,  0.00151157, -0.00074134, -0.00047029, -0.00166475,\n",
       "        -0.00112765,  0.00042481,  0.00317388, -0.00254596, -0.00021362,\n",
       "         0.00077707,  0.00043711, -0.00418357,  0.00145694,  0.00046335,\n",
       "        -0.00201655, -0.00085768,  0.00426288, -0.00101287, -0.00393895,\n",
       "        -0.00113271,  0.00245984, -0.0010701 ,  0.00095116,  0.00103001],\n",
       "       dtype=float32),\n",
       " (100,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.wv['sky'], ft_model.wv['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "40z0JRH5RirG",
    "outputId": "4a3f3b54-7f35-41c5-f7b2-5738421a029b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06948187\n",
      "0.034941223\n"
     ]
    }
   ],
   "source": [
    "print(ft_model.wv.similarity(w1='ham', w2='sky'))\n",
    "print(ft_model.wv.similarity(w1='ham', w2='sausages'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "7iwDNr-lRirI",
    "outputId": "c8988302-e591-4264-d174-5086d703b0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd one out for [ dog fox ham ]: ham\n",
      "Odd one out for [ bacon ham sky sausages ]: ham\n"
     ]
    }
   ],
   "source": [
    "st1 = \"dog fox ham\"\n",
    "print('Odd one out for [',st1, ']:',  \n",
    "      ft_model.wv.doesnt_match(st1.split()))\n",
    "\n",
    "st2 = \"bacon ham sky sausages\"\n",
    "print('Odd one out for [',st2, ']:', \n",
    "      ft_model.wv.doesnt_match(st2.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QxZeE7w8RirK"
   },
   "source": [
    "### Getting document level embeddings\n",
    "\n",
    "Now suppose we wanted to cluster the eight documents from our toy corpus, we would need to get the document level embeddings from each of the words present in each document. One strategy would be to average out the word embeddings for each word in a document. This is an extremely useful strategy and you can adopt the same for your own problems. Let’s apply this now on our corpus to get features for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8ETS-PxRirK"
   },
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "MPFoB8y5RirM",
    "outputId": "c6073b7d-b71e-4496-e325-805d3ccacad4"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (15,) (100,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/1901441932.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get document level embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m ft_doc_features = averaged_word_vectorizer(corpus=tokenized_corpus, model=ft_model,\n\u001b[1;32m----> 3\u001b[1;33m                                              num_features=feature_size)\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mft_doc_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/3001449133.py\u001b[0m in \u001b[0;36maveraged_word_vectorizer\u001b[1;34m(corpus, model, num_features)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n\u001b[1;32m---> 20\u001b[1;33m                     for tokenized_sentence in corpus]\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/3001449133.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n\u001b[1;32m---> 20\u001b[1;33m                     for tokenized_sentence in corpus]\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/3001449133.py\u001b[0m in \u001b[0;36maverage_word_vectors\u001b[1;34m(words, model, vocabulary, num_features)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mnwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnwords\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mfeature_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (15,) (100,) "
     ]
    }
   ],
   "source": [
    "# get document level embeddings\n",
    "ft_doc_features = averaged_word_vectorizer(corpus=tokenized_corpus, model=ft_model,\n",
    "                                             num_features=feature_size)\n",
    "pd.DataFrame(ft_doc_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgrKl_vqRirN"
   },
   "source": [
    "### Trying out document clustering\n",
    "\n",
    "Now that we have our features for each document, let’s cluster these documents using the Affinity Propagation algorithm, which is a clustering algorithm based on the concept of “message passing” between data points and does not need the number of clusters as an explicit input which is often required by partition-based clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "n9C0aRLwRirO",
    "outputId": "cfe31134-897f-4f74-cae9-1f9529c09786"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "ap = AffinityPropagation()\n",
    "ap.fit(ft_doc_features)\n",
    "\n",
    "cluster_labels = ap.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, \n",
    "                              columns=['ClusterLabel'])\n",
    "\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bB6pQQTPRirQ"
   },
   "source": [
    "We can see that our algorithm has clustered each document into the right group based on our Word2Vec features. Pretty neat! We can also visualize how each document in positioned in each cluster by using [_Principal Component Analysis (PCA)_](https://en.wikipedia.org/wiki/Principal_component_analysis) to reduce the feature dimensions to 2-D and then visualizing the same (by color coding each cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "3VA6oJnIRirR",
    "outputId": "05242d98-b335-44c6-f234-d3c067a16bfa"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pcs = pca.fit_transform(ft_doc_features)\n",
    "labels = ap.labels_\n",
    "categories = list(corpus_df['Category'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    color = 'orange' if label == 0 else 'blue' if label == 1 else 'green'\n",
    "    annotation_label = categories[i]\n",
    "    x, y = pcs[i]\n",
    "    plt.scatter(x, y, c=color, edgecolors='k')\n",
    "    plt.annotate(annotation_label, xy=(x+1e-2, y+1e-2), xytext=(0, 0), \n",
    "                 textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvFiGdkMRirS"
   },
   "source": [
    "Everything looks to be in order as documents in each cluster are closer to each other and far apart from other clusters."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03 -Text Representation - Embedding Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
