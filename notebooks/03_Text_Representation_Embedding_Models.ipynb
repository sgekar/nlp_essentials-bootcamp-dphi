{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JS9QXHRoRiqc"
   },
   "source": [
    "# Text Representation with Feature Engineering\n",
    "\n",
    "### Exploring Word Embeddings with New Deep Learning Models\n",
    "\n",
    "We have discussed in the previous sub-unit that Feature Engineering is the secret sauce to creating superior and better performing machine learning models. \n",
    "\n",
    "Traditional (count-based) feature engineering strategies for textual data involve models belonging to a family of models popularly known as the Bag of Words model. This includes term frequencies, TF-IDF (term frequency-inverse document frequency), N-grams and so on. While they are effective methods for extracting features from text, due to the inherent nature of the model being just a bag of unstructured words, we lose additional information like the semantics, structure, sequence and context around nearby words in each text document. \n",
    "\n",
    "This forms as enough motivation for us to explore more sophisticated models which can capture this information and give us features which are vector representation of words, popularly known as embeddings.\n",
    "\n",
    "Here we will explore the following feature engineering techniques:\n",
    "\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "\n",
    "Predictive methods like Neural Network based language models try to predict words from its neighboring words looking at word sequences in the corpus and in the process it learns distributed representations giving us dense word embeddings. We will be focusing on these predictive methods in this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYUGSn3dRiqd"
   },
   "source": [
    "# Prepare a Sample Corpus\n",
    "\n",
    "Let’s now take a sample corpus of documents on which we will run most of our analyses in this article. A corpus is typically a collection of text documents usually belonging to one or more subjects or domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "KH4ZwBgtRiqe",
    "outputId": "c285dcff-5578-4fa8-9ecf-63dba9b4f7ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Document Category\n",
       "0                                      The sky is blue and beautiful.  weather\n",
       "1                                   Love this blue and beautiful sky!  weather\n",
       "2                        The quick brown fox jumps over the lazy dog.  animals\n",
       "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
       "4                         I love green eggs, ham, sausages and bacon!     food\n",
       "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
       "6            The sky is very blue and the sky is very beautiful today  weather\n",
       "7                         The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLojJzK0Riqi"
   },
   "source": [
    "Let's go ahead and pre-process our text data now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7KR8w3qbRiqi"
   },
   "source": [
    "# Simple Text Pre-processing\n",
    "\n",
    "Since the focus of this unit is on feature engineering, we will build a simple text pre-processor which focuses on removing special characters, extra whitespaces, digits, stopwords and lower casing the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "5YHHrL7VRiqj",
    "outputId": "114e4855-473e-45da-a614-88cc52a190a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\peshw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peshw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog',\n",
       "       'kings breakfast sausages ham bacon eggs toast beans',\n",
       "       'love green eggs ham sausages bacon',\n",
       "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
       "       'dog lazy brown fox quick'], dtype='<U51')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JCyCJ6GRiql"
   },
   "source": [
    "# The Word2Vec Model\n",
    "\n",
    "This model was created by Google in 2013 and is a predictive deep learning based model to compute and generate high quality, distributed and continuous dense vector representations of words, which capture contextual and semantic similarity. Essentially these are unsupervised models which can take in massive textual corpora, create a vocabulary of possible words and generate dense word embeddings for each word in the vector space representing that vocabulary. \n",
    "\n",
    "Usually you can specify the size of the word embedding vectors and the total number of vectors are essentially the size of the vocabulary. This makes the dimensionality of this dense vector space much lower than the high-dimensional sparse vector space built using traditional Bag of Words models.\n",
    "\n",
    "There are two different model architectures which can be leveraged by Word2Vec to create these word embedding representations. These include,\n",
    "\n",
    "- The Continuous Bag of Words (CBOW) Model\n",
    "- The Skip-gram Model\n",
    "\n",
    "## The Continuous Bag of Words (CBOW) Model\n",
    "\n",
    "The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words). \n",
    "\n",
    "Considering a simple sentence, ___“the quick brown fox jumps over the lazy dog”___, this can be pairs of __(context_window, target_word)__ where if we consider a context window of size 2, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on. \n",
    "\n",
    "Thus the model tries to predict the __`target_word`__ based on the __`context_window`__ words.\n",
    "\n",
    "![](https://github.com/dipanjanS/nlp_workshop_odsc19/blob/master/Module04%20-%20Text%20Representation/cbow_arch.png?raw=1)\n",
    "\n",
    "\n",
    "## The Skip-gram Model\n",
    "\n",
    "The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word). \n",
    "\n",
    "Considering our simple sentence from earlier, ___“the quick brown fox jumps over the lazy dog”___. If we used the CBOW model, we get pairs of __(context_window, target_word)__ where if we consider a context window of size 2, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on. \n",
    "\n",
    "Now considering that the skip-gram model’s aim is to predict the context from the target word, the model typically inverts the contexts and targets, and tries to predict each context word from its target word. Hence the task becomes to predict the context __[quick, fox]__ given target word __‘brown’__ or __[the, brown]__ given target word __‘quick’__ and so on. \n",
    "\n",
    "Thus the model tries to predict the context_window words based on the target_word.\n",
    "\n",
    "![](https://github.com/dipanjanS/nlp_workshop_odsc19/blob/master/Module04%20-%20Text%20Representation/skipgram_arch.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHD8pUx7Riqm"
   },
   "source": [
    "# Robust Word2Vec Model with Gensim\n",
    "\n",
    "The __`gensim`__ framework, created by Radim Řehůřek consists of a robust, efficient and scalable implementation of the Word2Vec model. We will leverage the same on our sample toy corpus. In our workflow, we will tokenize our normalized corpus and then focus on the following four parameters in the Word2Vec model to build it.\n",
    "\n",
    "- __`size`:__ The word embedding dimensionality\n",
    "- __`window`:__ The context window size\n",
    "- __`min_count`:__ The minimum word count\n",
    "- __`sample`:__ The downsample setting for frequent words\n",
    "- __`sg`:__ Training model, 1 for skip-gram otherwise CBOW\n",
    "\n",
    "We will build a simple Word2Vec model on the corpus and visualize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-tKqisDuRiqm",
    "outputId": "f7106f35-82d8-4e0d-8940-02d524b4879a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1aace1affc8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "\n",
    "tokenized_corpus = [nltk.word_tokenize(doc) for doc in norm_corpus]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 15    # Word vector dimensionality  \n",
    "window_context = 20  # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "sg = 1               # skip-gram model\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, \n",
    "                              window=window_context, min_count = min_word_count,\n",
    "                              sg=sg, sample=sample)\n",
    "w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "6NY4l-zaRiqo",
    "outputId": "bd4ef9b9-421e-494a-9a0f-8e3985fba0a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAFlCAYAAADVmk8OAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABJ7klEQVR4nO3deXyU1d3//9cnQVFkU1lEBAPeLJJthASQJUR2N5aiVQgKQUmLotz4gyLiF7mhVGtptVTRYjFADRAFlIo7AgKKSoJhCRIRBRSpBhUlhiAJ5/fHDDFgWEJmMlnez8cjj5k51zKf6xCSd86c67rMOYeIiIiIiPhXSLALEBERERGpjBS0RUREREQCQEFbRERERCQAFLRFRERERAJAQVtEREREJAAUtEVEREREAqBasAs4U/Xq1XNhYWHBLkNEREREKrH09PT9zrn6/thXhQnaYWFhpKWlBbsMEREREanEzGy3v/alqSMiIiIiIgGgoC0iIiIiEgAK2iIiIiIiAaCgLSIiIiISAAraIiIiIiIBoKAtFcKuXbuIiIgIdhkiIiIiZ0xBW0REREQkABS0pcIoKChg5MiRhIeH07t3bw4dOsQzzzxDbGws0dHRDBo0iNzcXACGDx/OqFGjuOaaa2jevDnvvPMOI0aM4Morr2T48OHBPRARERGpEhS0pcLYsWMHd999N5mZmdStW5clS5bwm9/8hg0bNrBp0yauvPJK5syZU7j+999/z8qVK3nssce48cYbGTt2LJmZmWzZsoWMjIzgHYiIiIhUCQraUj4tSIHWYRAa4n1c9hLNmjXD4/EA0K5dO3bt2sXWrVvp2rUrkZGRpKSkkJmZWbiLG2+8ETMjMjKShg0bEhkZSUhICOHh4ezatSsYRyUiIiJVSIW5BbtUIQtSYFwSJOZCKyBrN0yfSPXq9QpXCQ0N5dChQwwfPpyXXnqJ6Oho5s6dy+rVqwvXqV69OgAhISGFz4+9zs/PL6ujERERkSpKI9pS/kyd5A3Z4Xj/FAwHbs6Db77+1aoHDx6kUaNGHDlyhJSUlLKuVEREROSkNKIt5c+OPd6R7KKaAz8f+dWq06ZNo0OHDlx++eVERkZy8ODBMilRRERE5HTMOVf6nZiNBe4EHLAFSARqAKlAGLAL+K1z7nvf+hOBO4AC4F7n3Bune4+YmBiXlpZW6lqlAmgdBoN2e0eyj8kEllwO23cFpyYRERGpEsws3TkX4499lXrqiJk1Bu4FYpxzEUAocCtwP/C2c64F8LbvNWbWxrc8HOgLzDKz0NLWIZXI5OmQXMMbrvPxPibX8LaLiIiIVBD+mqNdDTjfzKrhHcn+CugPzPMtnwcM8D3vDyxyzh12zn0OfAq091MdUhkMSYAZs70j2InmfZwx29suIiIiUkGUeo62c26vmc0A9gCHgDedc2+aWUPn3D7fOvvMrIFvk8bA+0V28aWv7VfMLAlIAmjatGlpS5WKZEiCgrWIiIhUaP6YOnIh3lHqZsClwAVmNvRUmxTTVuxEcefcbOdcjHMupn79+qUtVURERESkzPhj6khP4HPnXLZz7giwFOgEfG1mjQB8j9/41v8SaFJk+8vwTjUREREREak0/BG09wAdzayGmRnQA/gY+A8wzLfOMGCZ7/l/gFvNrLqZNQNaAB/6oQ4RERERkXLDH3O0PzCzxcBGvNeI+AiYDdQEnjezO/CG8Zt962ea2fPANt/6dzvnCkpbh4iIiIhIeeKX62iXBV1HW0REREQCrVxdR1tERERERH5NQVtEREREJAAUtEVEREREAkBBW0REREQkABS0RUREREQCQEFbRERERCQAFLRFRERERAJAQVtEREREJAAUtEVEREREAkBBW0REREQkABS0RUREREQCQEFbRERERCQAFLQlIMLCwti/f3+wyxAREREJGgVtEREREZEAUNCWUvvpp5+4/vrriY6OJiIigtTU1MJlhw4dom/fvvzzn/+kRYsWZGdnA3D06FH+53/+R6PeIiIiUmkpaEupvf7661x66aVs2rSJrVu30rdvXwBycnK48cYbGTJkCL/73e8YOnQoKSkpAKxYsYLo6Gjq1asXzNJFREREAkZBW0otMjKSFStWMGHCBNauXUudOnUA6N+/P4mJidx+++0AjBgxgvnz5wPw7LPPkpiYGLSaRURERAJNQVvOzoIUaB0GoSG07Neb9In3ExkZycSJE5k6dSoAnTt35rXXXsM5B0CTJk1o2LAhK1eu5IMPPuDaa68N4gGIiIiIBJaCtpTcghQYlwSDdkOy46veu6nx0FiGhhjjxo1j48aNAEydOpWLL76Yu+66q3DTO++8k6FDh/Lb3/6W0NDQYB2BiIiISMApaEvJTZ0EibkQDlSDLTWgfcEhPCMSmT59Og8++GDhqo8//jh5eXn84Q9/AKBfv37k5ORo2oiIiIhUenbsY/3yLiYmxqWlpQW7DAEIDYFkB9WKtOUDiQYFR0+5aVpaGmPHjmXt2rUBLVFERETkbJhZunMuxh/70oi2lFyLppB1QluWr/0UHnnkEQYNGsTDDz8csNJEREREygsFbSm5ydMhuQZk4h3JzsT7evL0U252//33s3v3brp06VIWVYqIiIgEVbXTryJygiEJ3sepk2DHHu9I9ozpv7SLiIiIiIK2nKUhCQrWIiIiIqegqSMiIiIiIgGgoC0iIiIiEgB+CdpmVtfMFpvZdjP72MyuNrOLzOwtM9vhe7ywyPoTzexTM8sysz7+qEFEREREpDzx14j234HXnXOtgWjgY+B+4G3nXAvgbd9rzKwNcCve2530BWaZmW4RKCIiIiKVSqmDtpnVBuKAOQDOuZ+dcweA/sA832rzgAG+5/2BRc65w865z4FPgfalrUNEREREpDzxx4h2cyAbSDazj8zsX2Z2AdDQObcPwPfYwLd+Y+CLItt/6Wv7FTNLMrM0M0vLzs72Q6kiIiIiImXDH0G7GtAWeMo5dxXwE75pIidhxbQVex9459xs51yMcy6mfv36pa9URERERKSM+CNofwl86Zz7wPd6Md7g/bWZNQLwPX5TZP0mRba/DPjKD3WIiIiIiJQbpQ7azrn/Al+YWStfUw9gG/AfYJivbRiwzPf8P8CtZlbdzJoBLYAPS1uHiIiIiEh54q87Q94DpJjZucBnQCLeEP+8md0B7AFuBnDOZZrZ83jDeD5wt3OuwE91iIiIiIiUC34J2s65DCCmmEU9TrL+dGC6P95bRERERKQ80p0hRUREREQCQEFbRERERCQAFLRFRERERAJAQVtEREREJAAUtEVEREREAkBBW0REREQkABS0RUREREQCQEFbRERERCQAFLRFRERERAJAQVtEREREJAAUtEVEREREAkBBW0REREQkABS0RUREREQCQEFbRERERCQAFLRFRERERAJAQVtEREREJAAUtEVEREREAkBBW0REREQkABS0RUREREQCQEFbRERERCQAFLRFRERERAJAQbsS6tSpU7BLEBE/+umnn7j++uuJjo4mIiKC1NRUpk6dSmxsLBERESQlJeGcAyA+Pp60tDQA9u/fT1hYGACZmZm0b98ej8dDVFQUO3bsAGDAgAG0a9eO8PBwZs+eXfiec+bMoWXLlsTHxzNy5EhGjx4NQHZ2NoMGDSI2NpbY2FjeffddAN555x08Hg8ej4errrqKgwcPllX3iIiUW9WCXYD433vvvRfsEkTEj15//XUuvfRSXnnlFQB++OEHevXqxeTJkwG47bbbWL58OTfeeONJ9/H0008zZswYEhIS+PnnnykoKADg2Wef5aKLLuLQoUPExsYyaNAgDh8+zLRp09i4cSO1atWie/fuREdHAzBmzBjGjh1Lly5d2LNnD3369OHjjz9mxowZPPnkk3Tu3JmcnBzOO++8APeKiEj5pxHtSqhmzZqsXr2aG264obBt9OjRzJ07F4CwsDAeeOABrr76amJiYti4cSN9+vThiiuu4OmnnwZg9erVxMXFMXDgQNq0acPvf/97jh49SkFBAcOHDyciIoLIyEgee+yxYByiSJUSGRnJihUrmDBhAmvXrqVOnTqsWrWKDh06EBkZycqVK8nMzDzlPq6++mr+9Kc/8ec//5ndu3dz/vnnAzBz5kyio6Pp2LEjX3zxBTt27ODDDz+kW7duXHTRRZxzzjncfPPNhftZsWIFo0ePxuPx0K9fP3788UcOHjxI586due+++5g5cyYHDhygWjWN44iI6CdhFdWkSRPWr1/P2LFjGT58OO+++y55eXmEh4fz+9//HoAPP/yQbdu2cfnll9O3b1+WLl1Ks2bN2Lt3L1u3bgXgwIEDQTwKkUpsQQpMnQQ79tCyRVPSJ07k1RoXMHHiRHr37s2TTz5JWloaTZo0YcqUKeTl5QFQrVo1jh49ClDYBjBkyBA6dOjAK6+8Qp8+ffjXv/5FSEgIK1asYP369dSoUYP4+Hjy8vIKp6EU5+jRo6xfv74wqB9z//33c/311/Pqq6/SsWNHVqxYQevWrQPQMSIiFYdGtKuofv36Ad6Rsg4dOlCrVi3q16/PeeedVxie27dvT/PmzQkNDWXw4MGsW7eO5s2b89lnn3HPPffw+uuvU7t27SAehUgltSAFxiXBoN2Q7Piq925qPDSWoSHGuHHj2LhxIwD16tUjJyeHxYsXF24aFhZGeno6wHHtn332Gc2bN+fee++lX79+bN68mR9++IELL7yQGjVqsH37dt5//33A+3//nXfe4fvvvyc/P58lS5YU7qd379488cQTha8zMjIA2LlzJ5GRkUyYMIGYmBi2b98esO4REakoFLQrgwUp0DoMQkO8j/n5x41qwfEjWwDVq1cHICQkpPD5sdf5+fkAmNlx25gZF154IZs2bSI+Pp4nn3ySO++8MzDHJFKVTZ0EibkQDlSDLTWgfcEhPCMSmT59Og8++CAjR44kMjKSAQMGEBsbW7jpuHHjeOqpp+jUqRP79+8vbE9NTSUiIgKPx8P27du5/fbb6du3L/n5+URFRfH//t//o2PHjgA0btyYBx54gA4dOtCzZ0/atGlDnTp1AO9Uk7S0NKKiomjTpk3hdLPHH3+ciIgIoqOjOf/887n22mvLrr9ERMopO9VHhCXakVkokAbsdc7dYGYXAalAGLAL+K1z7nvfuhOBO4AC4F7n3Bun239MTIw7dia9FHFs5CsxF1oBWVDzYfj473+n61//RlZWFnl5eXg8Hh566CGGDx9OWFgYaWlp1KtXj7lz55KWllY4QnVs2datW7n22msLp45ce+21JCUl0a1bN84991xq165NRkYGw4cPLxzREhE/CQ2BZHf85L58INGg4OjJtvKrnJwcatasSX5+PgMHDmTEiBEMHDiwTN5bRCSYzCzdORfjj335c0R7DPBxkdf3A28751oAb/teY2ZtgFvxjtX0BWb5QrqcjRNGvggHOweaPPk3fvvb3xIVFUVCQgJXXXVViXd99dVXc//99xMREUGzZs0YOHAge/fuJT4+Ho/Hw/Dhw3n44Yf9fkgiVV6LppB1QluWr72MTJkyBY/HU/j/f8CAAWX23iIilYVfRrTN7DJgHjAduM83op0FxDvn9plZI2C1c66VbzQb59zDvm3fAKY459af6j00on0SJ4x8fXsQ2k6C3d+XbuRr9erVzJgxg+XLl/upUBE5Y8V8UkVyDZgxG4YkBLs6EZFyJd83ZdZf/Dmi7a+qHgf+ANQq0tbQObcPwBe2G/jaGwPvF1nvS1+bnI0WTSFrN4TDV99D/B9hXFtgT9mNfImInx0L076rjtCiKcyYrpAtIlXStGnTSElJoUmTJtSrV4927dqxfPlyOnXqxLvvvku/fv2Ij4/nvvvuIycnp3BqbKNGjdi5cyd333032dnZ1KhRg2eeeYbWrVszfPhwateuTVpaGv/973959NFHuemmm/xee6mDtpndAHzjnEs3s/gz2aSYtmKH1c0sCUgCaNpUwbFYk6cXjnxd2go+GYFv5Gt6qXYbHx9PfHy8X0oUkbMwJEHBWkSqvLS0NJYsWcJHH31Efn4+bdu2pV27doD3EsPvvPMOR44coVu3bixbtoz69euTmprKpEmTePbZZ0lKSuLpp5+mRYsWfPDBB9x1112sXLkSgH379rFu3Tq2b99Ov379ymfQBjoD/czsOuA8oLaZPQd8bWaNikwd+ca3/pdAkyLbXwZ8VdyOnXOzgdngnTrih1orH418iYiISCW1bt06+vfvX3jt/qJ3wL3lllsAyMrKYuvWrfTq1QuAgoICGjVqRE5ODu+9995xN906fPhw4fMBAwYQEhJCmzZt+PrrrwNSf6mDtnNuIjARwDeiPc45N9TM/gIMAx7xPS7zbfIfYIGZ/Q24FGgBfFjaOqo0jXyJiIhIZVHkhl3u4rrQJb7Y1S644AIAnHOEh4ezfv3xp/v9+OOP1K1b96RXRyt6eWN/XYXvRIG8jvYjQC8z2wH08r3GOZcJPA9sA14H7nbOFQSwDhERERGpCE64YVeX67/n5ZdfIm9uMjk5Obzyyiu/2qRVq1ZkZ2cXBu0jR46QmZlJ7dq1adasGS+88ALgDdObNm0q08Pxa9B2zq12zt3ge/6tc66Hc66F7/G7IutNd85d4Zxr5Zx7zZ81iIiIiEgFdcJli2N7Qb+OjuikJH7zm98QExNTeAOtY84991wWL17MhAkTiI6OxuPx8N577wGQkpLCnDlziI6OJjw8nGXLlhXzpoHjtxvWBJou7yciIiJSyRVzw66cHKg5ysg9mENcXByzZ8+mbdu2ASuhvN6wRkRERETk7BVzw66kv4PnnGq0bduWQYMGBTRk+5v/ru4tIiIiIlIaRS5bfOyGXQu+rwHPVswbdiloi4iIiEj5UMkuW6ygLSIiIiLlRyW6bLHmaIuIiIiIBICCtoiIiIhIAChoi4iIiIgEgIK2iIiIiEgAKGiLiIiIiASAgraIiIiISAAoaIuIiIiIBICCtoiIiIhIAChoi4iIiIgEgIK2iIiIiEgAKGiLiIiIiASAgraIiIiISAAoaIuIiIiIBICCtkgFNHPmTK688koSEhKCXYqIiIicRLVgFyAiJTdr1ixee+01mjVrFuxSRERE5CQ0oi1Swfz+97/ns88+o1+/fvz1r39lwIABREVF0bFjRzZv3kx+fj6xsbGsXr0agIkTJzJp0qTgFi0iIlIFKWiLVDBPP/00l156KatWrWLXrl1cddVVbN68mT/96U/cfvvtVKtWjblz5zJq1CjeeustXn/9dR566KFgly0iIlLlaOqISAW2bt06lixZAkD37t359ttv+eGHHwgPD+e2227jxhtvZP369Zx77rlBrlTORM2aNcnJyQl2GSIi4ica0RapCBakQOswCA3xPv70EwDOuV+tamYAbNmyhbp16/L111+XYaEiIiJyjIK2SHm3IAXGJcGg3ZDsvI/ffwtLFhMXF0dKSgoAq1evpl69etSuXZulS5fy7bffsmbNGu69914OHDgQ3GOQEsnJyaFHjx60bduWyMhIli1bBninDXk8HjweD82aNeOaa65hzpw5jB07tnDbZ555hvvuuy9YpYuISBFW3IhYeRQTE+PS0tKCXYZI2Wsd5g3X4b80hd0FafUuI+TDTSQmJvL5559To0YNZs+ezaWXXkqnTp14++23adKkCTNnziQ9PZ158+YF7RDkzBybOpKfn09ubi61a9dm//79dOzYkR07dhR+WnHkyBG6d+/OH/7wB7p3705UVBTbt2/nnHPOoVOnTvzzn/8kMjIyyEcjIlIxmVm6cy7GH/vSHG2R8m7HHmh1fNOumUDiXrjoosLRzqI++eSTwuf33ntvgAsUf3PO8cADD7BmzRpCQkLYu3cvX3/9NZdccgkAY8aMoXv37tx4442Ad37+8uXLufLKKzly5IhCtohIOaGpIyLlXYumkHVCW5avXSq+ovPvc3NhQQopKSlkZ2eTnp5ORkYGDRs2JC8vD4C5c+eye/fu464kc+eddzJ37lySk5NJTEwM0oFIebVr1y4iIiKCXYZIlVTqoG1mTcxslZl9bGaZZjbG136Rmb1lZjt8jxcW2WaimX1qZllm1qe0NYhUapOnQ3INyATy8T4m1/C2S8V24vz7ag7GJfHDyrdp0KAB55xzDqtWrWL37t0ApKenM2PGDJ577jlCQn758d2hQwe++OILFixYwODBg4N1NCIicgJ/jGjnA/+fc+5KoCNwt5m1Ae4H3nbOtQDe9r3Gt+xWvDNO+wKzzCzUD3WIVE5DEmDGbFhyOSSa93HGbG+7VGxTJ0FirvenYTW8P5ETc0l4bzVpaWnExMSQkpJC69atAXjiiSf47rvvuOaaa/B4PNx5552Fu/rtb39L586dufDCC4t9K6na8vPzGTZsGFFRUdx0003k5uaSnp5Ot27daNeuHX369GHfvn2A94Ta2NhYoqOjGTRoELm5uQAMHz6ce++9l06dOtG8eXMWL14MwL59+4iLi8Pj8RAREcHatWuDdpwi5Y3fT4Y0s2XAE76veOfcPjNrBKx2zrUys4kAzrmHfeu/AUxxzq0/1X51MqSIVDqhIb6R7CJt+Xj/oCo4WqJd3XDDDYwdO5YePXr4tUSp+Hbt2kWzZs1Yt24dnTt3ZsSIEVx55ZW8+OKLLFu2jPr165Oamsobb7zBs88+y7fffsvFF18MwIMPPkjDhg255557GD58OD/99BOpqals376dfv368emnn/LXv/6VvLw8Jk2aREFBAbm5udSqVSvIRy1y9srtyZBmFgZcBXwANHTO7QPwhe0GvtUaA+8X2exLX1tx+0sCkgCaNtV8VBGpZFo0hazjryhT0vn3Bw4coH379kRHRytky0k1adKEzp07AzB06FD+9Kc/sXXrVnr16gVAQUEBjRo1AmDr1q08+OCDHDhwgJycHPr0+WWG54ABAwgJCaFNmzaF1+iPjY1lxIgRHDlyhAEDBuDxeMr24ETKMb+dDGlmNYElwP8653481arFtBU7rO6cm+2ci3HOxdSvX98fZYqIlB9+mH9ft25dPvnkE1544YVAVSkVUdGTbHt0wXzTP46pVasW4eHhZGRkkJGRwZYtW3jzzTcB7xSRJ554gi1btvDQQw8VnogLUL169cLnxz4Rj4uLY82aNTRu3JjbbruN+fPnB/74RCoIvwRtMzsHb8hOcc4t9TV/7Zsygu/xG1/7l0CTIptfBnzljzpERCoUzb+XQDjxJNu+e9nz7bes/78pACxcuJCOHTuSnZ3N+vXeWZtHjhwhMzMTgIMHD9KoUSOOHDlSeEOsU9m9ezcNGjRg5MiR3HHHHWzcuDFgh1bZnewKMfHx8Wj6bMXkj6uOGDAH+Ng597cii/4DDPM9HwYsK9J+q5lVN7NmQAvgw9LWISJSIQ1JgO27vHOyt+9SyJbSO/Ek25Zw5cUw77G/EhUVxXfffcc999zD4sWLmTBhAtHR0Xg8Ht577z0Apk2bRocOHejVq1fhibinsnr1ajweD1dddRVLlixhzJgxgT0+kQqk1CdDmlkXYC2wBTh29s4DeOdpPw80BfYANzvnvvNtMwkYgffD0v91zr12uvfRyZAiIiJnwI8n2UrZ2rVrF3379qVDhw589NFHtGzZkvnz53PdddcxY8YMYmJiCu8gC7B48WKWL1/O3Llzyc7O5ve//z179uwB4PHHHy+cly8lU65OhnTOraP4edcAxZ6Z45ybDugiwCIiIv7mh5NsJXiysrKYM2dO4RViZs2adUbbjRkzhrFjx9KlSxf27NlDnz59+PjjjwNcrZyObsEuIiJSmUye7p2jnZgLrfCG7OQaMEPjWxXBiVeImTlz5hltt2LFCrZt21b4+scff+TgwYO61GKQKWiLiIhUJsfm+U+dBDv2eEeyZ0zX/P/yaEHK8f9Oo/4X76lvvzjV66JXhDl69Cjr16/n/PPPD2zNUiJ+u7yfiIiIlBM6ybb8O/HqMIN2w/SJ7Nmzp/BqMAsXLqRLly7HbdawYUM+/vhjjh49yosvvljY3rt3b5544onC1xkZGWVyGHJqCtoiIiIiZe3Eq8OEAzfnceW55zBv3rzCK8SMGjXquM0eeeQRbrjhBrp37154kyGAmTNnkpaWRlRUFG3atOHpp58u08OR4vn9FuyBoquOiIiISKWhq8OUW/686ohGtEVERETKWoum3hNVi9LVYSodBW0RERGRsjZ5uvdqMJl4R7Iz8b6erKvDVCa66oiIiIhIWdPVYaoEBW0RERGRYBiSoGBdyWnqiIiIiIhIAChoi4iIiIgEgIK2iIiIiEgAKGiLiIiIiASAgraIiIiISAAoaIuIiIiIBICCtoiIiIhIAChoi4iIiIgEgIK2iIiIiEgAKGiLiIiIiASAgraIiIiISAAoaIuIiIiIBICCtoiIiIhIAChoi4iIiIgEgIK2iIiIiEgAKGiLiIiIiASAgraIiIiISAAoaIuIiIiIBEDQgraZ9TWzLDP71MzuD1YdIiJVyXPPPUf79u3xeDz87ne/o6CggDlz5tCyZUvi4+MZOXIko0ePBmDnzp107NiR2NhYJk+eTM2aNQHYt28fcXFxeDweIiIiWLt2bTAPSUSk3ApK0DazUOBJ4FqgDTDYzNoEoxYRkari448/JjU1lXfffZeMjAxCQ0NJSUlh2rRpvP/++7z11lts3769cP0xY8YwZswYNmzYwKWXXlrYvmDBAvr06UNGRgabNm3C4/EE4WhERMq/akF63/bAp865zwDMbBHQH9gWpHpERCq9t99+m/T0dGJjYwE4dOgQ7733Ht26deOiiy4C4Oabb+aTTz4BYP369bz00ksADBkyhHHjxgEQGxvLiBEjOHLkCAMGDFDQFhE5iWBNHWkMfFHk9Ze+tuOYWZKZpZlZWnZ2dpkVJyJSaSxIgdZhEBqCmzaZYTExZGRkkJGRQVZWFg899FCJdxkXF8eaNWto3Lgxt912G/Pnz/d/3SIilUCwgrYV0+Z+1eDcbOdcjHMupn79+mVQlohIJbIgBcYlwaDdkOzo0f97Fr+6nG+emgXAd999R9u2bXnnnXf4/vvvyc/PZ8mSJYWbd+zYsfD1okWLCtt3795NgwYNGDlyJHfccQcbN24s2+MSEakgghW0vwSaFHl9GfBVkGoREamcpk6CxFwIB6pBm3j4442O3mP/l6ioKHr16sW+fft44IEH6NChAz179qRNmzbUqVMHgMcff5y//e1vtG/fnn379hW2r169Go/Hw1VXXcWSJUsYM2ZM8I5RRKQcM+d+NZAc+Dc1qwZ8AvQA9gIbgCHOucyTbRMTE+PS0tLKqEIRkUogNASS3fFn4+QDiQYFRwubcnJyqFmzJvn5+QwcOJARI0YwcOBAcnNzOf/88zEzFi1axMKFC1m2bFmZH4aISFkys3TnXIw/9hWUkyGdc/lmNhp4AwgFnj1VyBYRkbPQoilk7faOaB+T5WsvYsqUKaxYsYK8vDx69+7NgAEDAEhPT2f06NE456hbty7PPvtsmZUuIlIZBGVE+2xoRFtEpISOzdFOzIVWeEN2cg2YMRuGJAS7OhGRcqnCj2iLiEgZOBamp06CHXu8I9kzpitki4iUEQVtEZHKbEiCgrWISJAE7RbsIiIiIiKVmYK2iIiIiEgAKGiLiIiIiASAgraIiIiISAAoaItfHThwgFmzZpVom+HDh7N48eIAVSQiIiISHAra4ldnE7RFREREKiMFbfGr+++/n507d+LxeBg/fjzjx48nIiKCyMhIUlNTAXDOMXr0aNq0acP111/PN998U7j91KlTiY2NJSIigqSkJJxz7Ny5k7Zt2xaus2PHDtq1a1fmxyYiIiJSEgra4lePPPIIV1xxBRkZGXTs2JGMjAw2bdrEihUrGD9+PPv27ePFF18kKyuLLVu28Mwzz/Dee+8Vbj969Gg2bNjA1q1bOXToEMuXL+eKK66gTp06ZGRkAJCcnMzw4cODc4AiIiIiZ0hBW/xjQQq0DoPmzWDHJ7AghXXr1jF48GBCQ0Np2LAh3bp1Y8OGDaxZs6aw/dJLL6V79+6Fu1m1ahUdOnQgMjKSlStXkpmZCcCdd95JcnIyBQUFpKamMmTIkCAdqIiIiMiZUdCW0luQAuOSYNBu+AtQ+wiMS8Jt//ikm5jZr9ry8vK46667WLx4MVu2bGHkyJHk5eUBMGjQIF577TWWL19Ou3btuPjiiwN1NCIiIiJ+oaAtpTd1EiTmQjjUqgkHHZCYS9zmjaSmplJQUEB2djZr1qyhffv2xMXFsWjRIgoKCti3bx+rVq0CKAzV9erVIycn57grkZx33nn06dOHUaNGkZiYGIyjFBERESmRasEuQCqBHXuglffpxbWgc0uImAfXfpVN1JAooqOjMTMeffRRLrnkEgYOHMjKlSuJjIykZcuWdOvWDYC6desycuRIIiMjCQsLIzY29ri3SUhIYOnSpfTu3busj1BERESkxMw5F+wazkhMTIxLS0sLdhlSnNZh3mkj4UXaMoEll8P2XX57mxkzZvDDDz8wbdo0v+1TREREpCgzS3fOxfhjXxrRltKbPN07Rzsx1zuynQUk14AZ0/32FgMHDmTnzp2sXLnSb/sUERERCSQFbSm9IQnex6mTvNNIWjT1huxj7X7w4osv+m1fIiIiImVBQVv8Y0iCX4O1iIiISEWnq46IiIiIiASAgraIiIiISAAoaIuIiIiIBICCtshZmDJlCjNmzAh2GSIiIlKOKWiLiIgUY9euXURERAS7DBGpwBS0Rc7Q9OnTadWqFT179iQrKwuAjIwMOnbsSFRUFAMHDuT7778HYMOGDURFRXH11Vczfvx4/bIWqaQKCgqCXYKIlGMK2iJnID09nUWLFvHRRx+xdOlSNmzYAMDtt9/On//8ZzZv3kxkZCT/93//B0BiYiJPP/0069evJzQ0NJili0gp5OfnM2zYMKKiorjpppvIzc0lLCyMqVOn0qVLF1544QUWLlxIZGQkERERTJgwAYDnn3+e++67D4C///3vNG/eHICdO3fSpUsXAMLCwnjooYdo27YtkZGRbN++PTgHKSIBo6AtcgbWrl3LwIEDqVGjBrVr16Zfv3789NNPHDhwgG7dugEwbNgw1qxZw4EDBzh48CCdOnUCYMiQIcEsXURKISsri6SkJDZv3kzt2rWZNWsWAOeddx7r1q0jLi6OCRMmsHLlSjIyMtiwYQMvvfQScXFxrF27FvD+/Lj44ovZu3cv69ato2vXroX7r1evHhs3bmTUqFE670OkElLQFjmZBSnQOgxCQ+BPU7GtW89oM+dcYOsSkTLTpEkTOnfuDMDQoUNZt24dALfccgvgnSYWHx9P/fr1qVatGgkJCaxZs4ZLLrmEnJwcDh48yBdffMGQIUNYs2YNa9euPS5o/+Y3vwGgXbt27Nq1q2wPTkQCrlRB28z+YmbbzWyzmb1oZnWLLJtoZp+aWZaZ9SnS3s7MtviWzTQzK00NIgGxIAXGJcGg3ZDsiLvxe158+SUOzU3m4MGDvPzyy1xwwQVceOGFhaNW//73v+nWrRsXXnghtWrV4v333wdg0aJFwTwSESmJon9g9+iC5eYet/jYr6wLLrgAOPUf1ldffTXJycm0atWKrl27snbtWtavX18Y3AGqV68OQGhoKPn5+X4+GBEJttKOaL8FRDjnooBPgIkAZtYGuBUIB/oCs8zs2ETVp4AkoIXvq28paxDxv6mTIDHX+x1cDdr2gFs6OzxJSQwaNKhwRGrevHmMHz+eqKgoMjIymDx5MgBz5swhKSmJq6++GuccderUCeLBiMgZOeEPbPruZc+337L+/6YAsHDhwsL51cd06NCBd955h/3791NQUMDChQsLp5PFxcUxY8YM4uLiuOqqq1i1ahXVq1fXzwORKqRaaTZ2zr1Z5OX7wE2+5/2BRc65w8DnZvYp0N7MdgG1nXPrAcxsPjAAeK00dYj43Y490Or4pkkjYNLaAnjzzePaj41cFxUeHs7mzZsBeOSRR4iJiQlYqSLiJ0X/wAZoCVdeDPMe+yu/W7KUFi1aMGrUKP7xj38UbtKoUSMefvhhrrnmGpxzXHfddfTv3x+Arl278sUXXxAXF0doaChNmjShdevWQTgwEQkW89d8UjN7GUh1zj1nZk8A7zvnnvMtm4M3TO8CHnHO9fS1dwUmOOduON3+Y2JiXFpaml9qFTmt1mHeUa3wIm2ZwJLLYfuu026emprKww8/TH5+Ppdffjlz586lfv36ASpWRPwiNMQ7kl10CCofSDQoOBqsqkSkjJlZunPOLyNkpx3RNrMVwCXFLJrknFvmW2cS3h9HKcc2K2Z9d4r2k713Et5pJjRt2vR0pYr4z+Tp3o+QE3O9I9tZQHINmDH9jDa/5ZZbCk+WEpEKokVTyDrhD+wsX7uIyFk4bdA+Nvp8MmY2DLgB6OF+GR7/EmhSZLXLgK987ZcV036y954NzAbviPbpahXxmyEJ3sepk7zTSFo09YbsY+0iUvmU8g9sEZETlWqOtpn1BSYA3ZxzRU/N/g+wwMz+BlyK96THD51zBWZ20Mw6Ah8AtwP/OHG/IuXCkAQFa5GqRH9gi4iflSpoA08A1YG3fJc8et8593vnXKaZPQ9swzul5G7n3LH71I4C5gLn4523rRMhRUSkfNAf2CLiR6W96sj/nGLZdOBXn7c559KAiNK8r4iIiIhIeac7Q4qIiIiIBICCtoiIiIhIAChoi4iIiIgEgIK2iIiIiEgAKGiLiIiIiASAgraIiIiISAAoaIuIiIiIBICCtoiIiIhIAChoi4iIiIgEgIK2iMgJdu3aRUSEbmArIiKlo6AtIiIiIhIACtoiIsXIz89n2LBhREVFcdNNN5Gbm8vUqVOJjY0lIiKCpKQknHMAfPrpp/Ts2ZPo6Gjatm3Lzp07cc4xfvx4IiIiiIyMJDU1FYDVq1cTHx/PTTfdROvWrUlISCjcj4iIVC4K2iIixcjKyiIpKYnNmzdTu3ZtZs2axejRo9mwYQNbt27l0KFDLF++HICEhATuvvtuNm3axHvvvUejRo1YunQpGRkZbNq0iRUrVjB+/Hj27dsHwEcffcTjjz/Otm3b+Oyzz3j33XeDeagiIhIgCtoiIsVo0qQJnTt3BmDo0KGsW7eOVatW0aFDByIjI1m5ciWZmZkcPHiQvXv3MnDgQADOO+88atSowbp16xg8eDChoaE0bNiQbt26sWHDBgDat2/PZZddRkhICB6Ph127dgXrMEVEJIAUtEVEFqRA6zAIDfE+LnsJMztuFTPjrrvuYvHixWzZsoWRI0eSl5d30mkfp5oOUr169cLnoaGh5Ofn++MoRESknFHQFpGqbUEKjEuCQbsh2Xkfp09kz549rF+/HoCFCxfSpUsXAOrVq0dOTg6LFy8GoHbt2lx22WW89NJLABw+fJjc3Fzi4uJITU2loKCA7Oxs1qxZQ/v27YNyiCIiEhwK2iJStU2dBIm5EA5Uw/t4cx5XnnsO8+bNIyoqiu+++45Ro0YxcuRIIiMjGTBgALGxsYW7+Pe//83MmTOJioqiU6dO/Pe//2XgwIFERUURHR1N9+7defTRR7nkkkuCdZQiIhIEVlHOdo+JiXFpaWnBLkNEKpvQEO9IdrUibflAokHB0WBVJSIiQWJm6c65GH/sSyPaIlK1tWgKWSe0ZfnaRURESkFBW0SqtsnTIbkGZOIdyc7E+3ry9CAXJiIiFV21068iIlKJDUnwPk6dBDv2eEeyZ0z/pV1EROQsKWiLiAxJULAWERG/09QREREREZEAUNAWERERqYB27dpFRESE3/cbHx9PcVd6e+GFF7jyyiu55pprSrzPP/3pT/4orcJR0BYRERGppAoKCvy2rzlz5jBr1ixWrVpV4m2ratDWHG0RERGRCio/P59hw4bx0Ucf0bJlS+bPn0+bNm0YMWIEb775JqNHj+aiiy7ioYce4vDhw1xxxRUkJydTs2ZNpk6dyssvv8yhQ4fo1KkT//znPzGzwn0fPXqUxMREmjRpwrnnnsu6dev4/PPP6devH3fffTe33XYbP/30EwBPPPEEnTp1Yt++fdxyyy38+OOP5Ofn89RTT/HKK69w6NAhPB4P4eHhpKSkBKu7ypxGtEVEREQqqKysLJKSkti8eTO1a9dm1qxZAJx33nmsW7eOnj178sc//pEVK1awceNGYmJi+Nvf/gbA6NGj2bBhA1u3buXQoUMsX768cL/5+fkkJCTQsmVL/vjHPzJ58mRiYmJISUnhL3/5Cw0aNOCtt95i48aNpKamcu+99wKwYMEC+vTpQ0ZGBps2bcLj8fDII49w/vnnk5GRUaVCNvgpaJvZODNzZlavSNtEM/vUzLLMrE+R9nZmtsW3bKYV/dNJRERERM5YkyZN6Ny5MwBDhw5l3bp1ANxyyy0AvP/++2zbto3OnTvj8XiYN28eu3fvBmDVqlV06NCByMhIVq5cSWZmZuF+f/e73xEREcGkSZOKfd8jR44wcuRIIiMjufnmm9m2bRsAsbGxJCcnM2XKFLZs2UKtWrUCduwVQamDtpk1AXoBe4q0tQFuBcKBvsAsMwv1LX4KSAJa+L76lrYGERERkSphQQq0DoPQEOjRBcvNPW7xsfHLCy64AADnHL169SIjI4OMjAy2bdvGnDlzyMvL46677mLx4sVs2bKFkSNHkpeXV7ifTp06sWrVquPainrsscdo2LAhmzZtIi0tjZ9//hmAuLg41qxZQ+PGjbntttuYP39+ADqh4vDHiPZjwB8AV6StP7DIOXfYOfc58CnQ3swaAbWdc+udcw6YDwzwQw0iIiIilduCFBiXBIN2Q7KDvnvZ8+23rP+/KQAsXLiQLl26HLdJx44deffdd/n0008ByM3N5ZNPPikM0PXq1SMnJ4fFixcft90dd9zBddddx80330x+fv6vSvnhhx9o1KgRISEh/Pvf/y486XL37t00aNCAkSNHcscdd7Bx40YAzjnnHI4cOeLX7qgIShW0zawfsNc5t+mERY2BL4q8/tLX1tj3/MR2ERERETmVqZMgMdc7X6Aa0BKuvBjmPfZXoqKi+O677xg1atRxm9SvX5+5c+cyePBgoqKi6NixI9u3b6du3bqFUz8GDBhAbGzsr97uvvvuo23bttx2220cPXr0uGV33XUX8+bNo2PHjnzyySeFI+irV6/G4/Fw1VVXsWTJEsaMGQNAUlISUVFRJCRUrZuDmXdg+RQrmK0ALilm0STgAaC3c+4HM9sFxDjn9pvZk8B659xzvn3MAV7FO73kYedcT197V+APzrkbT/LeSXinmdC0adN2x+YUiYiIiFQ5oSHekeyi14zLBxINCo6ebCspITNLd87F+GNfp72837FQXEwRkUAzYJNvPtBlwEYza493pLpJkdUvA77ytV9WTPvJ3ns2MBsgJibm1H8RiIiIiFRmLZpC1m7viPYxWb52KZfOeuqIc26Lc66Bcy7MOReGN0S3dc79F/gPcKuZVTezZnhPevzQObcPOGhmHX1XG7kdWFb6wxARERGp5CZPh+QakIl3JDsT7+vJ04NcmJxMQG5Y45zLNLPngW14vxXuds4duzXRKGAucD7wmu9LRERERE5liG9+89RJsGOPdyR7xvRf2qXcOe0c7fIiJibGpaWlBbsMEREREanE/DlHW3eGFBEREREJAAVtEREREZEAUNAWEREREQkABW0RERERkQBQ0BYREZEqKS0tjXvvvfeU69SsWbOMqpHKKCCX9xMREREp72JiYoiJ8cvFJUSKpRFtERERqTSmT59Oq1at6NmzJ4MHD2bGjBnEx8dz7BLB+/fvJywsDIDVq1dzww03AJCTk0NiYiKRkZFERUWxZMmS4/a7f/9+rr76al555ZUyPR6p2DSiLSIiIpVCeno6ixYt4qOPPiI/P5+2bdvSrl27M9p22rRp1KlThy1btgDw/fffFy77+uuv6devH3/84x/p1atXQGqXyklBW0RERCqFtWvXMnDgQGrUqAFAv379znjbFStWsGjRosLXF154IQBHjhyhR48ePPnkk3Tr1s2/BUulp6kjIiIiUnEtSIHWYRAaAn+aim3d+qtVqlWrxtGjRwHIy8srdjfOOcys2G3btWvHG2+84deypWpQ0BYREZGKaUEKjEuCQbsh2RF34/e8+PJLHJqbzMGDB3n55ZcBCAsLIz09HYDFixcXu6vevXvzxBNPFL4+NnXEzHj22WfZvn07jzzySIAPSCobBW0RERGpmKZOgsRcCAeqQdsecEtnhycpiUGDBtG1a1cAxo0bx1NPPUWnTp3Yv39/sbt68MEH+f7774mIiCA6OppVq1YVLgsNDWXRokWsWrWKWbNmlcWRSSVhzrlg13BGYmJi3LEzhkVEREQIDYFkd/wZZ/lAokHBUaZMmULNmjUZN25csCqUCsjM0p1zfrnuo0a0RUREpGJq0RSyTmjL8rWLlAO66oiIiIhUTJOne+doJ+ZCK7whO7kGzJgOwJQpU4JZnYiCtoiIiFRQQxK8j1MnwY493pHsGdN/aRcJMgVtERERqbiGJChYS7mlOdoiIiIiIgGgoC0iIiIiEgAK2iIiIiIiAaCgLSIiIiISAAraIiIiIiIBoKAtIiIiIhIACtoiIiIiIgGgoC0iIiIiEgAK2iIiIiIiAaCgLSIiIiISAAraIiIiIiIBUOqgbWb3mFmWmWWa2aNF2iea2ae+ZX2KtLczsy2+ZTPNzEpbg4iIiIhIeVOtNBub2TVAfyDKOXfYzBr42tsAtwLhwKXACjNr6ZwrAJ4CkoD3gVeBvsBrpalDRERERKS8Ke2I9ijgEefcYQDn3De+9v7AIufcYefc58CnQHszawTUds6td845YD4woJQ1iIiIiIiUO6UN2i2Brmb2gZm9Y2axvvbGwBdF1vvS19bY9/zE9mKZWZKZpZlZWnZ2dilLPXu7du0iIiIioPtfsGBB4eu0tDTuvfdeAA4fPkzPnj3xeDykpqaedB9z585l9OjRAatRRERERErmtFNHzGwFcEkxiyb5tr8Q6AjEAs+bWXOguHnX7hTtxXLOzQZmA8TExJx0vYruWNAeMmQIADExMcTExADw0UcfceTIETIyMoJYoYiIiIiU1GlHtJ1zPZ1zEcV8LcM7Ir3UeX0IHAXq+dqbFNnNZcBXvvbLimkv9/Lz8xk2bBhRUVHcdNNN5Obmkp6eTrdu3WjXrh19+vRh3759ADzzzDPExsYSHR3NoEGDyM3NBWD48OEsXry4cJ81a9YE4P7772ft2rV4PB4ee+wxVq9ezQ033MA333zD0KFDycjIwOPxsHPnTsLCwti/fz/gHfmOj48v244QEanijv3sFhE5ndJOHXkJ6A5gZi2Bc4H9wH+AW82supk1A1oAHzrn9gEHzayj72ojtwPLSllDmcjKyiIpKYnNmzdTu3ZtnnzySe655x4WL15Meno6I0aMYNKkSQD85je/YcOGDWzatIkrr7ySOXPmnHLfjzzyCF27diUjI4OxY8cWtjdo0IB//etfhcuuuOKKgB6jiIiIiPhPaYP2s0BzM9sKLAKG+Ua3M4HngW3A68DdviuOgPcEyn/hPUFyJ+X1iiMLUqB1GISGQI8uNLn4Yjp37gzA0KFDeeONN9i6dSu9evXC4/Hwxz/+kS+/9E4/37p1K127diUyMpKUlBQyMzODeCAiIhIIzjnGjx9PREQEkZGRhefR3HLLLbz66quF6w0fPpwlS5ZQUFDA+PHjiY2NJSoqin/+85/BKl1EykipLu/nnPsZGHqSZdOB6cW0pwGBO7PQHxakwLgkSMyFVsAHe7F/mrd9SAIAtWrVIjw8nPXr1/9q8+HDh/PSSy8RHR3N3LlzWb16NQDVqlXj6NGjgPcH9M8//1zi0oruIy8v7+yOT0RESm3p0qVkZGSwadMm9u/fT2xsLHFxcdx6662kpqZy3XXX8fPPP/P222/z1FNPMWfOHOrUqcOGDRs4fPgwnTt3pnfv3jRr1izYhyIiAaI7QxZn6iRvyA7H+6dIS9hT4Fg/cRwACxcupGPHjmRnZxcG7SNHjhSOXB88eJBGjRpx5MgRUlJSCncbFhZGeno6AMuWLePIkSOAN7QfPHjwjEoruo8lS5b442hFROR0in7KmZsLC1JYt24dgwcPJjQ0lIYNG9KtWzc2bNjAtddey8qVKzl8+DCvvfYacXFxnH/++bz55pvMnz8fj8dDhw4d+Pbbb9mxY0ewj0xEAkhBuzg79nhHsou48lKYt+e/REVF8d133xXOz54wYQLR0dF4PB7ee+89AKZNm0aHDh3o1asXrVu3LtzHyJEjeeedd2jfvj0ffPABF1xwAQBRUVFUq1aN6OhoHnvssVOW9tBDDzFmzBi6du1KaGiof49bRER+7dinnIN2Q7KDag7GJeG2f1zs6ueddx7x8fG88cYbpKamcuuttwLeTzL/8Y9/kJGRQUZGBp9//jm9e/cuyyMRkTJm3vvGlH8xMTEuLS2tbN6sdZj3B2p4kbZMYMnlsH1X2dQgIiLlwwm/E2qOgJz/D5bOrc8/I67i1Vdf5bvvviMmJoYPPviASy65hFdeeYV//etfpKWlsXPnTs4991xmz57Nq6++ygsvvMA555zDJ598QuPGjQsHXUSkfDCzdOdcjD/2pRHt4kyeDsk1vOE6H+9jcg1vu4iIVC3FfMpJKxi4L5uoqCiio6Pp3r07jz76KJdc4r3tRO/evVmzZg09e/bk3HPPBeDOO++kTZs2tG3bloiICH73u9+Rn59fxgcjImVJI9onsyDFO1d7xx5o0dQbsn0nQoqISBWiTzlFqhSNaJeFIQneH6AFR72PCtl+Udzt7Ivecl5EpNzRp5wicpZKdXk/EX8oest5EZFy59hAS9FPOWfoU04ROT2NaEvQfPbZZ1x11VX85S9/4YYbbgBgypQpjBgxgvj4eJo3b87MmTML1582bRqtW7emV69eDB48mBkzZgAwc+ZM2rRpQ1RUVOHZ/SIifqVPOUXkLGhEW4IiKyuLW2+9leTkZA4cOMA777xTuGz79u2sWrWKgwcP0qpVK0aNGsWmTZtYsmQJH330Efn5+bRt25Z27doB3lvYf/7551SvXp0DBw4E6YhEREREjqcRbSlz2dnZ9O/fn+eeew6Px/Or5ddffz3Vq1enXr16NGjQgK+//pp169bRv39/zj//fGrVqsWNN95YuH5UVBQJCQk899xzVKumvx1FRESkfFDQlsAreke1Hl2oExJCkyZNePfdd4tdvXr16oXPQ0NDyc/P51RXx3nllVe4++67SU9Pp127drpcloiIiJQLCtoSWCfeUa3vXs7d/zUvDb6V+fPns2DBgjPaTZcuXXj55ZfJy8sjJyeHV155BYCjR4/yxRdfcM011/Doo49y4MABcnJyAnlEIiIiImdEn7NLYE2dBIm5v1x/tiVQ13HBo9NY/n4GvXr14sEHHzztbmJjY+nXrx/R0dFcfvnlxMTEUKdOHQoKChg6dCg//PADzjnGjh1L3bp1A3lEIiIiImdEN6yRwAoN8Y5kF/2TLh9INO/Z+yWQk5NDzZo1yc3NJS4ujtmzZ9O2bVu/lisiIiJVmz9vWKMRbQmsFk0h64Q7qmX52ksoKSmJbdu2kZeXx7BhwxSyRUREpFxT0JbAmjzdO0c7MRda4Q3ZyTW8N3sooTOdzy0iIiJSHihoS2DpjmoiIiJSRSloS+ANSVCwFhERkSpHl/cTEREREQkABW0RERERkQBQ0BYRERERCQAFbRERERGRAFDQFhEREREJAAVtEREREZEAUNAWEREREQkABW2RADhw4ACzZs3y6z4ff/xxcnNz/bpPERERCRwFbZEAUNAWERGRUgVtM/OY2ftmlmFmaWbWvsiyiWb2qZllmVmfIu3tzGyLb9lMM7PS1CBSHt1///3s3LkTj8fD+PHjGT9+PBEREURGRpKamgpATk4OPXr0oG3btkRGRrJs2TIAfvrpJ66//nqio6OJiIggNTWVmTNn8tVXX3HNNddwzTXXBPPQRERE5AyZc+7sNzZ7E3jMOfeamV0H/ME5F29mbYCFQHvgUmAF0NI5V2BmHwJjgPeBV4GZzrnXTvdeMTExLi0t7axrFSlLu3bt4oYbbmDr1q0sWbKEp59+mtdff539+/cTGxvLBx98QP369cnNzaV27drs37+fjh07smPHDpYuXcrrr7/OM888A8APP/xAnTp1CAsLIy0tjXr16gX56ERERCovM0t3zsX4Y1+lnTrigNq+53WAr3zP+wOLnHOHnXOfA58C7c2sEVDbObfeeRP+fGBAKWsQKdfWrVvH4MGDCQ0NpWHDhnTr1o0NGzbgnOOBBx4gKiqKnj17snfvXr7++msiIyNZsWIFEyZMYO3atdSpUyfYhyAiIiJnobRB+3+Bv5jZF8AMYKKvvTHwRZH1vvS1NfY9P7FdpHJYkAKtw6B5M9jxCSxI4WSfGqWkpJCdnU16ejoZGRk0bNiQvLw8WrZsSXp6OpGRkUycOJGpU6eW7TGIiIiIX5w2aJvZCjPbWsxXf2AUMNY51wQYC8w5tlkxu3KnaD/Zeyf55n6nZWdnn/5oRIJpQQqMS4JBu6n1BBw87wiMSyLOIDU1lYKCArKzs1mzZg3t27fnhx9+oEGDBpxzzjmsWrWK3bt3A/DVV19Ro0YNhg4dyrhx49i4cSMAtWrV4uDBg8E8QhERESmBaqdbwTnX82TLzGw+3vnWAC8A//I9/xJoUmTVy/BOK/nS9/zE9pO992xgNnjnaJ+uVpGgmjoJEnMhHC4GOodDxI5crp2XTNQdSURHR2NmPProo1xyySUkJCRw4403EhMTg8fjoXXr1gBs2bKF8ePHExISwjnnnMNTTz0FQFJSEtdeey2NGjVi1apVQTxQEREROROlPRnyY2CUc261mfUAHnXOtTOzcGABv5wM+TbQwncy5AbgHuADvCdD/sM59+rp3ksnQ0q5FxoCye74P1/zgUSDgqPBqkpERERKwJ8nQ552RPs0RgJ/N7NqQB6QBOCcyzSz54FteKPG3c65At82o4C5wPnAa74vkYqvRVPI2g3hRdqyfO0iIiJS5ZQqaDvn1gHtTrJsOjC9mPY0IKI07ytSLk2e7p2jnZgLrfCG7OQaMONX/w1ERESkCijtiLaIHDMkwfs4dRLs2OMdyZ4x/Zd2ERERqVIUtEX8aUiCgrWIiIgApb+OtoiIiIiIFENBW0REREQkABS0RUREREQCQEFbRERERCQAFLRFRERERAJAQVtEREREJAAUtEVEREREAkBBW0REREQkABS0RUREREQCQEFbRERERCQAzDkX7BrOiJllA7uDXYcf1QP2B7uICkZ9VnLqs5JTn5Wc+qzk1GdnR/1WcuqzkmvlnKvljx1V88dOyoJzrn6wa/AnM0tzzsUEu46KRH1WcuqzklOflZz6rOTUZ2dH/VZy6rOSM7M0f+1LU0dERERERAJAQVtEREREJAAUtINndrALqIDUZyWnPis59VnJqc9KTn12dtRvJac+Kzm/9VmFORlSRERERKQi0Yi2iIiIiEgAKGiXATO7x8yyzCzTzB4t0j7RzD71LetTpL2dmW3xLZtpZhacyoPLzMaZmTOzekXa1GfFMLO/mNl2M9tsZi+aWd0iy9RnZ8jM+vr66VMzuz/Y9ZQXZtbEzFaZ2ce+n2NjfO0XmdlbZrbD93hhkW2K/b6rSsws1Mw+MrPlvtfqr9Mws7pmttj38+xjM7ta/XZqZjbW9/9yq5ktNLPz1GfHM7NnzewbM9tapK3EfXRWvzedc/oK4BdwDbACqO573cD32AbYBFQHmgE7gVDfsg+BqwEDXgOuDfZxBKHfmgBv4L12ej312Wn7qzdQzff8z8Cf1Wcl7sNQX/80B8719VubYNdVHr6ARkBb3/NawCe+761Hgft97fefyfddVfoC7gMWAMt9r9Vfp++zecCdvufnAnXVb6fsr8bA58D5vtfPA8PVZ7/qpzigLbC1SFuJ++hsfm9qRDvwRgGPOOcOAzjnvvG19wcWOecOO+c+Bz4F2ptZI6C2c2698/6rzgcGBKHuYHsM+ANQ9CQC9dlJOOfedM7l+16+D1zme64+O3PtgU+dc585534GFuHtvyrPObfPObfR9/wg8DHeX/D98QYjfI8DfM+L/b4r06KDzMwuA64H/lWkWf11CmZWG28gmgPgnPvZOXcA9dvpVAPON7NqQA3gK9Rnx3HOrQG+O6G5RH10tr83FbQDryXQ1cw+MLN3zCzW194Y+KLIel/62hr7np/YXmWYWT9gr3Nu0wmL1GdnZgTev7RBfVYSJ+srKcLMwoCrgA+Ahs65feAN40AD32rqS3gc72DB0SJt6q9Taw5kA8m+KTf/MrMLUL+dlHNuLzAD2APsA35wzr2J+uxMlLSPzur3ZoW5M2R5ZmYrgEuKWTQJbx9fCHQEYoHnzaw53o8dTuRO0V6pnKbPHsA7FeJXmxXTpj6DSc65Zb51JgH5QMqxzYpZv8r0WQmpT07DzGoCS4D/dc79eIrpiVW6L83sBuAb51y6mcWfySbFtFWZ/iqiGt6P9+9xzn1gZn/H+5H+yVT5fvPNK+6Pd4rDAeAFMxt6qk2KaatSfXYG/Pp7U0HbD5xzPU+2zMxGAUt9HzN8aGZHgXp4/xJqUmTVy/B+3PMlv3zsX7S9UjlZn5lZJN4fGJt8v8QvAzaaWXvUZyf9PgMws2HADUAP3/cbVPE+K6GT9ZUAZnYO3pCd4pxb6mv+2swaOef2+T5WPTY1rqr3ZWegn5ldB5wH1Daz51B/nc6XwJfOuQ98rxfjDdrqt5PrCXzunMsGMLOlQCfUZ2eipH10Vr83NXUk8F4CugOYWUu8J3fsB/4D3Gpm1c2sGdAC+ND38cVBM+voO5v1dmBZUCoPAufcFudcA+dcmHMuDO83dlvn3H9Rn52UmfUFJgD9nHO5RRapz87cBqCFmTUzs3OBW/H2X5Xn+x6ZA3zsnPtbkUX/AYb5ng/jl++hYr/vyqreYHPOTXTOXeb7GXYrsNI5NxT11yn5fs5/YWatfE09gG2o305lD9DRzGr4/p/2wHsOhfrs9ErUR2f9e7Oszvisql94g/VzwFZgI9C9yLJJeM9mzaLImatAjG/9ncAT+G4sVBW/gF34rjqiPjtlP32Kd05Zhu/rafXZWfXjdXivqLET75ScoNdUHr6ALng/It1c5HvsOuBi4G1gh+/xoiLbFPt9V9W+gHh+ueqI+uv0/eUB0nzfay/hnXqpfjt1n/0fsN338/zfeK+WoT47vo8W4p3DfgTvAN4dZ9NHZ/N7U3eGFBEREREJAE0dEREREREJAAVtEREREZEAUNAWEREREQkABW0RERERkQBQ0BYRERERCQAFbRERERGRAFDQFhEREREJAAVtEREREZEA+P8BRcYx2pRXraEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# visualize embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = w2v_model.wv.index_to_key\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=5)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "V7h2GmytRiqq",
    "outputId": "1a4fac8b-d815-4444-da85-cfbde23d1482"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00053284,  0.0002355 ,  0.00510643,  0.00900563, -0.00930537,\n",
       "        -0.00712327,  0.00645674,  0.0089788 , -0.0050154 , -0.00376946,\n",
       "         0.00738573, -0.00153804, -0.00453778,  0.00655376, -0.00486056,\n",
       "        -0.00181047,  0.00287806,  0.00099188, -0.00827994, -0.00944862,\n",
       "         0.00731287,  0.00506933,  0.00676219,  0.00076189,  0.00634605,\n",
       "        -0.00340157, -0.00094943,  0.00576983, -0.00751381, -0.0039374 ,\n",
       "        -0.0075127 , -0.00093124,  0.00954387, -0.00731466, -0.00233048,\n",
       "        -0.00193676,  0.00808468, -0.00593122,  0.00004186, -0.00475079,\n",
       "        -0.00960139,  0.00500511, -0.00876237, -0.00438649, -0.00003868,\n",
       "        -0.00029575, -0.00766406,  0.00961257,  0.00498083,  0.00923122,\n",
       "        -0.00816112,  0.00449227, -0.00413935,  0.00082709,  0.00849971,\n",
       "        -0.00445931,  0.00451762, -0.00678583, -0.00353982,  0.00940205,\n",
       "        -0.00158375,  0.00032194, -0.00413758, -0.00768055, -0.00151129,\n",
       "         0.00247213, -0.00089148,  0.00553118, -0.00274054,  0.00226044,\n",
       "         0.005455  ,  0.00834704, -0.00145317, -0.00920385,  0.00437388,\n",
       "         0.00057305,  0.00744097, -0.00081716, -0.00264176, -0.00875302,\n",
       "        -0.0008586 ,  0.00283103,  0.0054063 ,  0.00705656, -0.00570774,\n",
       "         0.00186162,  0.00608684, -0.00479632, -0.0031075 ,  0.00679664,\n",
       "         0.00163663,  0.00018893,  0.00347234,  0.00022199,  0.00961403,\n",
       "         0.00505904, -0.00890995, -0.00704142,  0.00090108,  0.00639393],\n",
       "       dtype=float32),\n",
       " (100,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['sky'], w2v_model.wv['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "3INpkJK4Riqs",
    "outputId": "bebb8557-724d-40c7-bf92-bbeb5e0bf737"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>-0.000533</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>0.009006</td>\n",
       "      <td>-0.009305</td>\n",
       "      <td>-0.007123</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.008979</td>\n",
       "      <td>-0.005015</td>\n",
       "      <td>-0.003769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>-0.008910</td>\n",
       "      <td>-0.007041</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.006394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>-0.008618</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>-0.006173</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>-0.002838</td>\n",
       "      <td>-0.006180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>-0.001575</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>-0.007882</td>\n",
       "      <td>-0.002711</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>-0.002390</td>\n",
       "      <td>-0.009514</td>\n",
       "      <td>0.004512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>-0.006810</td>\n",
       "      <td>-0.001383</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>-0.003669</td>\n",
       "      <td>0.002650</td>\n",
       "      <td>-0.008329</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004499</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>0.009187</td>\n",
       "      <td>-0.004101</td>\n",
       "      <td>0.007971</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.008220</td>\n",
       "      <td>-0.007033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>-0.008245</td>\n",
       "      <td>0.009298</td>\n",
       "      <td>-0.000197</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>-0.004096</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007427</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>-0.000795</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>0.009684</td>\n",
       "      <td>-0.000456</td>\n",
       "      <td>0.005872</td>\n",
       "      <td>-0.007450</td>\n",
       "      <td>-0.002506</td>\n",
       "      <td>-0.005550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>-0.007133</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>-0.007173</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.005829</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>-0.004113</td>\n",
       "      <td>0.007220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>-0.004713</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>-0.004228</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>-0.008046</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.003012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>-0.008727</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>-0.000872</td>\n",
       "      <td>-0.009319</td>\n",
       "      <td>-0.009427</td>\n",
       "      <td>-0.001411</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>-0.006501</td>\n",
       "      <td>-0.006875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.008939</td>\n",
       "      <td>-0.008208</td>\n",
       "      <td>-0.003013</td>\n",
       "      <td>0.009890</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>-0.001589</td>\n",
       "      <td>-0.008696</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>-0.006679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0.008133</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>-0.001070</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.003248</td>\n",
       "      <td>-0.001512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002699</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>-0.003533</td>\n",
       "      <td>-0.000422</td>\n",
       "      <td>-0.000705</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>-0.005735</td>\n",
       "      <td>-0.001659</td>\n",
       "      <td>0.005570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.008170</td>\n",
       "      <td>-0.004445</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.008263</td>\n",
       "      <td>-0.004437</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>-0.003926</td>\n",
       "      <td>-0.005566</td>\n",
       "      <td>-0.006509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>-0.004005</td>\n",
       "      <td>-0.008242</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>-0.001943</td>\n",
       "      <td>-0.000667</td>\n",
       "      <td>-0.001773</td>\n",
       "      <td>-0.004537</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>-0.004270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>-0.009579</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>0.006644</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>-0.004425</td>\n",
       "      <td>-0.006803</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005085</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>-0.001536</td>\n",
       "      <td>0.009932</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>0.005891</td>\n",
       "      <td>-0.005581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>-0.005155</td>\n",
       "      <td>-0.006665</td>\n",
       "      <td>-0.007776</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>-0.001977</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>-0.004149</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>-0.002873</td>\n",
       "      <td>-0.003756</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008972</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>-0.007287</td>\n",
       "      <td>-0.009038</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.009391</td>\n",
       "      <td>0.003507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.007097</td>\n",
       "      <td>-0.001560</td>\n",
       "      <td>0.007953</td>\n",
       "      <td>-0.009501</td>\n",
       "      <td>-0.008024</td>\n",
       "      <td>-0.006648</td>\n",
       "      <td>-0.003998</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.003811</td>\n",
       "      <td>-0.008331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>-0.001259</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>-0.005638</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.009460</td>\n",
       "      <td>-0.005479</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>-0.008114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>-0.006455</td>\n",
       "      <td>-0.001426</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>-0.004620</td>\n",
       "      <td>-0.003995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>-0.003261</td>\n",
       "      <td>-0.009266</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>-0.005632</td>\n",
       "      <td>-0.007864</td>\n",
       "      <td>-0.002973</td>\n",
       "      <td>-0.004928</td>\n",
       "      <td>-0.002319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>-0.001944</td>\n",
       "      <td>-0.005268</td>\n",
       "      <td>0.009447</td>\n",
       "      <td>-0.009299</td>\n",
       "      <td>0.004504</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>-0.001409</td>\n",
       "      <td>0.009007</td>\n",
       "      <td>0.009885</td>\n",
       "      <td>-0.005475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>-0.002565</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>-0.007660</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.008732</td>\n",
       "      <td>0.005983</td>\n",
       "      <td>0.006815</td>\n",
       "      <td>0.007823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>-0.009500</td>\n",
       "      <td>0.009562</td>\n",
       "      <td>-0.007771</td>\n",
       "      <td>-0.002646</td>\n",
       "      <td>-0.004906</td>\n",
       "      <td>-0.004967</td>\n",
       "      <td>-0.008024</td>\n",
       "      <td>-0.007784</td>\n",
       "      <td>-0.004553</td>\n",
       "      <td>-0.001275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.007234</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>-0.001347</td>\n",
       "      <td>-0.005890</td>\n",
       "      <td>-0.004533</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>-0.003135</td>\n",
       "      <td>-0.006339</td>\n",
       "      <td>0.009870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>0.007699</td>\n",
       "      <td>0.009124</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>-0.008330</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>-0.003700</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.009690</td>\n",
       "      <td>-0.009298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007101</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.006382</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>-0.006126</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>-0.006098</td>\n",
       "      <td>0.009436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>-0.007188</td>\n",
       "      <td>0.004236</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>-0.004882</td>\n",
       "      <td>-0.004570</td>\n",
       "      <td>-0.006096</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>-0.004498</td>\n",
       "      <td>0.008516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>-0.005791</td>\n",
       "      <td>-0.008747</td>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.006747</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.006761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>0.001300</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>-0.003130</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007012</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>0.007094</td>\n",
       "      <td>-0.005694</td>\n",
       "      <td>0.007241</td>\n",
       "      <td>-0.009295</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>-0.007757</td>\n",
       "      <td>0.004193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.007055</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>-0.007001</td>\n",
       "      <td>0.007711</td>\n",
       "      <td>-0.006006</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>-0.004019</td>\n",
       "      <td>-0.004710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>-0.003604</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>-0.001865</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>-0.007048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>0.009739</td>\n",
       "      <td>-0.009770</td>\n",
       "      <td>-0.006493</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>-0.005377</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>0.009129</td>\n",
       "      <td>-0.006816</td>\n",
       "      <td>-0.006113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007453</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>-0.002702</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.005627</td>\n",
       "      <td>0.005497</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>0.006559</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>-0.004207</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>-0.005234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000959</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>-0.008595</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>-0.009208</td>\n",
       "      <td>-0.009625</td>\n",
       "      <td>-0.008512</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.009249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5   \\\n",
       "sky       -0.000533  0.000235  0.005106  0.009006 -0.009305 -0.007123   \n",
       "blue      -0.008618  0.003673  0.005192  0.005741  0.007473 -0.006173   \n",
       "lazy       0.000100  0.003081 -0.006810 -0.001383  0.007674  0.007340   \n",
       "beautiful -0.008245  0.009298 -0.000197 -0.001961  0.004605 -0.004096   \n",
       "quick     -0.007133  0.001239 -0.007173 -0.002247  0.003712  0.005829   \n",
       "brown     -0.008727  0.002131 -0.000872 -0.009319 -0.009427 -0.001411   \n",
       "fox        0.008133 -0.004452 -0.001070  0.001003 -0.000184  0.001146   \n",
       "dog        0.008170 -0.004445  0.008983  0.008263 -0.004437  0.000309   \n",
       "sausages  -0.009579  0.008943  0.004165  0.009235  0.006644  0.002925   \n",
       "ham       -0.005155 -0.006665 -0.007776  0.008309 -0.001977 -0.006860   \n",
       "bacon      0.007097 -0.001560  0.007953 -0.009501 -0.008024 -0.006648   \n",
       "eggs       0.009774  0.008167  0.001282  0.005096  0.001410 -0.006455   \n",
       "love      -0.001944 -0.005268  0.009447 -0.009299  0.004504  0.005404   \n",
       "breakfast -0.009500  0.009562 -0.007771 -0.002646 -0.004906 -0.004967   \n",
       "kings      0.007699  0.009124  0.001138 -0.008330  0.008427 -0.003700   \n",
       "green     -0.007188  0.004236  0.002163  0.007438 -0.004882 -0.004570   \n",
       "jumps      0.001300 -0.009804  0.004588 -0.000538  0.006332  0.001783   \n",
       "toast      0.001815  0.007055  0.002960 -0.007001  0.007711 -0.006006   \n",
       "beans      0.009739 -0.009770 -0.006493  0.002774  0.006442 -0.005377   \n",
       "today      0.005627  0.005497  0.001829  0.005749 -0.008968  0.006559   \n",
       "\n",
       "                 6         7         8         9   ...        90        91  \\\n",
       "sky        0.006457  0.008979 -0.005015 -0.003769  ...  0.001637  0.000189   \n",
       "blue       0.001114  0.006053 -0.002838 -0.006180  ...  0.001093 -0.001575   \n",
       "lazy      -0.003669  0.002650 -0.008329  0.006200  ... -0.004499  0.005705   \n",
       "beautiful  0.002745  0.006941  0.006061 -0.007511  ... -0.007427 -0.001066   \n",
       "quick      0.001199  0.002107 -0.004113  0.007220  ...  0.003142 -0.004713   \n",
       "brown      0.004435  0.003705 -0.006501 -0.006875  ...  0.009072  0.008939   \n",
       "fox        0.006115 -0.000016 -0.003248 -0.001512  ... -0.002699  0.000448   \n",
       "dog        0.004274 -0.003926 -0.005566 -0.006509  ...  0.002053 -0.004005   \n",
       "sausages   0.009804 -0.004425 -0.006803  0.004227  ... -0.005085  0.001131   \n",
       "ham       -0.004149  0.005151 -0.002873 -0.003756  ... -0.008972  0.008593   \n",
       "bacon     -0.003998  0.004995 -0.003811 -0.008331  ...  0.007524  0.001501   \n",
       "eggs      -0.001426  0.006450 -0.004620 -0.003995  ...  0.004776 -0.003261   \n",
       "love      -0.001409  0.009007  0.009885 -0.005475  ...  0.002651 -0.002565   \n",
       "breakfast -0.008024 -0.007784 -0.004553 -0.001275  ...  0.008380  0.007234   \n",
       "kings      0.005745  0.004394  0.009690 -0.009298  ...  0.007101  0.001903   \n",
       "green     -0.006096  0.003307 -0.004498  0.008516  ... -0.003475  0.003491   \n",
       "jumps     -0.003130  0.007760  0.001555  0.000055  ...  0.007012  0.004829   \n",
       "toast      0.009003  0.002971 -0.004019 -0.004710  ...  0.009149  0.003593   \n",
       "beans      0.002761  0.009129 -0.006816 -0.006113  ...  0.007453 -0.004294   \n",
       "today      0.009226 -0.004207  0.001608 -0.005234  ... -0.000959  0.001310   \n",
       "\n",
       "                 92        93        94        95        96        97  \\\n",
       "sky        0.003472  0.000222  0.009614  0.005059 -0.008910 -0.007041   \n",
       "blue       0.002202 -0.007882 -0.002711  0.002665  0.005343 -0.002390   \n",
       "lazy       0.009187 -0.004101  0.007971  0.005383  0.005884  0.000506   \n",
       "beautiful -0.000795 -0.002566  0.009684 -0.000456  0.005872 -0.007450   \n",
       "quick      0.005281 -0.004228  0.002642 -0.008046  0.006211  0.004815   \n",
       "brown     -0.008208 -0.003013  0.009890  0.005106 -0.001589 -0.008696   \n",
       "fox       -0.003533 -0.000422 -0.000705  0.000826  0.008197 -0.005735   \n",
       "dog       -0.008242  0.006279 -0.001943 -0.000667 -0.001773 -0.004537   \n",
       "sausages   0.002883 -0.001536  0.009932  0.008350  0.002416  0.007118   \n",
       "ham        0.004052  0.007469  0.009752 -0.007287 -0.009038  0.005833   \n",
       "bacon     -0.001259  0.005773 -0.005638  0.000038  0.009460 -0.005479   \n",
       "eggs      -0.009266  0.003788  0.007163 -0.005632 -0.007864 -0.002973   \n",
       "love       0.006448 -0.007660  0.003394  0.000490  0.008732  0.005983   \n",
       "breakfast  0.001730 -0.001347 -0.005890 -0.004533  0.008648 -0.003135   \n",
       "kings      0.005202  0.006382  0.001914 -0.006126 -0.000006  0.008268   \n",
       "green     -0.005791 -0.008747 -0.005515  0.006747  0.006422  0.009441   \n",
       "jumps      0.008683  0.007094 -0.005694  0.007241 -0.009295 -0.002588   \n",
       "toast      0.006569 -0.003604  0.006792  0.007247 -0.002123 -0.001865   \n",
       "beans      0.004584  0.009089  0.003047  0.003142  0.004065 -0.002702   \n",
       "today     -0.008595  0.008749 -0.009208 -0.009625 -0.008512  0.007313   \n",
       "\n",
       "                 98        99  \n",
       "sky        0.000901  0.006394  \n",
       "blue      -0.009514  0.004512  \n",
       "lazy       0.008220 -0.007033  \n",
       "beautiful -0.002506 -0.005550  \n",
       "quick      0.000790  0.003012  \n",
       "brown      0.002962 -0.006679  \n",
       "fox       -0.001659  0.005570  \n",
       "dog        0.004066 -0.004270  \n",
       "sausages   0.005891 -0.005581  \n",
       "ham        0.009391  0.003507  \n",
       "bacon      0.003814 -0.008114  \n",
       "eggs      -0.004928 -0.002319  \n",
       "love       0.006815  0.007823  \n",
       "breakfast -0.006339  0.009870  \n",
       "kings     -0.006098  0.009436  \n",
       "green      0.007051  0.006761  \n",
       "jumps     -0.007757  0.004193  \n",
       "toast      0.003616 -0.007048  \n",
       "beans      0.003822  0.000336  \n",
       "today      0.005466  0.009249  \n",
       "\n",
       "[20 rows x 100 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df = pd.DataFrame(wvs, index=words)\n",
    "vec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTCiDguVRiqu"
   },
   "source": [
    "### Looking at term semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "id": "S63FJhi2Riqv",
    "outputId": "ca368c74-ffbc-4b7a-9d48-25c2b3aa9358"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sky</th>\n",
       "      <th>blue</th>\n",
       "      <th>lazy</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>quick</th>\n",
       "      <th>brown</th>\n",
       "      <th>fox</th>\n",
       "      <th>dog</th>\n",
       "      <th>sausages</th>\n",
       "      <th>ham</th>\n",
       "      <th>bacon</th>\n",
       "      <th>eggs</th>\n",
       "      <th>love</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>kings</th>\n",
       "      <th>green</th>\n",
       "      <th>jumps</th>\n",
       "      <th>toast</th>\n",
       "      <th>beans</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010486</td>\n",
       "      <td>-0.052628</td>\n",
       "      <td>-0.111528</td>\n",
       "      <td>-0.027551</td>\n",
       "      <td>-0.059903</td>\n",
       "      <td>0.016010</td>\n",
       "      <td>0.093210</td>\n",
       "      <td>0.027032</td>\n",
       "      <td>0.216277</td>\n",
       "      <td>0.063204</td>\n",
       "      <td>0.079641</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>-0.041206</td>\n",
       "      <td>-0.037588</td>\n",
       "      <td>0.054451</td>\n",
       "      <td>0.218867</td>\n",
       "      <td>-0.113870</td>\n",
       "      <td>-0.014342</td>\n",
       "      <td>0.002472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>-0.010486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023673</td>\n",
       "      <td>0.068212</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.009366</td>\n",
       "      <td>-0.113922</td>\n",
       "      <td>-0.115575</td>\n",
       "      <td>0.033609</td>\n",
       "      <td>-0.095451</td>\n",
       "      <td>-0.134269</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>-0.003616</td>\n",
       "      <td>0.137272</td>\n",
       "      <td>0.161227</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.085752</td>\n",
       "      <td>0.159690</td>\n",
       "      <td>-0.159853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>-0.052628</td>\n",
       "      <td>-0.023673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013439</td>\n",
       "      <td>0.170080</td>\n",
       "      <td>0.064290</td>\n",
       "      <td>0.146087</td>\n",
       "      <td>-0.002024</td>\n",
       "      <td>0.199144</td>\n",
       "      <td>-0.032889</td>\n",
       "      <td>-0.101577</td>\n",
       "      <td>0.173161</td>\n",
       "      <td>0.046518</td>\n",
       "      <td>-0.135233</td>\n",
       "      <td>-0.185223</td>\n",
       "      <td>-0.019798</td>\n",
       "      <td>-0.017042</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.152968</td>\n",
       "      <td>-0.035183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>-0.111528</td>\n",
       "      <td>0.068212</td>\n",
       "      <td>-0.013439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.044538</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.041589</td>\n",
       "      <td>-0.013624</td>\n",
       "      <td>0.074971</td>\n",
       "      <td>-0.169292</td>\n",
       "      <td>0.012924</td>\n",
       "      <td>0.041348</td>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>0.178155</td>\n",
       "      <td>-0.153942</td>\n",
       "      <td>-0.260758</td>\n",
       "      <td>0.041376</td>\n",
       "      <td>-0.041950</td>\n",
       "      <td>-0.001830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>-0.027551</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.170080</td>\n",
       "      <td>-0.044538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.138832</td>\n",
       "      <td>0.034770</td>\n",
       "      <td>-0.028082</td>\n",
       "      <td>-0.069024</td>\n",
       "      <td>-0.173125</td>\n",
       "      <td>-0.258496</td>\n",
       "      <td>-0.005777</td>\n",
       "      <td>0.150277</td>\n",
       "      <td>0.252918</td>\n",
       "      <td>-0.086099</td>\n",
       "      <td>0.016038</td>\n",
       "      <td>0.108613</td>\n",
       "      <td>-0.029504</td>\n",
       "      <td>0.032947</td>\n",
       "      <td>-0.276838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>-0.059903</td>\n",
       "      <td>0.009366</td>\n",
       "      <td>0.064290</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.138832</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019214</td>\n",
       "      <td>-0.057632</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>-0.105102</td>\n",
       "      <td>0.019992</td>\n",
       "      <td>0.166978</td>\n",
       "      <td>-0.145121</td>\n",
       "      <td>0.044112</td>\n",
       "      <td>-0.109112</td>\n",
       "      <td>-0.026460</td>\n",
       "      <td>0.047645</td>\n",
       "      <td>0.071818</td>\n",
       "      <td>-0.140769</td>\n",
       "      <td>-0.126858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0.016010</td>\n",
       "      <td>-0.113922</td>\n",
       "      <td>0.146087</td>\n",
       "      <td>0.041589</td>\n",
       "      <td>0.034770</td>\n",
       "      <td>0.019214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>-0.083685</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.012811</td>\n",
       "      <td>0.007866</td>\n",
       "      <td>-0.146393</td>\n",
       "      <td>0.163691</td>\n",
       "      <td>-0.107197</td>\n",
       "      <td>0.074837</td>\n",
       "      <td>0.110690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.093210</td>\n",
       "      <td>-0.115575</td>\n",
       "      <td>-0.002024</td>\n",
       "      <td>-0.013624</td>\n",
       "      <td>-0.028082</td>\n",
       "      <td>-0.057632</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.144459</td>\n",
       "      <td>-0.093106</td>\n",
       "      <td>0.108749</td>\n",
       "      <td>0.111620</td>\n",
       "      <td>-0.040845</td>\n",
       "      <td>-0.025583</td>\n",
       "      <td>0.105429</td>\n",
       "      <td>0.080558</td>\n",
       "      <td>0.174864</td>\n",
       "      <td>0.119101</td>\n",
       "      <td>0.079293</td>\n",
       "      <td>0.318992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>0.027032</td>\n",
       "      <td>0.033609</td>\n",
       "      <td>0.199144</td>\n",
       "      <td>0.074971</td>\n",
       "      <td>-0.069024</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>-0.144459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>0.026763</td>\n",
       "      <td>0.037713</td>\n",
       "      <td>-0.040441</td>\n",
       "      <td>-0.122462</td>\n",
       "      <td>-0.076904</td>\n",
       "      <td>-0.014549</td>\n",
       "      <td>-0.120705</td>\n",
       "      <td>0.012425</td>\n",
       "      <td>0.015794</td>\n",
       "      <td>0.096739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.216277</td>\n",
       "      <td>-0.095451</td>\n",
       "      <td>-0.032889</td>\n",
       "      <td>-0.169292</td>\n",
       "      <td>-0.173125</td>\n",
       "      <td>-0.105102</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>-0.093106</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>-0.074221</td>\n",
       "      <td>-0.045663</td>\n",
       "      <td>-0.106110</td>\n",
       "      <td>-0.045302</td>\n",
       "      <td>0.010598</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>-0.160124</td>\n",
       "      <td>0.020276</td>\n",
       "      <td>-0.037992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.063204</td>\n",
       "      <td>-0.134269</td>\n",
       "      <td>-0.101577</td>\n",
       "      <td>0.012924</td>\n",
       "      <td>-0.258496</td>\n",
       "      <td>0.019992</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>0.108749</td>\n",
       "      <td>0.026763</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109504</td>\n",
       "      <td>0.128159</td>\n",
       "      <td>-0.001125</td>\n",
       "      <td>-0.045570</td>\n",
       "      <td>-0.042557</td>\n",
       "      <td>-0.012008</td>\n",
       "      <td>0.098029</td>\n",
       "      <td>-0.049844</td>\n",
       "      <td>0.086437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.079641</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>0.173161</td>\n",
       "      <td>0.041348</td>\n",
       "      <td>-0.005777</td>\n",
       "      <td>0.166978</td>\n",
       "      <td>-0.083685</td>\n",
       "      <td>0.111620</td>\n",
       "      <td>0.037713</td>\n",
       "      <td>-0.074221</td>\n",
       "      <td>0.109504</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030283</td>\n",
       "      <td>-0.162883</td>\n",
       "      <td>-0.058880</td>\n",
       "      <td>0.013226</td>\n",
       "      <td>-0.148470</td>\n",
       "      <td>0.060082</td>\n",
       "      <td>0.132717</td>\n",
       "      <td>0.097388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.093197</td>\n",
       "      <td>-0.003616</td>\n",
       "      <td>0.046518</td>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.150277</td>\n",
       "      <td>-0.145121</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>-0.040845</td>\n",
       "      <td>-0.040441</td>\n",
       "      <td>-0.045663</td>\n",
       "      <td>0.128159</td>\n",
       "      <td>-0.030283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.076390</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>-0.003628</td>\n",
       "      <td>-0.116366</td>\n",
       "      <td>-0.067320</td>\n",
       "      <td>0.035484</td>\n",
       "      <td>-0.001872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>-0.041206</td>\n",
       "      <td>0.137272</td>\n",
       "      <td>-0.135233</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>0.252918</td>\n",
       "      <td>0.044112</td>\n",
       "      <td>0.012811</td>\n",
       "      <td>-0.025583</td>\n",
       "      <td>-0.122462</td>\n",
       "      <td>-0.106110</td>\n",
       "      <td>-0.001125</td>\n",
       "      <td>-0.162883</td>\n",
       "      <td>-0.076390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027042</td>\n",
       "      <td>-0.144852</td>\n",
       "      <td>-0.032478</td>\n",
       "      <td>0.116573</td>\n",
       "      <td>0.142687</td>\n",
       "      <td>-0.049997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>-0.037588</td>\n",
       "      <td>0.161227</td>\n",
       "      <td>-0.185223</td>\n",
       "      <td>0.178155</td>\n",
       "      <td>-0.086099</td>\n",
       "      <td>-0.109112</td>\n",
       "      <td>0.007866</td>\n",
       "      <td>0.105429</td>\n",
       "      <td>-0.076904</td>\n",
       "      <td>-0.045302</td>\n",
       "      <td>-0.045570</td>\n",
       "      <td>-0.058880</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.027042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.097812</td>\n",
       "      <td>0.048859</td>\n",
       "      <td>0.247344</td>\n",
       "      <td>-0.005284</td>\n",
       "      <td>0.162057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.054451</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>-0.019798</td>\n",
       "      <td>-0.153942</td>\n",
       "      <td>0.016038</td>\n",
       "      <td>-0.026460</td>\n",
       "      <td>-0.146393</td>\n",
       "      <td>0.080558</td>\n",
       "      <td>-0.014549</td>\n",
       "      <td>0.010598</td>\n",
       "      <td>-0.042557</td>\n",
       "      <td>0.013226</td>\n",
       "      <td>-0.003628</td>\n",
       "      <td>-0.144852</td>\n",
       "      <td>-0.097812</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.065524</td>\n",
       "      <td>-0.065974</td>\n",
       "      <td>-0.064708</td>\n",
       "      <td>0.000697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>0.218867</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>-0.017042</td>\n",
       "      <td>-0.260758</td>\n",
       "      <td>0.108613</td>\n",
       "      <td>0.047645</td>\n",
       "      <td>0.163691</td>\n",
       "      <td>0.174864</td>\n",
       "      <td>-0.120705</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>-0.012008</td>\n",
       "      <td>-0.148470</td>\n",
       "      <td>-0.116366</td>\n",
       "      <td>-0.032478</td>\n",
       "      <td>0.048859</td>\n",
       "      <td>0.065524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.148985</td>\n",
       "      <td>0.059518</td>\n",
       "      <td>-0.038573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>-0.113870</td>\n",
       "      <td>0.085752</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.041376</td>\n",
       "      <td>-0.029504</td>\n",
       "      <td>0.071818</td>\n",
       "      <td>-0.107197</td>\n",
       "      <td>0.119101</td>\n",
       "      <td>0.012425</td>\n",
       "      <td>-0.160124</td>\n",
       "      <td>0.098029</td>\n",
       "      <td>0.060082</td>\n",
       "      <td>-0.067320</td>\n",
       "      <td>0.116573</td>\n",
       "      <td>0.247344</td>\n",
       "      <td>-0.065974</td>\n",
       "      <td>-0.148985</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.120099</td>\n",
       "      <td>-0.046950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>-0.014342</td>\n",
       "      <td>0.159690</td>\n",
       "      <td>0.152968</td>\n",
       "      <td>-0.041950</td>\n",
       "      <td>0.032947</td>\n",
       "      <td>-0.140769</td>\n",
       "      <td>0.074837</td>\n",
       "      <td>0.079293</td>\n",
       "      <td>0.015794</td>\n",
       "      <td>0.020276</td>\n",
       "      <td>-0.049844</td>\n",
       "      <td>0.132717</td>\n",
       "      <td>0.035484</td>\n",
       "      <td>0.142687</td>\n",
       "      <td>-0.005284</td>\n",
       "      <td>-0.064708</td>\n",
       "      <td>0.059518</td>\n",
       "      <td>0.120099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.002472</td>\n",
       "      <td>-0.159853</td>\n",
       "      <td>-0.035183</td>\n",
       "      <td>-0.001830</td>\n",
       "      <td>-0.276838</td>\n",
       "      <td>-0.126858</td>\n",
       "      <td>0.110690</td>\n",
       "      <td>0.318992</td>\n",
       "      <td>0.096739</td>\n",
       "      <td>-0.037992</td>\n",
       "      <td>0.086437</td>\n",
       "      <td>0.097388</td>\n",
       "      <td>-0.001872</td>\n",
       "      <td>-0.049997</td>\n",
       "      <td>0.162057</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>-0.038573</td>\n",
       "      <td>-0.046950</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sky      blue      lazy  beautiful     quick     brown  \\\n",
       "sky        1.000000 -0.010486 -0.052628  -0.111528 -0.027551 -0.059903   \n",
       "blue      -0.010486  1.000000 -0.023673   0.068212  0.004253  0.009366   \n",
       "lazy      -0.052628 -0.023673  1.000000  -0.013439  0.170080  0.064290   \n",
       "beautiful -0.111528  0.068212 -0.013439   1.000000 -0.044538  0.131579   \n",
       "quick     -0.027551  0.004253  0.170080  -0.044538  1.000000  0.138832   \n",
       "brown     -0.059903  0.009366  0.064290   0.131579  0.138832  1.000000   \n",
       "fox        0.016010 -0.113922  0.146087   0.041589  0.034770  0.019214   \n",
       "dog        0.093210 -0.115575 -0.002024  -0.013624 -0.028082 -0.057632   \n",
       "sausages   0.027032  0.033609  0.199144   0.074971 -0.069024  0.060613   \n",
       "ham        0.216277 -0.095451 -0.032889  -0.169292 -0.173125 -0.105102   \n",
       "bacon      0.063204 -0.134269 -0.101577   0.012924 -0.258496  0.019992   \n",
       "eggs       0.079641  0.008203  0.173161   0.041348 -0.005777  0.166978   \n",
       "love       0.093197 -0.003616  0.046518  -0.009290  0.150277 -0.145121   \n",
       "breakfast -0.041206  0.137272 -0.135233   0.006648  0.252918  0.044112   \n",
       "kings     -0.037588  0.161227 -0.185223   0.178155 -0.086099 -0.109112   \n",
       "green      0.054451  0.123320 -0.019798  -0.153942  0.016038 -0.026460   \n",
       "jumps      0.218867  0.022355 -0.017042  -0.260758  0.108613  0.047645   \n",
       "toast     -0.113870  0.085752  0.002146   0.041376 -0.029504  0.071818   \n",
       "beans     -0.014342  0.159690  0.152968  -0.041950  0.032947 -0.140769   \n",
       "today      0.002472 -0.159853 -0.035183  -0.001830 -0.276838 -0.126858   \n",
       "\n",
       "                fox       dog  sausages       ham     bacon      eggs  \\\n",
       "sky        0.016010  0.093210  0.027032  0.216277  0.063204  0.079641   \n",
       "blue      -0.113922 -0.115575  0.033609 -0.095451 -0.134269  0.008203   \n",
       "lazy       0.146087 -0.002024  0.199144 -0.032889 -0.101577  0.173161   \n",
       "beautiful  0.041589 -0.013624  0.074971 -0.169292  0.012924  0.041348   \n",
       "quick      0.034770 -0.028082 -0.069024 -0.173125 -0.258496 -0.005777   \n",
       "brown      0.019214 -0.057632  0.060613 -0.105102  0.019992  0.166978   \n",
       "fox        1.000000  0.004815  0.008904  0.001921  0.050417 -0.083685   \n",
       "dog        0.004815  1.000000 -0.144459 -0.093106  0.108749  0.111620   \n",
       "sausages   0.008904 -0.144459  1.000000  0.044722  0.026763  0.037713   \n",
       "ham        0.001921 -0.093106  0.044722  1.000000  0.014945 -0.074221   \n",
       "bacon      0.050417  0.108749  0.026763  0.014945  1.000000  0.109504   \n",
       "eggs      -0.083685  0.111620  0.037713 -0.074221  0.109504  1.000000   \n",
       "love       0.000719 -0.040845 -0.040441 -0.045663  0.128159 -0.030283   \n",
       "breakfast  0.012811 -0.025583 -0.122462 -0.106110 -0.001125 -0.162883   \n",
       "kings      0.007866  0.105429 -0.076904 -0.045302 -0.045570 -0.058880   \n",
       "green     -0.146393  0.080558 -0.014549  0.010598 -0.042557  0.013226   \n",
       "jumps      0.163691  0.174864 -0.120705  0.006430 -0.012008 -0.148470   \n",
       "toast     -0.107197  0.119101  0.012425 -0.160124  0.098029  0.060082   \n",
       "beans      0.074837  0.079293  0.015794  0.020276 -0.049844  0.132717   \n",
       "today      0.110690  0.318992  0.096739 -0.037992  0.086437  0.097388   \n",
       "\n",
       "               love  breakfast     kings     green     jumps     toast  \\\n",
       "sky        0.093197  -0.041206 -0.037588  0.054451  0.218867 -0.113870   \n",
       "blue      -0.003616   0.137272  0.161227  0.123320  0.022355  0.085752   \n",
       "lazy       0.046518  -0.135233 -0.185223 -0.019798 -0.017042  0.002146   \n",
       "beautiful -0.009290   0.006648  0.178155 -0.153942 -0.260758  0.041376   \n",
       "quick      0.150277   0.252918 -0.086099  0.016038  0.108613 -0.029504   \n",
       "brown     -0.145121   0.044112 -0.109112 -0.026460  0.047645  0.071818   \n",
       "fox        0.000719   0.012811  0.007866 -0.146393  0.163691 -0.107197   \n",
       "dog       -0.040845  -0.025583  0.105429  0.080558  0.174864  0.119101   \n",
       "sausages  -0.040441  -0.122462 -0.076904 -0.014549 -0.120705  0.012425   \n",
       "ham       -0.045663  -0.106110 -0.045302  0.010598  0.006430 -0.160124   \n",
       "bacon      0.128159  -0.001125 -0.045570 -0.042557 -0.012008  0.098029   \n",
       "eggs      -0.030283  -0.162883 -0.058880  0.013226 -0.148470  0.060082   \n",
       "love       1.000000  -0.076390  0.092150 -0.003628 -0.116366 -0.067320   \n",
       "breakfast -0.076390   1.000000  0.027042 -0.144852 -0.032478  0.116573   \n",
       "kings      0.092150   0.027042  1.000000 -0.097812  0.048859  0.247344   \n",
       "green     -0.003628  -0.144852 -0.097812  1.000000  0.065524 -0.065974   \n",
       "jumps     -0.116366  -0.032478  0.048859  0.065524  1.000000 -0.148985   \n",
       "toast     -0.067320   0.116573  0.247344 -0.065974 -0.148985  1.000000   \n",
       "beans      0.035484   0.142687 -0.005284 -0.064708  0.059518  0.120099   \n",
       "today     -0.001872  -0.049997  0.162057  0.000697 -0.038573 -0.046950   \n",
       "\n",
       "              beans     today  \n",
       "sky       -0.014342  0.002472  \n",
       "blue       0.159690 -0.159853  \n",
       "lazy       0.152968 -0.035183  \n",
       "beautiful -0.041950 -0.001830  \n",
       "quick      0.032947 -0.276838  \n",
       "brown     -0.140769 -0.126858  \n",
       "fox        0.074837  0.110690  \n",
       "dog        0.079293  0.318992  \n",
       "sausages   0.015794  0.096739  \n",
       "ham        0.020276 -0.037992  \n",
       "bacon     -0.049844  0.086437  \n",
       "eggs       0.132717  0.097388  \n",
       "love       0.035484 -0.001872  \n",
       "breakfast  0.142687 -0.049997  \n",
       "kings     -0.005284  0.162057  \n",
       "green     -0.064708  0.000697  \n",
       "jumps      0.059518 -0.038573  \n",
       "toast      0.120099 -0.046950  \n",
       "beans      1.000000  0.000495  \n",
       "today      0.000495  1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(vec_df.values)\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "ngIHnn__Riqx",
    "outputId": "d05dd28c-5955-4313-ce19-029b87b8dfba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sky                  [jumps, ham, dog]\n",
       "blue         [kings, beans, breakfast]\n",
       "lazy           [sausages, eggs, quick]\n",
       "beautiful     [kings, brown, sausages]\n",
       "quick          [breakfast, lazy, love]\n",
       "brown         [eggs, quick, beautiful]\n",
       "fox               [jumps, lazy, today]\n",
       "dog              [today, jumps, toast]\n",
       "sausages      [lazy, today, beautiful]\n",
       "ham             [sky, sausages, beans]\n",
       "bacon                [love, eggs, dog]\n",
       "eggs              [lazy, brown, beans]\n",
       "love               [quick, bacon, sky]\n",
       "breakfast         [quick, beans, blue]\n",
       "kings        [toast, beautiful, today]\n",
       "green               [blue, dog, jumps]\n",
       "jumps                  [sky, dog, fox]\n",
       "toast              [kings, beans, dog]\n",
       "beans          [blue, lazy, breakfast]\n",
       "today                [dog, kings, fox]\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(words)\n",
    "similarity_df.apply(lambda row: feature_names[np.argsort(-row.values)[1:4]], \n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTUAkYxQRiqz"
   },
   "source": [
    "# The GloVe Model\n",
    "\n",
    "The GloVe model stands for Global Vectors which is an unsupervised learning model which can be used to obtain dense word vectors similar to Word2Vec. However the technique is different and training is performed on an aggregated global word-word co-occurrence matrix, giving us a vector space with meaningful sub-structures. This method was invented in Stanford by Pennington et al. and I recommend you to read the original paper on GloVe, _[‘GloVe: Global Vectors for Word Representation’ by Pennington et al.](https://nlp.stanford.edu/pubs/glove.pdf)_ which is an excellent read to get some perspective on how this model works.\n",
    "\n",
    "The basic methodology of the GloVe model is to first create a huge word-context co-occurence matrix consisting of (word, context) pairs such that each element in this matrix represents how often a word occurs with the context (which can be a sequence of words). The idea then is to apply matrix factorization to approximate this matrix as depicted in the following figure.\n",
    "\n",
    "![](https://github.com/dipanjanS/nlp_workshop_odsc19/blob/master/Module04%20-%20Text%20Representation/glove_arch.png?raw=1)\n",
    "\n",
    "Considering the __Word-Context (WC)__ matrix, __Word-Feature (WF)__ matrix and __Feature-Context (FC)__ matrix, we try to factorize __WC = WF x FC__\n",
    "\n",
    "Such that we we aim to reconstruct __WC__ from __WF__ and __FC__ by multiplying them. For this, we typically initialize __WF__ and __FC__ with some random weights and attempt to multiply them to get __WC'__ (an approximation of __WC__) and measure how close it is to __WC__. We do this multiple times using Stochastic Gradient Descent (SGD) to minimize the error. Finally, the __Word-Feature matrix (WF)__ gives us the word embeddings for each word where __F__ can be preset to a specific number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_UFyeFqfRiq0"
   },
   "source": [
    "# Robust Glove Model with SpaCy\n",
    "\n",
    "Let’s try and leverage GloVe based embeddings for our document clustering task. The very popular spacy framework comes with capabilities to leverage GloVe embeddings based on different language models. You can also get pre-trained word vectors and load them up as needed using gensim or spacy.\n",
    "\n",
    "If you have spacy installed, we will be using the __[`en_vectors_web_lg`](https://spacy.io/models/en#en_vectors_web_lg)__ model which consists of 300-dimensional word vectors trained on [Common Crawl](http://commoncrawl.org) with GloVe.\n",
    "\n",
    "__Install Instructions:__\n",
    "\n",
    "```\n",
    "# Use the following command to install spaCy\n",
    "> pip install -U spacy\n",
    "OR\n",
    "> conda install -c conda-forge spacy\n",
    "\n",
    "C:\\WINDOWS\\system32>python -m spacy download en_vectors_web_lg\n",
    "Collecting en_vectors_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.0.0/en_vectors_web_lg-2.0.0.tar.gz#egg=en_vectors_web_lg==2.0.0\n",
    "  Downloading https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.0.0/en_vectors_web_lg-2.0.0.tar.gz (661.8MB)\n",
    "    100% |████████████████████████████████| 661.8MB 392kB/s\n",
    "Installing collected packages: en-vectors-web-lg\n",
    "  Running setup.py install for en-vectors-web-lg ... done\n",
    "Successfully installed en-vectors-web-lg-2.0.0\n",
    "You are using pip version 10.0.1, however version 18.0 is available.\n",
    "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
    "\n",
    "    Linking successful\n",
    "    C:\\Anaconda3\\lib\\site-packages\\en_vectors_web_lg -->\n",
    "    C:\\Anaconda3\\lib\\site-packages\\spacy\\data\\en_vectors_web_lg\n",
    "\n",
    "    You can now load the model via spacy.load('en_vectors_web_lg')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "geEhPWzgTA7K",
    "outputId": "925b33eb-8de8-417f-d596-da42af328273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\programdata\\miniconda3\\lib\\site-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.10.0.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.62.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.5)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (58.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\peshw\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\programdata\\miniconda3\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
      "[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full\n",
      "pipeline package name 'en_core_web_sm' instead.\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "sz4gggzKTkYx",
    "outputId": "1b8aa213-261e-46d5-96a6-3ab1d2ac32d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[x] No compatible package found for 'en_vectors_web_lg' (spaCy v3.3.1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_vectors_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xCfnNf5fRiq0",
    "outputId": "32218af6-78f9-42f1-fcb4-d46673e47c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word vectors: 342918\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "total_vectors = len(nlp.vocab.vectors)\n",
    "\n",
    "print('Total word vectors:', total_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Gf4dcBuRiq2"
   },
   "source": [
    "This validates that everything is working and in order. Let’s get the GloVe embeddings for each of our words now in our toy corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "colab_type": "code",
    "id": "sd68_DV0Riq3",
    "outputId": "0beef2e4-3e0e-4290-deee-4163bb2681bc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>-0.156570</td>\n",
       "      <td>0.594890</td>\n",
       "      <td>-0.031445</td>\n",
       "      <td>-0.077586</td>\n",
       "      <td>0.278630</td>\n",
       "      <td>-0.509210</td>\n",
       "      <td>-0.066350</td>\n",
       "      <td>-0.081890</td>\n",
       "      <td>-0.047986</td>\n",
       "      <td>2.80360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326580</td>\n",
       "      <td>-0.413380</td>\n",
       "      <td>0.367910</td>\n",
       "      <td>-0.262630</td>\n",
       "      <td>-0.203690</td>\n",
       "      <td>-0.296560</td>\n",
       "      <td>-0.014873</td>\n",
       "      <td>-0.250060</td>\n",
       "      <td>-0.115940</td>\n",
       "      <td>0.083741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>-0.417810</td>\n",
       "      <td>-0.035192</td>\n",
       "      <td>-0.126150</td>\n",
       "      <td>-0.215930</td>\n",
       "      <td>-0.669740</td>\n",
       "      <td>0.513250</td>\n",
       "      <td>-0.797090</td>\n",
       "      <td>-0.068611</td>\n",
       "      <td>0.634660</td>\n",
       "      <td>1.25630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232860</td>\n",
       "      <td>-0.139740</td>\n",
       "      <td>-0.681080</td>\n",
       "      <td>-0.370920</td>\n",
       "      <td>-0.545510</td>\n",
       "      <td>0.073728</td>\n",
       "      <td>0.111620</td>\n",
       "      <td>-0.324700</td>\n",
       "      <td>0.059721</td>\n",
       "      <td>0.159160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>-0.334840</td>\n",
       "      <td>0.215990</td>\n",
       "      <td>-0.350440</td>\n",
       "      <td>-0.260020</td>\n",
       "      <td>0.411070</td>\n",
       "      <td>0.154010</td>\n",
       "      <td>-0.386110</td>\n",
       "      <td>0.206380</td>\n",
       "      <td>0.386700</td>\n",
       "      <td>1.46050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107030</td>\n",
       "      <td>-0.279480</td>\n",
       "      <td>-0.186200</td>\n",
       "      <td>-0.543140</td>\n",
       "      <td>-0.479980</td>\n",
       "      <td>-0.284680</td>\n",
       "      <td>0.036022</td>\n",
       "      <td>0.190290</td>\n",
       "      <td>0.692290</td>\n",
       "      <td>-0.071501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>-0.374120</td>\n",
       "      <td>-0.076264</td>\n",
       "      <td>0.109260</td>\n",
       "      <td>0.186620</td>\n",
       "      <td>0.029943</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>-0.631980</td>\n",
       "      <td>0.133060</td>\n",
       "      <td>-0.128980</td>\n",
       "      <td>0.60343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015404</td>\n",
       "      <td>0.392890</td>\n",
       "      <td>-0.034826</td>\n",
       "      <td>-0.720300</td>\n",
       "      <td>-0.365320</td>\n",
       "      <td>0.740510</td>\n",
       "      <td>0.108390</td>\n",
       "      <td>-0.365760</td>\n",
       "      <td>-0.288190</td>\n",
       "      <td>0.114630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>-0.445630</td>\n",
       "      <td>0.191510</td>\n",
       "      <td>-0.249210</td>\n",
       "      <td>0.465900</td>\n",
       "      <td>0.161950</td>\n",
       "      <td>0.212780</td>\n",
       "      <td>-0.046480</td>\n",
       "      <td>0.021170</td>\n",
       "      <td>0.417660</td>\n",
       "      <td>1.68690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329460</td>\n",
       "      <td>0.421860</td>\n",
       "      <td>-0.039543</td>\n",
       "      <td>0.150180</td>\n",
       "      <td>0.338220</td>\n",
       "      <td>0.049554</td>\n",
       "      <td>0.149420</td>\n",
       "      <td>-0.038789</td>\n",
       "      <td>-0.019069</td>\n",
       "      <td>0.348650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>0.171200</td>\n",
       "      <td>0.534390</td>\n",
       "      <td>-0.348540</td>\n",
       "      <td>-0.097234</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>-0.170860</td>\n",
       "      <td>0.295650</td>\n",
       "      <td>-0.041816</td>\n",
       "      <td>-0.516550</td>\n",
       "      <td>2.11720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285540</td>\n",
       "      <td>0.104670</td>\n",
       "      <td>0.126310</td>\n",
       "      <td>0.120040</td>\n",
       "      <td>0.254380</td>\n",
       "      <td>0.247400</td>\n",
       "      <td>0.207670</td>\n",
       "      <td>0.172580</td>\n",
       "      <td>0.063875</td>\n",
       "      <td>0.350990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>-0.353320</td>\n",
       "      <td>-0.299710</td>\n",
       "      <td>-0.176230</td>\n",
       "      <td>-0.321940</td>\n",
       "      <td>-0.385640</td>\n",
       "      <td>0.586110</td>\n",
       "      <td>0.411160</td>\n",
       "      <td>-0.418680</td>\n",
       "      <td>0.073093</td>\n",
       "      <td>1.48650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402310</td>\n",
       "      <td>-0.038554</td>\n",
       "      <td>-0.288670</td>\n",
       "      <td>-0.244130</td>\n",
       "      <td>0.460990</td>\n",
       "      <td>0.514170</td>\n",
       "      <td>0.136260</td>\n",
       "      <td>0.344190</td>\n",
       "      <td>-0.845300</td>\n",
       "      <td>-0.077383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>0.129450</td>\n",
       "      <td>0.036518</td>\n",
       "      <td>0.032298</td>\n",
       "      <td>-0.060034</td>\n",
       "      <td>0.399840</td>\n",
       "      <td>-0.103020</td>\n",
       "      <td>-0.507880</td>\n",
       "      <td>0.076630</td>\n",
       "      <td>-0.422920</td>\n",
       "      <td>0.81573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.501280</td>\n",
       "      <td>0.169010</td>\n",
       "      <td>0.548250</td>\n",
       "      <td>-0.319380</td>\n",
       "      <td>-0.072887</td>\n",
       "      <td>0.382950</td>\n",
       "      <td>0.237410</td>\n",
       "      <td>0.052289</td>\n",
       "      <td>0.182060</td>\n",
       "      <td>0.412640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>0.130740</td>\n",
       "      <td>-0.193730</td>\n",
       "      <td>0.253270</td>\n",
       "      <td>0.090102</td>\n",
       "      <td>-0.272580</td>\n",
       "      <td>-0.030571</td>\n",
       "      <td>0.096945</td>\n",
       "      <td>-0.115060</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>0.84838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142080</td>\n",
       "      <td>0.481910</td>\n",
       "      <td>0.045167</td>\n",
       "      <td>0.057151</td>\n",
       "      <td>-0.149520</td>\n",
       "      <td>-0.495130</td>\n",
       "      <td>-0.086677</td>\n",
       "      <td>-0.569040</td>\n",
       "      <td>-0.359290</td>\n",
       "      <td>0.097443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>-0.174290</td>\n",
       "      <td>-0.064869</td>\n",
       "      <td>-0.046976</td>\n",
       "      <td>0.287420</td>\n",
       "      <td>-0.128150</td>\n",
       "      <td>0.647630</td>\n",
       "      <td>0.056315</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>-0.025094</td>\n",
       "      <td>0.50222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302240</td>\n",
       "      <td>0.195470</td>\n",
       "      <td>-0.653980</td>\n",
       "      <td>-0.291150</td>\n",
       "      <td>-0.684290</td>\n",
       "      <td>-0.266370</td>\n",
       "      <td>0.304310</td>\n",
       "      <td>-0.806830</td>\n",
       "      <td>0.619540</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>-0.072368</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.137260</td>\n",
       "      <td>-0.156630</td>\n",
       "      <td>0.248440</td>\n",
       "      <td>0.349870</td>\n",
       "      <td>-0.241700</td>\n",
       "      <td>-0.091426</td>\n",
       "      <td>-0.530150</td>\n",
       "      <td>1.34130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.405170</td>\n",
       "      <td>0.243570</td>\n",
       "      <td>0.437300</td>\n",
       "      <td>-0.461520</td>\n",
       "      <td>-0.352710</td>\n",
       "      <td>0.336250</td>\n",
       "      <td>0.069899</td>\n",
       "      <td>-0.111550</td>\n",
       "      <td>0.532930</td>\n",
       "      <td>0.712680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>-0.423290</td>\n",
       "      <td>-0.264500</td>\n",
       "      <td>0.200870</td>\n",
       "      <td>0.082187</td>\n",
       "      <td>0.066944</td>\n",
       "      <td>1.027600</td>\n",
       "      <td>-0.989140</td>\n",
       "      <td>-0.259950</td>\n",
       "      <td>0.145960</td>\n",
       "      <td>0.76645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048760</td>\n",
       "      <td>0.351680</td>\n",
       "      <td>-0.786260</td>\n",
       "      <td>-0.368790</td>\n",
       "      <td>-0.528640</td>\n",
       "      <td>0.287650</td>\n",
       "      <td>-0.273120</td>\n",
       "      <td>-1.114000</td>\n",
       "      <td>0.064322</td>\n",
       "      <td>0.223620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>-0.430730</td>\n",
       "      <td>-0.016025</td>\n",
       "      <td>0.484620</td>\n",
       "      <td>0.101390</td>\n",
       "      <td>-0.299200</td>\n",
       "      <td>0.761820</td>\n",
       "      <td>-0.353130</td>\n",
       "      <td>-0.325290</td>\n",
       "      <td>0.156730</td>\n",
       "      <td>0.87321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304240</td>\n",
       "      <td>0.413440</td>\n",
       "      <td>-0.540730</td>\n",
       "      <td>-0.035930</td>\n",
       "      <td>-0.429450</td>\n",
       "      <td>-0.246590</td>\n",
       "      <td>0.161490</td>\n",
       "      <td>-1.065400</td>\n",
       "      <td>-0.244940</td>\n",
       "      <td>0.269540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.139490</td>\n",
       "      <td>0.534530</td>\n",
       "      <td>-0.252470</td>\n",
       "      <td>-0.125650</td>\n",
       "      <td>0.048748</td>\n",
       "      <td>0.152440</td>\n",
       "      <td>0.199060</td>\n",
       "      <td>-0.065970</td>\n",
       "      <td>0.128830</td>\n",
       "      <td>2.05590</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124380</td>\n",
       "      <td>0.178440</td>\n",
       "      <td>-0.099469</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.089213</td>\n",
       "      <td>-0.075513</td>\n",
       "      <td>-0.049069</td>\n",
       "      <td>-0.015228</td>\n",
       "      <td>0.088408</td>\n",
       "      <td>0.302170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>0.073378</td>\n",
       "      <td>0.227670</td>\n",
       "      <td>0.208420</td>\n",
       "      <td>-0.456790</td>\n",
       "      <td>-0.078219</td>\n",
       "      <td>0.601960</td>\n",
       "      <td>-0.024494</td>\n",
       "      <td>-0.467980</td>\n",
       "      <td>0.054627</td>\n",
       "      <td>2.28370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647710</td>\n",
       "      <td>0.373820</td>\n",
       "      <td>0.019931</td>\n",
       "      <td>-0.033672</td>\n",
       "      <td>-0.073184</td>\n",
       "      <td>0.296830</td>\n",
       "      <td>0.340420</td>\n",
       "      <td>-0.599390</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>0.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>0.259230</td>\n",
       "      <td>-0.854690</td>\n",
       "      <td>0.360010</td>\n",
       "      <td>-0.642000</td>\n",
       "      <td>0.568530</td>\n",
       "      <td>-0.321420</td>\n",
       "      <td>0.173250</td>\n",
       "      <td>0.133030</td>\n",
       "      <td>-0.089720</td>\n",
       "      <td>1.52860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.470090</td>\n",
       "      <td>0.063743</td>\n",
       "      <td>-0.545210</td>\n",
       "      <td>-0.192310</td>\n",
       "      <td>-0.301020</td>\n",
       "      <td>1.068500</td>\n",
       "      <td>0.231160</td>\n",
       "      <td>-0.147330</td>\n",
       "      <td>0.662490</td>\n",
       "      <td>-0.577420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>0.312550</td>\n",
       "      <td>-0.303080</td>\n",
       "      <td>0.019587</td>\n",
       "      <td>-0.354940</td>\n",
       "      <td>0.100180</td>\n",
       "      <td>-0.141530</td>\n",
       "      <td>-0.514270</td>\n",
       "      <td>0.886110</td>\n",
       "      <td>-0.530540</td>\n",
       "      <td>1.55660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.667050</td>\n",
       "      <td>0.279110</td>\n",
       "      <td>0.500970</td>\n",
       "      <td>-0.277580</td>\n",
       "      <td>-0.143720</td>\n",
       "      <td>0.342710</td>\n",
       "      <td>0.287580</td>\n",
       "      <td>0.537740</td>\n",
       "      <td>0.363490</td>\n",
       "      <td>0.496920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>-0.348680</td>\n",
       "      <td>-0.077720</td>\n",
       "      <td>0.177750</td>\n",
       "      <td>-0.094953</td>\n",
       "      <td>-0.452890</td>\n",
       "      <td>0.237790</td>\n",
       "      <td>0.209440</td>\n",
       "      <td>0.037886</td>\n",
       "      <td>0.035064</td>\n",
       "      <td>0.89901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283050</td>\n",
       "      <td>0.270240</td>\n",
       "      <td>-0.654800</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>-0.068738</td>\n",
       "      <td>-0.534750</td>\n",
       "      <td>0.061783</td>\n",
       "      <td>0.123610</td>\n",
       "      <td>-0.553700</td>\n",
       "      <td>-0.544790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>-0.401760</td>\n",
       "      <td>0.370570</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>-0.341250</td>\n",
       "      <td>0.049538</td>\n",
       "      <td>0.294400</td>\n",
       "      <td>-0.173760</td>\n",
       "      <td>-0.279820</td>\n",
       "      <td>0.067622</td>\n",
       "      <td>2.16930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022908</td>\n",
       "      <td>-0.259290</td>\n",
       "      <td>-0.308620</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>-0.189620</td>\n",
       "      <td>0.547890</td>\n",
       "      <td>0.311940</td>\n",
       "      <td>0.246930</td>\n",
       "      <td>0.299290</td>\n",
       "      <td>-0.074861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>-0.773320</td>\n",
       "      <td>-0.282540</td>\n",
       "      <td>0.580760</td>\n",
       "      <td>0.841480</td>\n",
       "      <td>0.258540</td>\n",
       "      <td>0.585210</td>\n",
       "      <td>-0.021890</td>\n",
       "      <td>-0.463680</td>\n",
       "      <td>0.139070</td>\n",
       "      <td>0.65872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464470</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>-0.829200</td>\n",
       "      <td>0.354910</td>\n",
       "      <td>0.224530</td>\n",
       "      <td>-0.493920</td>\n",
       "      <td>0.456930</td>\n",
       "      <td>-0.649100</td>\n",
       "      <td>-0.131930</td>\n",
       "      <td>0.372040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5    \\\n",
       "today     -0.156570  0.594890 -0.031445 -0.077586  0.278630 -0.509210   \n",
       "eggs      -0.417810 -0.035192 -0.126150 -0.215930 -0.669740  0.513250   \n",
       "jumps     -0.334840  0.215990 -0.350440 -0.260020  0.411070  0.154010   \n",
       "brown     -0.374120 -0.076264  0.109260  0.186620  0.029943  0.182700   \n",
       "quick     -0.445630  0.191510 -0.249210  0.465900  0.161950  0.212780   \n",
       "beautiful  0.171200  0.534390 -0.348540 -0.097234  0.101800 -0.170860   \n",
       "lazy      -0.353320 -0.299710 -0.176230 -0.321940 -0.385640  0.586110   \n",
       "blue       0.129450  0.036518  0.032298 -0.060034  0.399840 -0.103020   \n",
       "toast      0.130740 -0.193730  0.253270  0.090102 -0.272580 -0.030571   \n",
       "sausages  -0.174290 -0.064869 -0.046976  0.287420 -0.128150  0.647630   \n",
       "green     -0.072368  0.233200  0.137260 -0.156630  0.248440  0.349870   \n",
       "beans     -0.423290 -0.264500  0.200870  0.082187  0.066944  1.027600   \n",
       "bacon     -0.430730 -0.016025  0.484620  0.101390 -0.299200  0.761820   \n",
       "love       0.139490  0.534530 -0.252470 -0.125650  0.048748  0.152440   \n",
       "breakfast  0.073378  0.227670  0.208420 -0.456790 -0.078219  0.601960   \n",
       "kings      0.259230 -0.854690  0.360010 -0.642000  0.568530 -0.321420   \n",
       "sky        0.312550 -0.303080  0.019587 -0.354940  0.100180 -0.141530   \n",
       "fox       -0.348680 -0.077720  0.177750 -0.094953 -0.452890  0.237790   \n",
       "dog       -0.401760  0.370570  0.021281 -0.341250  0.049538  0.294400   \n",
       "ham       -0.773320 -0.282540  0.580760  0.841480  0.258540  0.585210   \n",
       "\n",
       "                6         7         8        9    ...       290       291  \\\n",
       "today     -0.066350 -0.081890 -0.047986  2.80360  ... -0.326580 -0.413380   \n",
       "eggs      -0.797090 -0.068611  0.634660  1.25630  ... -0.232860 -0.139740   \n",
       "jumps     -0.386110  0.206380  0.386700  1.46050  ... -0.107030 -0.279480   \n",
       "brown     -0.631980  0.133060 -0.128980  0.60343  ... -0.015404  0.392890   \n",
       "quick     -0.046480  0.021170  0.417660  1.68690  ... -0.329460  0.421860   \n",
       "beautiful  0.295650 -0.041816 -0.516550  2.11720  ... -0.285540  0.104670   \n",
       "lazy       0.411160 -0.418680  0.073093  1.48650  ...  0.402310 -0.038554   \n",
       "blue      -0.507880  0.076630 -0.422920  0.81573  ... -0.501280  0.169010   \n",
       "toast      0.096945 -0.115060  0.484000  0.84838  ...  0.142080  0.481910   \n",
       "sausages   0.056315 -0.240440 -0.025094  0.50222  ...  0.302240  0.195470   \n",
       "green     -0.241700 -0.091426 -0.530150  1.34130  ... -0.405170  0.243570   \n",
       "beans     -0.989140 -0.259950  0.145960  0.76645  ...  0.048760  0.351680   \n",
       "bacon     -0.353130 -0.325290  0.156730  0.87321  ...  0.304240  0.413440   \n",
       "love       0.199060 -0.065970  0.128830  2.05590  ... -0.124380  0.178440   \n",
       "breakfast -0.024494 -0.467980  0.054627  2.28370  ...  0.647710  0.373820   \n",
       "kings      0.173250  0.133030 -0.089720  1.52860  ... -0.470090  0.063743   \n",
       "sky       -0.514270  0.886110 -0.530540  1.55660  ... -0.667050  0.279110   \n",
       "fox        0.209440  0.037886  0.035064  0.89901  ... -0.283050  0.270240   \n",
       "dog       -0.173760 -0.279820  0.067622  2.16930  ...  0.022908 -0.259290   \n",
       "ham       -0.021890 -0.463680  0.139070  0.65872  ...  0.464470  0.481400   \n",
       "\n",
       "                292       293       294       295       296       297  \\\n",
       "today      0.367910 -0.262630 -0.203690 -0.296560 -0.014873 -0.250060   \n",
       "eggs      -0.681080 -0.370920 -0.545510  0.073728  0.111620 -0.324700   \n",
       "jumps     -0.186200 -0.543140 -0.479980 -0.284680  0.036022  0.190290   \n",
       "brown     -0.034826 -0.720300 -0.365320  0.740510  0.108390 -0.365760   \n",
       "quick     -0.039543  0.150180  0.338220  0.049554  0.149420 -0.038789   \n",
       "beautiful  0.126310  0.120040  0.254380  0.247400  0.207670  0.172580   \n",
       "lazy      -0.288670 -0.244130  0.460990  0.514170  0.136260  0.344190   \n",
       "blue       0.548250 -0.319380 -0.072887  0.382950  0.237410  0.052289   \n",
       "toast      0.045167  0.057151 -0.149520 -0.495130 -0.086677 -0.569040   \n",
       "sausages  -0.653980 -0.291150 -0.684290 -0.266370  0.304310 -0.806830   \n",
       "green      0.437300 -0.461520 -0.352710  0.336250  0.069899 -0.111550   \n",
       "beans     -0.786260 -0.368790 -0.528640  0.287650 -0.273120 -1.114000   \n",
       "bacon     -0.540730 -0.035930 -0.429450 -0.246590  0.161490 -1.065400   \n",
       "love      -0.099469  0.008682  0.089213 -0.075513 -0.049069 -0.015228   \n",
       "breakfast  0.019931 -0.033672 -0.073184  0.296830  0.340420 -0.599390   \n",
       "kings     -0.545210 -0.192310 -0.301020  1.068500  0.231160 -0.147330   \n",
       "sky        0.500970 -0.277580 -0.143720  0.342710  0.287580  0.537740   \n",
       "fox       -0.654800  0.105300 -0.068738 -0.534750  0.061783  0.123610   \n",
       "dog       -0.308620  0.001754 -0.189620  0.547890  0.311940  0.246930   \n",
       "ham       -0.829200  0.354910  0.224530 -0.493920  0.456930 -0.649100   \n",
       "\n",
       "                298       299  \n",
       "today     -0.115940  0.083741  \n",
       "eggs       0.059721  0.159160  \n",
       "jumps      0.692290 -0.071501  \n",
       "brown     -0.288190  0.114630  \n",
       "quick     -0.019069  0.348650  \n",
       "beautiful  0.063875  0.350990  \n",
       "lazy      -0.845300 -0.077383  \n",
       "blue       0.182060  0.412640  \n",
       "toast     -0.359290  0.097443  \n",
       "sausages   0.619540  0.201200  \n",
       "green      0.532930  0.712680  \n",
       "beans      0.064322  0.223620  \n",
       "bacon     -0.244940  0.269540  \n",
       "love       0.088408  0.302170  \n",
       "breakfast -0.061114  0.232200  \n",
       "kings      0.662490 -0.577420  \n",
       "sky        0.363490  0.496920  \n",
       "fox       -0.553700 -0.544790  \n",
       "dog        0.299290 -0.074861  \n",
       "ham       -0.131930  0.372040  \n",
       "\n",
       "[20 rows x 300 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set([word for sublist in tokenized_corpus for word in sublist]))\n",
    "\n",
    "word_glove_vectors = np.array([nlp(word).vector for word in unique_words])\n",
    "vec_df = pd.DataFrame(word_glove_vectors, index=unique_words)\n",
    "vec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSKGVbkeRiq5"
   },
   "source": [
    "We can now use t-SNE to visualize these embeddings similar to what we did using our Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "j6aSesHGRiq5",
    "outputId": "50fb644e-cbd4-4937-ac07-25b83d2f331c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(word_glove_vectors)\n",
    "labels = unique_words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='red', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ce6BV5bFRiq7"
   },
   "source": [
    "### Looking at term semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "id": "fyO3JzS2Riq8",
    "outputId": "9b88675d-eb0f-4693-b318-a43bddcaeaf2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>today</th>\n",
       "      <th>eggs</th>\n",
       "      <th>jumps</th>\n",
       "      <th>brown</th>\n",
       "      <th>quick</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>lazy</th>\n",
       "      <th>blue</th>\n",
       "      <th>toast</th>\n",
       "      <th>sausages</th>\n",
       "      <th>green</th>\n",
       "      <th>beans</th>\n",
       "      <th>bacon</th>\n",
       "      <th>love</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>kings</th>\n",
       "      <th>sky</th>\n",
       "      <th>fox</th>\n",
       "      <th>dog</th>\n",
       "      <th>ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.183048</td>\n",
       "      <td>0.142099</td>\n",
       "      <td>0.147418</td>\n",
       "      <td>0.370023</td>\n",
       "      <td>0.314184</td>\n",
       "      <td>0.286803</td>\n",
       "      <td>0.180088</td>\n",
       "      <td>0.174257</td>\n",
       "      <td>0.094879</td>\n",
       "      <td>0.246754</td>\n",
       "      <td>0.147652</td>\n",
       "      <td>0.158949</td>\n",
       "      <td>0.371650</td>\n",
       "      <td>0.288788</td>\n",
       "      <td>0.204743</td>\n",
       "      <td>0.256770</td>\n",
       "      <td>0.150072</td>\n",
       "      <td>0.223730</td>\n",
       "      <td>0.104115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.183048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150621</td>\n",
       "      <td>0.466903</td>\n",
       "      <td>0.245487</td>\n",
       "      <td>0.214437</td>\n",
       "      <td>0.236769</td>\n",
       "      <td>0.297546</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.548311</td>\n",
       "      <td>0.392072</td>\n",
       "      <td>0.585054</td>\n",
       "      <td>0.620538</td>\n",
       "      <td>0.254177</td>\n",
       "      <td>0.431108</td>\n",
       "      <td>0.118819</td>\n",
       "      <td>0.223585</td>\n",
       "      <td>0.247314</td>\n",
       "      <td>0.291202</td>\n",
       "      <td>0.489116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>0.142099</td>\n",
       "      <td>0.150621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.301096</td>\n",
       "      <td>0.142554</td>\n",
       "      <td>0.222112</td>\n",
       "      <td>0.187263</td>\n",
       "      <td>0.119099</td>\n",
       "      <td>0.083306</td>\n",
       "      <td>0.150752</td>\n",
       "      <td>0.095544</td>\n",
       "      <td>0.091021</td>\n",
       "      <td>0.185114</td>\n",
       "      <td>0.103865</td>\n",
       "      <td>0.086659</td>\n",
       "      <td>0.278595</td>\n",
       "      <td>0.250834</td>\n",
       "      <td>0.307961</td>\n",
       "      <td>0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>0.147418</td>\n",
       "      <td>0.466903</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.217009</td>\n",
       "      <td>0.355700</td>\n",
       "      <td>0.256875</td>\n",
       "      <td>0.683011</td>\n",
       "      <td>0.347578</td>\n",
       "      <td>0.289625</td>\n",
       "      <td>0.646850</td>\n",
       "      <td>0.453427</td>\n",
       "      <td>0.432891</td>\n",
       "      <td>0.331943</td>\n",
       "      <td>0.277158</td>\n",
       "      <td>0.193487</td>\n",
       "      <td>0.408844</td>\n",
       "      <td>0.406912</td>\n",
       "      <td>0.341204</td>\n",
       "      <td>0.335111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>0.370023</td>\n",
       "      <td>0.245487</td>\n",
       "      <td>0.301096</td>\n",
       "      <td>0.217009</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.289178</td>\n",
       "      <td>0.400060</td>\n",
       "      <td>0.220331</td>\n",
       "      <td>0.292624</td>\n",
       "      <td>0.162966</td>\n",
       "      <td>0.287504</td>\n",
       "      <td>0.251534</td>\n",
       "      <td>0.265370</td>\n",
       "      <td>0.292446</td>\n",
       "      <td>0.358221</td>\n",
       "      <td>0.127679</td>\n",
       "      <td>0.192515</td>\n",
       "      <td>0.192686</td>\n",
       "      <td>0.299892</td>\n",
       "      <td>0.191665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>0.314184</td>\n",
       "      <td>0.214437</td>\n",
       "      <td>0.142554</td>\n",
       "      <td>0.355700</td>\n",
       "      <td>0.289178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.294308</td>\n",
       "      <td>0.461366</td>\n",
       "      <td>0.189482</td>\n",
       "      <td>0.114540</td>\n",
       "      <td>0.392392</td>\n",
       "      <td>0.165268</td>\n",
       "      <td>0.151157</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.314472</td>\n",
       "      <td>0.158615</td>\n",
       "      <td>0.428081</td>\n",
       "      <td>0.210050</td>\n",
       "      <td>0.280659</td>\n",
       "      <td>0.110936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>0.286803</td>\n",
       "      <td>0.236769</td>\n",
       "      <td>0.222112</td>\n",
       "      <td>0.256875</td>\n",
       "      <td>0.400060</td>\n",
       "      <td>0.294308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230513</td>\n",
       "      <td>0.231329</td>\n",
       "      <td>0.168205</td>\n",
       "      <td>0.230945</td>\n",
       "      <td>0.245672</td>\n",
       "      <td>0.264376</td>\n",
       "      <td>0.339281</td>\n",
       "      <td>0.319941</td>\n",
       "      <td>0.190515</td>\n",
       "      <td>0.259361</td>\n",
       "      <td>0.267240</td>\n",
       "      <td>0.301678</td>\n",
       "      <td>0.222571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>0.180088</td>\n",
       "      <td>0.297546</td>\n",
       "      <td>0.187263</td>\n",
       "      <td>0.683011</td>\n",
       "      <td>0.220331</td>\n",
       "      <td>0.461366</td>\n",
       "      <td>0.230513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.232264</td>\n",
       "      <td>0.097443</td>\n",
       "      <td>0.764083</td>\n",
       "      <td>0.261304</td>\n",
       "      <td>0.242987</td>\n",
       "      <td>0.364961</td>\n",
       "      <td>0.197103</td>\n",
       "      <td>0.207656</td>\n",
       "      <td>0.627800</td>\n",
       "      <td>0.371178</td>\n",
       "      <td>0.314065</td>\n",
       "      <td>0.185452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>0.174257</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.119099</td>\n",
       "      <td>0.347578</td>\n",
       "      <td>0.292624</td>\n",
       "      <td>0.189482</td>\n",
       "      <td>0.231329</td>\n",
       "      <td>0.232264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498062</td>\n",
       "      <td>0.287608</td>\n",
       "      <td>0.449284</td>\n",
       "      <td>0.622701</td>\n",
       "      <td>0.274246</td>\n",
       "      <td>0.513436</td>\n",
       "      <td>0.125741</td>\n",
       "      <td>0.216372</td>\n",
       "      <td>0.163371</td>\n",
       "      <td>0.182994</td>\n",
       "      <td>0.500586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>0.094879</td>\n",
       "      <td>0.548311</td>\n",
       "      <td>0.083306</td>\n",
       "      <td>0.289625</td>\n",
       "      <td>0.162966</td>\n",
       "      <td>0.114540</td>\n",
       "      <td>0.168205</td>\n",
       "      <td>0.097443</td>\n",
       "      <td>0.498062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.233169</td>\n",
       "      <td>0.546647</td>\n",
       "      <td>0.729162</td>\n",
       "      <td>0.146917</td>\n",
       "      <td>0.415231</td>\n",
       "      <td>0.070071</td>\n",
       "      <td>0.061753</td>\n",
       "      <td>0.133348</td>\n",
       "      <td>0.247529</td>\n",
       "      <td>0.622770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.246754</td>\n",
       "      <td>0.392072</td>\n",
       "      <td>0.150752</td>\n",
       "      <td>0.646850</td>\n",
       "      <td>0.287504</td>\n",
       "      <td>0.392392</td>\n",
       "      <td>0.230945</td>\n",
       "      <td>0.764083</td>\n",
       "      <td>0.287608</td>\n",
       "      <td>0.233169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.462498</td>\n",
       "      <td>0.386664</td>\n",
       "      <td>0.327302</td>\n",
       "      <td>0.288437</td>\n",
       "      <td>0.151519</td>\n",
       "      <td>0.488385</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.272693</td>\n",
       "      <td>0.290761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>0.147652</td>\n",
       "      <td>0.585054</td>\n",
       "      <td>0.095544</td>\n",
       "      <td>0.453427</td>\n",
       "      <td>0.251534</td>\n",
       "      <td>0.165268</td>\n",
       "      <td>0.245672</td>\n",
       "      <td>0.261304</td>\n",
       "      <td>0.449284</td>\n",
       "      <td>0.546647</td>\n",
       "      <td>0.462498</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.617909</td>\n",
       "      <td>0.249891</td>\n",
       "      <td>0.378215</td>\n",
       "      <td>0.092759</td>\n",
       "      <td>0.160064</td>\n",
       "      <td>0.118537</td>\n",
       "      <td>0.230778</td>\n",
       "      <td>0.495773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.158949</td>\n",
       "      <td>0.620538</td>\n",
       "      <td>0.091021</td>\n",
       "      <td>0.432891</td>\n",
       "      <td>0.265370</td>\n",
       "      <td>0.151157</td>\n",
       "      <td>0.264376</td>\n",
       "      <td>0.242987</td>\n",
       "      <td>0.622701</td>\n",
       "      <td>0.729162</td>\n",
       "      <td>0.386664</td>\n",
       "      <td>0.617909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.292268</td>\n",
       "      <td>0.487737</td>\n",
       "      <td>0.146716</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.210082</td>\n",
       "      <td>0.295123</td>\n",
       "      <td>0.738816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.371650</td>\n",
       "      <td>0.254177</td>\n",
       "      <td>0.185114</td>\n",
       "      <td>0.331943</td>\n",
       "      <td>0.292446</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.339281</td>\n",
       "      <td>0.364961</td>\n",
       "      <td>0.274246</td>\n",
       "      <td>0.146917</td>\n",
       "      <td>0.327302</td>\n",
       "      <td>0.249891</td>\n",
       "      <td>0.292268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245735</td>\n",
       "      <td>0.263805</td>\n",
       "      <td>0.351084</td>\n",
       "      <td>0.255051</td>\n",
       "      <td>0.358715</td>\n",
       "      <td>0.218128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>0.288788</td>\n",
       "      <td>0.431108</td>\n",
       "      <td>0.103865</td>\n",
       "      <td>0.277158</td>\n",
       "      <td>0.358221</td>\n",
       "      <td>0.314472</td>\n",
       "      <td>0.319941</td>\n",
       "      <td>0.197103</td>\n",
       "      <td>0.513436</td>\n",
       "      <td>0.415231</td>\n",
       "      <td>0.288437</td>\n",
       "      <td>0.378215</td>\n",
       "      <td>0.487737</td>\n",
       "      <td>0.245735</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.161840</td>\n",
       "      <td>0.205429</td>\n",
       "      <td>0.165064</td>\n",
       "      <td>0.295957</td>\n",
       "      <td>0.371687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>0.204743</td>\n",
       "      <td>0.118819</td>\n",
       "      <td>0.086659</td>\n",
       "      <td>0.193487</td>\n",
       "      <td>0.127679</td>\n",
       "      <td>0.158615</td>\n",
       "      <td>0.190515</td>\n",
       "      <td>0.207656</td>\n",
       "      <td>0.125741</td>\n",
       "      <td>0.070071</td>\n",
       "      <td>0.151519</td>\n",
       "      <td>0.092759</td>\n",
       "      <td>0.146716</td>\n",
       "      <td>0.263805</td>\n",
       "      <td>0.161840</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.263785</td>\n",
       "      <td>0.226735</td>\n",
       "      <td>0.156674</td>\n",
       "      <td>0.127651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>0.256770</td>\n",
       "      <td>0.223585</td>\n",
       "      <td>0.278595</td>\n",
       "      <td>0.408844</td>\n",
       "      <td>0.192515</td>\n",
       "      <td>0.428081</td>\n",
       "      <td>0.259361</td>\n",
       "      <td>0.627800</td>\n",
       "      <td>0.216372</td>\n",
       "      <td>0.061753</td>\n",
       "      <td>0.488385</td>\n",
       "      <td>0.160064</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.351084</td>\n",
       "      <td>0.205429</td>\n",
       "      <td>0.263785</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.303459</td>\n",
       "      <td>0.219087</td>\n",
       "      <td>0.174496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0.150072</td>\n",
       "      <td>0.247314</td>\n",
       "      <td>0.250834</td>\n",
       "      <td>0.406912</td>\n",
       "      <td>0.192686</td>\n",
       "      <td>0.210050</td>\n",
       "      <td>0.267240</td>\n",
       "      <td>0.371178</td>\n",
       "      <td>0.163371</td>\n",
       "      <td>0.133348</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.118537</td>\n",
       "      <td>0.210082</td>\n",
       "      <td>0.255051</td>\n",
       "      <td>0.165064</td>\n",
       "      <td>0.226735</td>\n",
       "      <td>0.303459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.485855</td>\n",
       "      <td>0.209454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.223730</td>\n",
       "      <td>0.291202</td>\n",
       "      <td>0.307961</td>\n",
       "      <td>0.341204</td>\n",
       "      <td>0.299892</td>\n",
       "      <td>0.280659</td>\n",
       "      <td>0.301678</td>\n",
       "      <td>0.314065</td>\n",
       "      <td>0.182994</td>\n",
       "      <td>0.247529</td>\n",
       "      <td>0.272693</td>\n",
       "      <td>0.230778</td>\n",
       "      <td>0.295123</td>\n",
       "      <td>0.358715</td>\n",
       "      <td>0.295957</td>\n",
       "      <td>0.156674</td>\n",
       "      <td>0.219087</td>\n",
       "      <td>0.485855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.262579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.104115</td>\n",
       "      <td>0.489116</td>\n",
       "      <td>0.046432</td>\n",
       "      <td>0.335111</td>\n",
       "      <td>0.191665</td>\n",
       "      <td>0.110936</td>\n",
       "      <td>0.222571</td>\n",
       "      <td>0.185452</td>\n",
       "      <td>0.500586</td>\n",
       "      <td>0.622770</td>\n",
       "      <td>0.290761</td>\n",
       "      <td>0.495773</td>\n",
       "      <td>0.738816</td>\n",
       "      <td>0.218128</td>\n",
       "      <td>0.371687</td>\n",
       "      <td>0.127651</td>\n",
       "      <td>0.174496</td>\n",
       "      <td>0.209454</td>\n",
       "      <td>0.262579</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              today      eggs     jumps     brown     quick  beautiful  \\\n",
       "today      1.000000  0.183048  0.142099  0.147418  0.370023   0.314184   \n",
       "eggs       0.183048  1.000000  0.150621  0.466903  0.245487   0.214437   \n",
       "jumps      0.142099  0.150621  1.000000  0.110400  0.301096   0.142554   \n",
       "brown      0.147418  0.466903  0.110400  1.000000  0.217009   0.355700   \n",
       "quick      0.370023  0.245487  0.301096  0.217009  1.000000   0.289178   \n",
       "beautiful  0.314184  0.214437  0.142554  0.355700  0.289178   1.000000   \n",
       "lazy       0.286803  0.236769  0.222112  0.256875  0.400060   0.294308   \n",
       "blue       0.180088  0.297546  0.187263  0.683011  0.220331   0.461366   \n",
       "toast      0.174257  0.495935  0.119099  0.347578  0.292624   0.189482   \n",
       "sausages   0.094879  0.548311  0.083306  0.289625  0.162966   0.114540   \n",
       "green      0.246754  0.392072  0.150752  0.646850  0.287504   0.392392   \n",
       "beans      0.147652  0.585054  0.095544  0.453427  0.251534   0.165268   \n",
       "bacon      0.158949  0.620538  0.091021  0.432891  0.265370   0.151157   \n",
       "love       0.371650  0.254177  0.185114  0.331943  0.292446   0.594739   \n",
       "breakfast  0.288788  0.431108  0.103865  0.277158  0.358221   0.314472   \n",
       "kings      0.204743  0.118819  0.086659  0.193487  0.127679   0.158615   \n",
       "sky        0.256770  0.223585  0.278595  0.408844  0.192515   0.428081   \n",
       "fox        0.150072  0.247314  0.250834  0.406912  0.192686   0.210050   \n",
       "dog        0.223730  0.291202  0.307961  0.341204  0.299892   0.280659   \n",
       "ham        0.104115  0.489116  0.046432  0.335111  0.191665   0.110936   \n",
       "\n",
       "               lazy      blue     toast  sausages     green     beans  \\\n",
       "today      0.286803  0.180088  0.174257  0.094879  0.246754  0.147652   \n",
       "eggs       0.236769  0.297546  0.495935  0.548311  0.392072  0.585054   \n",
       "jumps      0.222112  0.187263  0.119099  0.083306  0.150752  0.095544   \n",
       "brown      0.256875  0.683011  0.347578  0.289625  0.646850  0.453427   \n",
       "quick      0.400060  0.220331  0.292624  0.162966  0.287504  0.251534   \n",
       "beautiful  0.294308  0.461366  0.189482  0.114540  0.392392  0.165268   \n",
       "lazy       1.000000  0.230513  0.231329  0.168205  0.230945  0.245672   \n",
       "blue       0.230513  1.000000  0.232264  0.097443  0.764083  0.261304   \n",
       "toast      0.231329  0.232264  1.000000  0.498062  0.287608  0.449284   \n",
       "sausages   0.168205  0.097443  0.498062  1.000000  0.233169  0.546647   \n",
       "green      0.230945  0.764083  0.287608  0.233169  1.000000  0.462498   \n",
       "beans      0.245672  0.261304  0.449284  0.546647  0.462498  1.000000   \n",
       "bacon      0.264376  0.242987  0.622701  0.729162  0.386664  0.617909   \n",
       "love       0.339281  0.364961  0.274246  0.146917  0.327302  0.249891   \n",
       "breakfast  0.319941  0.197103  0.513436  0.415231  0.288437  0.378215   \n",
       "kings      0.190515  0.207656  0.125741  0.070071  0.151519  0.092759   \n",
       "sky        0.259361  0.627800  0.216372  0.061753  0.488385  0.160064   \n",
       "fox        0.267240  0.371178  0.163371  0.133348  0.323800  0.118537   \n",
       "dog        0.301678  0.314065  0.182994  0.247529  0.272693  0.230778   \n",
       "ham        0.222571  0.185452  0.500586  0.622770  0.290761  0.495773   \n",
       "\n",
       "              bacon      love  breakfast     kings       sky       fox  \\\n",
       "today      0.158949  0.371650   0.288788  0.204743  0.256770  0.150072   \n",
       "eggs       0.620538  0.254177   0.431108  0.118819  0.223585  0.247314   \n",
       "jumps      0.091021  0.185114   0.103865  0.086659  0.278595  0.250834   \n",
       "brown      0.432891  0.331943   0.277158  0.193487  0.408844  0.406912   \n",
       "quick      0.265370  0.292446   0.358221  0.127679  0.192515  0.192686   \n",
       "beautiful  0.151157  0.594739   0.314472  0.158615  0.428081  0.210050   \n",
       "lazy       0.264376  0.339281   0.319941  0.190515  0.259361  0.267240   \n",
       "blue       0.242987  0.364961   0.197103  0.207656  0.627800  0.371178   \n",
       "toast      0.622701  0.274246   0.513436  0.125741  0.216372  0.163371   \n",
       "sausages   0.729162  0.146917   0.415231  0.070071  0.061753  0.133348   \n",
       "green      0.386664  0.327302   0.288437  0.151519  0.488385  0.323800   \n",
       "beans      0.617909  0.249891   0.378215  0.092759  0.160064  0.118537   \n",
       "bacon      1.000000  0.292268   0.487737  0.146716  0.172964  0.210082   \n",
       "love       0.292268  1.000000   0.245735  0.263805  0.351084  0.255051   \n",
       "breakfast  0.487737  0.245735   1.000000  0.161840  0.205429  0.165064   \n",
       "kings      0.146716  0.263805   0.161840  1.000000  0.263785  0.226735   \n",
       "sky        0.172964  0.351084   0.205429  0.263785  1.000000  0.303459   \n",
       "fox        0.210082  0.255051   0.165064  0.226735  0.303459  1.000000   \n",
       "dog        0.295123  0.358715   0.295957  0.156674  0.219087  0.485855   \n",
       "ham        0.738816  0.218128   0.371687  0.127651  0.174496  0.209454   \n",
       "\n",
       "                dog       ham  \n",
       "today      0.223730  0.104115  \n",
       "eggs       0.291202  0.489116  \n",
       "jumps      0.307961  0.046432  \n",
       "brown      0.341204  0.335111  \n",
       "quick      0.299892  0.191665  \n",
       "beautiful  0.280659  0.110936  \n",
       "lazy       0.301678  0.222571  \n",
       "blue       0.314065  0.185452  \n",
       "toast      0.182994  0.500586  \n",
       "sausages   0.247529  0.622770  \n",
       "green      0.272693  0.290761  \n",
       "beans      0.230778  0.495773  \n",
       "bacon      0.295123  0.738816  \n",
       "love       0.358715  0.218128  \n",
       "breakfast  0.295957  0.371687  \n",
       "kings      0.156674  0.127651  \n",
       "sky        0.219087  0.174496  \n",
       "fox        0.485855  0.209454  \n",
       "dog        1.000000  0.262579  \n",
       "ham        0.262579  1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(vec_df.values)\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=unique_words, columns=unique_words)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "JC-aQ1IERiq9",
    "outputId": "caead4fe-3923-423f-9b0f-fbe71bcc6097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "today        [love, quick, beautiful]\n",
       "eggs         [bacon, beans, sausages]\n",
       "jumps               [dog, quick, sky]\n",
       "brown             [blue, green, eggs]\n",
       "quick        [lazy, today, breakfast]\n",
       "beautiful           [love, blue, sky]\n",
       "lazy         [quick, love, breakfast]\n",
       "blue              [green, brown, sky]\n",
       "toast         [bacon, breakfast, ham]\n",
       "sausages           [bacon, ham, eggs]\n",
       "green              [blue, brown, sky]\n",
       "beans         [bacon, eggs, sausages]\n",
       "bacon          [ham, sausages, toast]\n",
       "love         [beautiful, today, blue]\n",
       "breakfast        [toast, bacon, eggs]\n",
       "kings                [love, sky, fox]\n",
       "sky          [blue, green, beautiful]\n",
       "fox                [dog, brown, blue]\n",
       "dog                [fox, love, brown]\n",
       "ham          [bacon, sausages, toast]\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(unique_words)\n",
    "similarity_df.apply(lambda row: feature_names[np.argsort(-row.values)[1:4]], \n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pU1BAEUWRiq_"
   },
   "source": [
    "# The FastText Model\n",
    "\n",
    "The FastText model was first introduced by Facebook in 2016 as an extension and supposedly improvement of the vanilla Word2Vec model. Based on the original paper titled _[‘Enriching Word Vectors with Subword Information’](https://arxiv.org/pdf/1607.04606.pdf)_ by Mikolov et al. which is an excellent read to gain an in-depth understanding of how this model works. Overall, FastText is a framework for learning word representations and also performing robust, fast and accurate text classification. The framework is open-sourced by Facebook on [GitHub](https://github.com/facebookresearch/fastText) and claims to have the following.\n",
    "\n",
    "- Recent state-of-the-art English word vectors.\n",
    "- Word vectors for 157 languages trained on Wikipedia and Crawl.\n",
    "- Models for language identification and various supervised tasks.\n",
    "\n",
    "Though I haven't implemented this model from scratch, based on the research paper, following is what I learnt about how the model works. In general, predictive models like the Word2Vec model typically considers each word as a distinct entity (e.g. where) and generates a dense embedding for the word. However this poses to be a serious limitation with languages having massive vocabularies and many rare words which may not occur a lot in different corpora. \n",
    "\n",
    "The Word2Vec model typically ignores the morphological structure of each word and considers a word as a single entity. The FastText model ___considers each word as a Bag of Character n-grams___. This is also called as a ___subword model___ in the paper.\n",
    "\n",
    "We add special boundary symbols __<__ and __>__ at the beginning and end of words. This enables us to distinguish prefixes and suffixes from other character sequences. We also include the word __w__ itself in the set of its n-grams, to learn a representation for each word (in addition to its character n-grams). \n",
    "\n",
    "Taking the word where and __n=3 (tri-grams)__ as an example, it will be represented by the __character n-grams__: __<wh, whe, her, ere, re>__ and the special sequence __< where >__ representing the whole word. Note that the sequence , corresponding to the word __< her >__ is different from the tri-gram __her__ from the word __where__.\n",
    "\n",
    "In practice, the paper recommends in extracting all the n-grams for __n ≥ 3__ and __n ≤ 6__. This is a very simple approach, and different sets of n-grams could be considered, for example taking all prefixes and suffixes. We typically associate a vector representation (embedding) to each n-gram for a word. \n",
    "\n",
    "Thus, we can represent a word by the sum of the vector representations of its n-grams or the average of the embedding of these n-grams. Thus, due to this effect of leveraging n-grams from individual words based on their characters, there is a higher chance for rare words to get a good representation since their character based n-grams should occur across other words of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTpq6CyxRiq_"
   },
   "source": [
    "# Robust FastText Model with Gensim\n",
    "\n",
    "The __`gensim`__ package has nice wrappers providing us interfaces to leverage the FastText model available under the `gensim.models.fasttext` module. Let’s apply this once again on our toy corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qzrr2AiJRirA",
    "outputId": "953c4f62-98e4-42fc-f68d-dda2bc6c5989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.fasttext.FastText at 0x1aafb101e48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 15    # Word vector dimensionality  \n",
    "window_context = 20  # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "sg = 1               # skip-gram model\n",
    "\n",
    "ft_model = FastText(tokenized_corpus, \n",
    "                     window=window_context, min_count = min_word_count,\n",
    "                     sg=sg, sample=sample)\n",
    "ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "iR6sYaASRirB",
    "outputId": "233613d4-3851-4b02-dc33-1ff52f081948"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = ft_model.wv.index_to_key\n",
    "wvs = ft_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=5)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='green', edgecolors='k')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzqhdlSpbY8s"
   },
   "source": [
    "## Embedding Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_LplYuHBRirE",
    "outputId": "00a2d159-159b-4e65-c398-464ffdd3f53c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00221835, -0.00319054,  0.00277566,  0.00102296, -0.00153656,\n",
       "        -0.00163024, -0.00231491,  0.00240707, -0.00048566,  0.00098691,\n",
       "        -0.00085169,  0.00126206, -0.00265305,  0.00327311,  0.00120383,\n",
       "        -0.00019749,  0.00075437,  0.00002203, -0.0023516 ,  0.00075234,\n",
       "         0.00159587, -0.0020277 , -0.00128701, -0.0006478 ,  0.00230742,\n",
       "        -0.00089073,  0.00187692, -0.00030547,  0.00170599,  0.00128906,\n",
       "        -0.00051703, -0.00038594,  0.00242666,  0.00100433, -0.00119173,\n",
       "         0.00170316,  0.00242299, -0.00012247, -0.00204472,  0.00158365,\n",
       "        -0.00056463,  0.00147082,  0.00026043, -0.00049712,  0.00064704,\n",
       "        -0.00022024, -0.00114704,  0.00118757,  0.000212  , -0.00046281,\n",
       "         0.00192766,  0.00272246, -0.0024699 , -0.00020753,  0.00364099,\n",
       "        -0.00227419,  0.00199629, -0.00278769,  0.00419264,  0.00065458,\n",
       "         0.00161865, -0.00176483, -0.0050681 ,  0.0039457 ,  0.00419691,\n",
       "        -0.00438108,  0.00179408, -0.00103201, -0.00248051,  0.00057169,\n",
       "         0.00197521,  0.00369702, -0.00132672, -0.00333336, -0.00037761,\n",
       "        -0.00063061,  0.00151157, -0.00074134, -0.00047029, -0.00166475,\n",
       "        -0.00112765,  0.00042481,  0.00317388, -0.00254596, -0.00021362,\n",
       "         0.00077707,  0.00043711, -0.00418357,  0.00145694,  0.00046335,\n",
       "        -0.00201655, -0.00085768,  0.00426288, -0.00101287, -0.00393895,\n",
       "        -0.00113271,  0.00245984, -0.0010701 ,  0.00095116,  0.00103001],\n",
       "       dtype=float32),\n",
       " (100,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.wv['sky'], ft_model.wv['sky'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "40z0JRH5RirG",
    "outputId": "4a3f3b54-7f35-41c5-f7b2-5738421a029b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06948187\n",
      "0.034941223\n"
     ]
    }
   ],
   "source": [
    "print(ft_model.wv.similarity(w1='ham', w2='sky'))\n",
    "print(ft_model.wv.similarity(w1='ham', w2='sausages'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "7iwDNr-lRirI",
    "outputId": "c8988302-e591-4264-d174-5086d703b0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd one out for [ dog fox ham ]: ham\n",
      "Odd one out for [ bacon ham sky sausages ]: ham\n"
     ]
    }
   ],
   "source": [
    "st1 = \"dog fox ham\"\n",
    "print('Odd one out for [',st1, ']:',  \n",
    "      ft_model.wv.doesnt_match(st1.split()))\n",
    "\n",
    "st2 = \"bacon ham sky sausages\"\n",
    "print('Odd one out for [',st2, ']:', \n",
    "      ft_model.wv.doesnt_match(st2.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QxZeE7w8RirK"
   },
   "source": [
    "### Getting document level embeddings\n",
    "\n",
    "Now suppose we wanted to cluster the eight documents from our toy corpus, we would need to get the document level embeddings from each of the words present in each document. One strategy would be to average out the word embeddings for each word in a document. This is an extremely useful strategy and you can adopt the same for your own problems. Let’s apply this now on our corpus to get features for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8ETS-PxRirK"
   },
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "MPFoB8y5RirM",
    "outputId": "c6073b7d-b71e-4496-e325-805d3ccacad4"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (15,) (100,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21448/1901441932.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get document level embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m ft_doc_features = averaged_word_vectorizer(corpus=tokenized_corpus, model=ft_model,\n\u001b[1;32m----> 3\u001b[1;33m                                              num_features=feature_size)\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mft_doc_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21448/3001449133.py\u001b[0m in \u001b[0;36maveraged_word_vectorizer\u001b[1;34m(corpus, model, num_features)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n\u001b[1;32m---> 20\u001b[1;33m                     for tokenized_sentence in corpus]\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21448/3001449133.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n\u001b[1;32m---> 20\u001b[1;33m                     for tokenized_sentence in corpus]\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21448/3001449133.py\u001b[0m in \u001b[0;36maverage_word_vectors\u001b[1;34m(words, model, vocabulary, num_features)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mnwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnwords\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mfeature_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (15,) (100,) "
     ]
    }
   ],
   "source": [
    "# get document level embeddings\n",
    "ft_doc_features = averaged_word_vectorizer(corpus=tokenized_corpus, model=ft_model,\n",
    "                                             num_features=feature_size)\n",
    "pd.DataFrame(ft_doc_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgrKl_vqRirN"
   },
   "source": [
    "### Trying out document clustering\n",
    "\n",
    "Now that we have our features for each document, let’s cluster these documents using the Affinity Propagation algorithm, which is a clustering algorithm based on the concept of “message passing” between data points and does not need the number of clusters as an explicit input which is often required by partition-based clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "n9C0aRLwRirO",
    "outputId": "cfe31134-897f-4f74-cae9-1f9529c09786"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "ap = AffinityPropagation()\n",
    "ap.fit(ft_doc_features)\n",
    "\n",
    "cluster_labels = ap.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, \n",
    "                              columns=['ClusterLabel'])\n",
    "\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bB6pQQTPRirQ"
   },
   "source": [
    "We can see that our algorithm has clustered each document into the right group based on our Word2Vec features. Pretty neat! We can also visualize how each document in positioned in each cluster by using [_Principal Component Analysis (PCA)_](https://en.wikipedia.org/wiki/Principal_component_analysis) to reduce the feature dimensions to 2-D and then visualizing the same (by color coding each cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "3VA6oJnIRirR",
    "outputId": "05242d98-b335-44c6-f234-d3c067a16bfa"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pcs = pca.fit_transform(ft_doc_features)\n",
    "labels = ap.labels_\n",
    "categories = list(corpus_df['Category'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    color = 'orange' if label == 0 else 'blue' if label == 1 else 'green'\n",
    "    annotation_label = categories[i]\n",
    "    x, y = pcs[i]\n",
    "    plt.scatter(x, y, c=color, edgecolors='k')\n",
    "    plt.annotate(annotation_label, xy=(x+1e-2, y+1e-2), xytext=(0, 0), \n",
    "                 textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvFiGdkMRirS"
   },
   "source": [
    "Everything looks to be in order as documents in each cluster are closer to each other and far apart from other clusters."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03 -Text Representation - Embedding Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
